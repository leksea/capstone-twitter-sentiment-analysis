{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70934233-4e52-4f0f-883b-56cbc48f299c",
      "metadata": {
        "id": "70934233-4e52-4f0f-883b-56cbc48f299c"
      },
      "source": [
        "# Twitter Sentiment Analysis of Public Reaction to COVID-19 News\n",
        "\n",
        "**Project Overview:**\n",
        "\n",
        "This project aims to analyze a large dataset of COVID-19-related tweets to understand how public sentiment evolves and spreads in response to news announcements and events. By leveraging natural language processing (NLP) techniques and sentiment analysis models, we seek to gain valuable insights into the dynamics of online conversations surrounding the pandemic.\n",
        "\n",
        "**Importance and Motivation:**\n",
        "\n",
        "Understanding public sentiment during a global crisis like the COVID-19 pandemic is crucial for various stakeholders, including:\n",
        "\n",
        "- **Public Health Officials:** To gauge public response to health policies and interventions.\n",
        "- **Media Outlets:** To assess the impact of their news coverage on public perception.\n",
        "- **Government Agencies:** To monitor public opinion and tailor communication strategies.\n",
        "- **Researchers:** To study the spread of information and misinformation on social media.\n",
        "\n",
        "This project contributes to this understanding by providing a comprehensive analysis of Twitter data, revealing trends and patterns in public sentiment related to COVID-19.\n",
        "\n",
        "Notebook Structure\n",
        "---\n",
        "<details>\n",
        "<summary><b>1. Business Problem and Objectives</b></summary>\n",
        "   Define the problem being addressed and its relevance to real-world scenarios.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>2. Data Acquisition and Preparation</b></summary>\n",
        "\n",
        "- ### **2.1 Data Source and Download**  \n",
        "  Explanation of the dataset source and how it was obtained.  \n",
        "\n",
        "- ### **2.2 Installing Required Modules**  \n",
        "  List and install the libraries needed for the project.  \n",
        "\n",
        "- ### **2.3 Importing Modules and Global Variables**  \n",
        "  Set up imports and define constants or global variables.  \n",
        "\n",
        "- ### **2.4 Defining Supplemental Functions**  \n",
        "  Helper functions to streamline data processing.  \n",
        "\n",
        "- ### **2.5 Data Loading**  \n",
        "  Load the dataset into a DataFrame or suitable data structure.  \n",
        "\n",
        "- ### **2.6 Basic Data Understanding**  \n",
        "  Perform initial data exploration, including shape, columns, and types.  \n",
        "\n",
        "- ### **2.7 Data Cleaning and EDA: Date**  \n",
        "  Extract and analyze temporal trends in the dataset.\n",
        "\n",
        "- ### **2.8 Data Cleaning and EDA: Language**  \n",
        "  Extract relevant language subset.\n",
        "\n",
        "- ### **2.9 Data Cleaning and EDA: Location**  \n",
        "  Process location data to standardize and extract insights.  \n",
        "\n",
        "- ### **2.10 Data Cleaning and EDA: Source**  \n",
        "  Analyze the platforms from which tweets were sent.  \n",
        "\n",
        "- ### **2.11 Data Cleaning and EDA: Sentiment**  \n",
        "  Explore sentiment labels and their distribution.  \n",
        "\n",
        "- ### **2.12 Exploratory Data Analysis (EDA): Social Connections**  \n",
        "  Investigate user mentions, retweets, and network connections.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>3. Text Preprocessing and Feature Engineering</b></summary>\n",
        "\n",
        "- ### **3.1 Cleaning**  \n",
        "  Remove noise, including special characters, links, mentions, and hashtags.  \n",
        "\n",
        "- ### **3.2 Preprocessing**  \n",
        "  Tokenize, lemmatize, and remove stop words from the text data.  \n",
        "\n",
        "- ### **3.3 Feature Extraction**  \n",
        "  Generate n-grams, TF-IDF features, or embeddings for model input.  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>4. Sentiment Analysis</b></summary>\n",
        "\n",
        "- ### **4.1 Train-Test Data Split**  \n",
        "- ### **4.2 Base Model Training, Evaluation**   \n",
        "  Choose a simple base classification model and train it on the preprocessed data. Assess model performance using metrics like accuracy, precision, and recall.\n",
        "- ### **4.3 Model Hyperparameter Search**\n",
        "  Vary base model parameters to see if it improves model's perfomance.      \n",
        "- ### **4.4 Insights and Visualization**  \n",
        "  Visualize results and discuss findings, including strengths and limitations.\n",
        "- ### **4.5 Other Models Tested**\n",
        "  List other models tested, their parameters and performance.   \n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>6. Conclusion</b></summary>\n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc",
      "metadata": {
        "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc"
      },
      "source": [
        "## 1.1 Business Problem and Objectives\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "Media outlets and public health organizations need a better understanding of how their COVID-19-related news and announcements influence public sentiment on Twitter. This project addresses this need by analyzing a large dataset of tweets to identify and track sentiment trends in response to news events.\n",
        "\n",
        "**Key Questions:**\n",
        "\n",
        "- How do positive and negative sentiments spread among users following a COVID-19 news announcement?\n",
        "- What are the key topics and themes associated with different sentiment trends?\n",
        "- Can we identify any patterns or correlations between news events and changes in public sentiment?\n",
        "\n",
        "**Project Objectives:**\n",
        "\n",
        "- To develop a robust NLP pipeline for cleaning, preprocessing, and analyzing Twitter data.\n",
        "- To apply sentiment analysis models to classify tweets and track sentiment trends over time.\n",
        "- To visualize and interpret the sentiment analysis results to provide actionable insights.\n",
        "- To potentially identify key influencers and networks driving sentiment on Twitter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Acquisition and Preparation\n"
      ],
      "metadata": {
        "id": "VveiYpccgaRY"
      },
      "id": "VveiYpccgaRY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Data Understanding.\n",
        "\n",
        "\n",
        "This section outlines the source of the data used in this project,  and provides instructions for downloading it.\n",
        "\n",
        "**Data Sources:**\n",
        "\n",
        "1. **Covid-19 Twitter Dataset:** The primary dataset for this Twitter sentiment analysis project is the \"Covid-19 Twitter Dataset\" available on Kaggle. This dataset contains a large collection of tweets related to COVID-19, including tweet text, user details, location, and sentiment labels.\n",
        "\n",
        "2. **GloVe Embeddings:** To enhance the analysis, we will utilize pre-trained GloVe embeddings from Stanford NLP. These word embeddings capture semantic relationships between words and can improve the performance of NLP models.\n",
        "\n",
        "**Data Relevance**:\n",
        "\n",
        "* The Covid-19 Twitter Dataset contains a vast collection of tweets related to the pandemic, providing a valuable source of public opinion and sentiment during this period.\n",
        "* This dataset is suitable for our project because it includes sentiment labels, allowing us to train and evaluate sentiment analysis models.\n",
        "\n",
        "\n",
        "**Data Limitations**\n",
        "\n",
        "* Recent changes to the Twitter API have significantly impacted the accessibility of tweet data for research and analysis.  Specifically, Twitter has severely restricted free API access. This means that retrieving the original dataset used in this project [(see Panacea's lab github page)](https://github.com/thepanacealab/covid19_twitter) is no longer possible without incurring substantial costs.\n",
        "* This dataset contains pre-computed sentiment labels, however, we don't know what method was used and how the accuracy of the sentiment was evaluated.\n",
        "\n",
        "**Download Instructions:**\n",
        "\n",
        "1. **Kaggle Dataset:** The dataset can be accessed and downloaded from the following Kaggle page:\n",
        "    [Covid-19 Twitter Dataset](https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset/data)\n",
        "\n",
        "2. ** GloVe Embeddings:** The pre-trained GloVe embeddings can be obtained from the [Stanford NLP website](https://nlp.stanford.edu/projects/glove/). For this project, we will use the \"glove.twitter.27B.zip\" file, which contains 10-200-dimensional embeddings trained on 27B Twitter tokens.\n",
        "\n",
        "**Data Storage:**\n",
        "\n",
        "   - **Local Execution:** If you are running the notebook locally, please download the dataset files and place them into a folder named `Data` within your project directory.\n",
        "  \n",
        "   - **Colab Environment:** If you are using Google Colab, it's best recommended to mount a google drive.\n",
        "\n",
        "**Data Loading:**\n",
        "\n",
        "1. **Loading Datasets:** code would check runtime environment and for local environment set the `data_dir` to be `Data` folder on the same level as the notebook. If you're using Colab, you'll need to adjust the defauld directory. Highly recommend to not use runtime-dependent directory.   \n",
        "2. **Loading GloVe Embeddings:** The script will automatically check if the GloVe embeddings are already present in the 'Data' directory. If not, it will download and extract them for you:\n",
        "```\n",
        "download_and_extract_glove(data_dir)\n",
        "```"
      ],
      "metadata": {
        "id": "lSJyODVEaJZs"
      },
      "id": "lSJyODVEaJZs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Installing Required Modules\n",
        "\n",
        "This section focuses on installing the necessary Python libraries and packages required for our Twitter sentiment analysis project. We accomplish this through the following steps:\n",
        "\n",
        "1. **Requirements File:**\n",
        "    - We retrieve the list of required packages from a `requirements.txt` file hosted on GitHub using `wget`. This file contains the names and versions of all the dependencies.\n",
        "    - This ensures that we install the correct versions of the libraries for compatibility and reproducibility.\n",
        "    - Here's the link to the requirements file on GitHub:\n",
        "       `https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt`\n",
        "\n",
        "2. **Installation using pip:**\n",
        "    - We use Python's `pip` package manager to install the libraries listed in the `requirements.txt` file.\n",
        "    - The `-r` flag instructs `pip` to read the requirements file and install all the packages listed within."
      ],
      "metadata": {
        "id": "fQwFcnPjZpd9"
      },
      "id": "fQwFcnPjZpd9"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "h4WgG9gZZbu4",
        "outputId": "395ab501-ee29-4c0d-e408-0e5dd40176cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h4WgG9gZZbu4",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
        "!pip install -r 'requirements.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "st62fukEaHir",
        "outputId": "6282121c-dd5f-45ec-b869-7cd802e5ebb6"
      },
      "id": "st62fukEaHir",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 04:20:38--  https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     165  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-04 04:20:38 (16.2 MB/s) - ‘requirements.txt’ saved [165/165]\n",
            "\n",
            "Collecting scikit-learn (from -r requirements.txt (line 1))\n",
            "  Using cached scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting scipy (from -r requirements.txt (line 2))\n",
            "  Using cached scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting seaborn (from -r requirements.txt (line 3))\n",
            "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pandas (from -r requirements.txt (line 4))\n",
            "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.24.4)\n",
            "Collecting branca (from -r requirements.txt (line 6))\n",
            "  Using cached branca-0.8.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting cartopy (from -r requirements.txt (line 7))\n",
            "  Using cached Cartopy-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
            "Collecting folium (from -r requirements.txt (line 9))\n",
            "  Using cached folium-0.19.3-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting nltk (from -r requirements.txt (line 10))\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 11))\n",
            "  Using cached pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting wordcloud (from -r requirements.txt (line 12))\n",
            "  Using cached wordcloud-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (4.66.1)\n",
            "Collecting emot (from -r requirements.txt (line 14))\n",
            "  Using cached emot-3.1-py3-none-any.whl.metadata (396 bytes)\n",
            "Collecting geopy (from -r requirements.txt (line 15))\n",
            "  Using cached geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting emoji (from -r requirements.txt (line 18))\n",
            "  Using cached emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting networkx (from -r requirements.txt (line 19))\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting ipython (from -r requirements.txt (line 20))\n",
            "  Using cached ipython-8.31.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting transformers (from -r requirements.txt (line 21))\n",
            "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting torch (from -r requirements.txt (line 22))\n",
            "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 1))\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 1))\n",
            "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn->-r requirements.txt (line 3))\n",
            "  Using cached matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas->-r requirements.txt (line 4))\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 4))\n",
            "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 4))\n",
            "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting jinja2>=3 (from branca->-r requirements.txt (line 6))\n",
            "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting shapely>=1.8 (from cartopy->-r requirements.txt (line 7))\n",
            "  Using cached shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 7)) (23.2)\n",
            "Collecting pyshp>=2.3 (from cartopy->-r requirements.txt (line 7))\n",
            "  Using cached pyshp-2.3.1-py2.py3-none-any.whl.metadata (55 kB)\n",
            "Collecting pyproj>=3.3.1 (from cartopy->-r requirements.txt (line 7))\n",
            "  Using cached pyproj-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->-r requirements.txt (line 8)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->-r requirements.txt (line 8)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->-r requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->-r requirements.txt (line 8)) (2023.11.17)\n",
            "Collecting xyzservices (from folium->-r requirements.txt (line 9))\n",
            "  Using cached xyzservices-2024.9.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting click (from nltk->-r requirements.txt (line 10))\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk->-r requirements.txt (line 10))\n",
            "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting geographiclib<3,>=1.52 (from geopy->-r requirements.txt (line 15))\n",
            "  Using cached geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting decorator (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting exceptiongroup (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting matplotlib-inline (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pexpect>4.3 (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting pygments>=2.4.0 (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting stack_data (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5.13.0 (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting typing_extensions>=4.6 (from ipython->-r requirements.txt (line 20))\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting filelock (from transformers->-r requirements.txt (line 21))\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers->-r requirements.txt (line 21))\n",
            "  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers->-r requirements.txt (line 21))\n",
            "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers->-r requirements.txt (line 21))\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 21))\n",
            "  Using cached safetensors-0.5.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 22))\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch->-r requirements.txt (line 22))\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->-r requirements.txt (line 22))\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython->-r requirements.txt (line 20))\n",
            "  Using cached parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=3->branca->-r requirements.txt (line 6))\n",
            "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3))\n",
            "  Using cached contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3))\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3))\n",
            "  Using cached fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3))\n",
            "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3))\n",
            "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython->-r requirements.txt (line 20))\n",
            "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython->-r requirements.txt (line 20))\n",
            "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4))\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting executing>=1.2.0 (from stack_data->ipython->-r requirements.txt (line 20))\n",
            "  Using cached executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack_data->ipython->-r requirements.txt (line 20))\n",
            "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack_data->ipython->-r requirements.txt (line 20))\n",
            "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Using cached scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\n",
            "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached branca-0.8.1-py3-none-any.whl (26 kB)\n",
            "Using cached Cartopy-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "Using cached folium-0.19.3-py2.py3-none-any.whl (110 kB)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Using cached pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "Using cached wordcloud-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511 kB)\n",
            "Using cached emot-3.1-py3-none-any.whl (61 kB)\n",
            "Using cached geopy-2.4.1-py3-none-any.whl (125 kB)\n",
            "Using cached emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached ipython-8.31.0-py3-none-any.whl (821 kB)\n",
            "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "Using cached geographiclib-2.0-py3-none-any.whl (40 kB)\n",
            "Using cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Using cached matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "Using cached prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
            "Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "Using cached pyproj-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "Using cached pyshp-2.3.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "Using cached safetensors-0.5.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
            "Using cached shapely-2.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Using cached matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Using cached xyzservices-2024.9.0-py3-none-any.whl (85 kB)\n",
            "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Using cached contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Using cached fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pytz, pure-eval, ptyprocess, mpmath, emot, xyzservices, tzdata, typing_extensions, traitlets, threadpoolctl, sympy, six, shapely, scipy, safetensors, regex, pyyaml, pyshp, pyproj, pyparsing, pygments, prompt_toolkit, pillow, pexpect, parso, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, kiwisolver, joblib, geographiclib, fsspec, fonttools, filelock, executing, exceptiongroup, emoji, decorator, cycler, contourpy, click, asttokens, triton, stack_data, scikit-learn, python-dateutil, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, matplotlib-inline, jinja2, jedi, huggingface-hub, geopy, tokenizers, pandas, nvidia-cusolver-cu12, matplotlib, ipython, branca, wordcloud, transformers, torch, seaborn, folium, cartopy\n",
            "Successfully installed MarkupSafe-3.0.2 asttokens-3.0.0 branca-0.8.1 cartopy-0.24.1 click-8.1.8 contourpy-1.3.1 cycler-0.12.1 decorator-5.1.1 emoji-2.14.0 emot-3.1 exceptiongroup-1.2.2 executing-2.1.0 filelock-3.16.1 folium-0.19.3 fonttools-4.55.3 fsspec-2024.12.0 geographiclib-2.0 geopy-2.4.1 huggingface-hub-0.27.0 ipython-8.31.0 jedi-0.19.2 jinja2-3.1.5 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.10.0 matplotlib-inline-0.1.7 mpmath-1.3.0 networkx-3.4.2 nltk-3.9.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pandas-2.2.3 parso-0.8.4 pexpect-4.9.0 pillow-11.1.0 prompt_toolkit-3.0.48 ptyprocess-0.7.0 pure-eval-0.2.3 pygments-2.18.0 pyparsing-3.2.1 pyproj-3.7.0 pyshp-2.3.1 python-dateutil-2.9.0.post0 pytz-2024.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.0 scikit-learn-1.6.0 scipy-1.15.0 seaborn-0.13.2 shapely-2.0.6 six-1.17.0 stack_data-0.6.3 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.5.1 traitlets-5.14.3 transformers-4.47.1 triton-3.1.0 typing_extensions-4.12.2 tzdata-2024.2 wcwidth-0.2.13 wordcloud-1.9.4 xyzservices-2024.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "matplotlib_inline",
                  "pexpect",
                  "prompt_toolkit",
                  "six",
                  "wcwidth"
                ]
              },
              "id": "444b4badb27442268e8a8b34cff80666"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Importing Modules and Global Variables\n",
        "\n",
        "This section focuses on setting up the necessary environment for our analysis by importing the required Python modules and declaring global variables. We perform the following:\n",
        "\n",
        "1. **Module Imports:** We import a variety of modules that will be essential for data manipulation, analysis, visualization, and natural language processing tasks. These modules include:\n",
        "\n",
        "    - **Built-in Modules:** `os`, `string`, `re`, `glob`, `time`, `datetime`, `concurrent.futures`, `json`, `collections`, `concurrent`.\n",
        "    - **Data Processing and Analysis:** `numpy`, `pandas`.\n",
        "    - **Visualization:** `matplotlib.pyplot`, `seaborn`, `networkx`, `folium`, `branca.colormap`, `cartopy`.\n",
        "    - **Natural Language Processing (NLP):** `nltk`, `emot`, `emoji`.\n",
        "    - **Machine Learning:** `sklearn`.\n",
        "\n",
        "2. **Global Variable Declarations:**\n",
        "    - We define and initialize global variables that will be used throughout the analysis. These include:\n",
        "        - `geolocator`: An instance of the `Nominatim` geolocator from the `geopy` library for location standardization.\n",
        "        - `tqdm`: Enabling progress bars for long computations using `tqdm.pandas()`.\n",
        "        - `stop_words`: A set of English stop words from `nltk.corpus` for text preprocessing.\n",
        "        - `lemmatizer`: An instance of the `WordNetLemmatizer` from `nltk.stem` for lemmatization.\n",
        "\n",
        "3. **Downloading NLP Resources:**\n",
        "    - We download necessary resources for NLP tasks, such as stopwords, wordnet, and punkt using `nltk.download()`."
      ],
      "metadata": {
        "id": "Zd0e2IvvfuDJ"
      },
      "id": "Zd0e2IvvfuDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# built-in modules\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "from collections import Counter\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "# Optimization with Parallel Computing:\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "# url processing for extracting the coordinates\n",
        "import requests\n",
        "# progress bar monitoring\n",
        "from tqdm import tqdm\n",
        "# data manupulation, analysis, sparce matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import rand, randint\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "# general data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# networkx for user connection visualization\n",
        "import networkx as nx\n",
        "# world maps\n",
        "import folium\n",
        "from folium import plugins\n",
        "from folium.plugins import HeatMap\n",
        "import branca.colormap as cm\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
        "# displaying the folium heatmap\n",
        "from IPython.display import display, HTML\n",
        "#for location standartization\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "# world cloud\n",
        "from wordcloud import WordCloud\n",
        "# Natural Language Processing (NLP)\n",
        "import nltk\n",
        "from emot.emo_unicode import UNICODE_EMOJI\n",
        "import emoji\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Embeddings\n",
        "from transformers import BertTokenizer, BertModel\n",
        "# GPU-friendly modules\n",
        "import tensorflow as tf\n",
        "from scipy.sparse import csr_matrix\n",
        "import torch\n",
        "# Classification\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "%matplotlib inline\n",
        "## GLOBAL VARIABLES\n",
        "# stop words for tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "# Initialize geolocator globally for efficient geocoding cache\n",
        "geolocator = Nominatim(user_agent=\"batch-geocoding\")\n",
        "# Enable tqdm for pandas, progress bar for long computations\n",
        "tqdm.pandas()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "py58BnTUdbsd",
        "outputId": "4a7f586f-68c8-4cb7-9d90-77dbec6ac5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "id": "py58BnTUdbsd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cartopy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1a88abd817b0>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfolium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHeatMap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbranca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolormap\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mccrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLongitudeFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLatitudeFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cartopy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo install cartopy, click the button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_snippet",
                "actionText": "Install cartopy",
                "snippetFilter": "cartopy"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Defining Supplemental Functions\n",
        "\n",
        "\n",
        "* ```def determine_data_dir()```\n",
        "* ```def download_and_extract_glove(data_dir)```\n",
        "* ```def display_categorical_vals(df)```\n",
        "* ```def has_special_chars(location) ```\n",
        "* ```def save_cache_to_json(cache, file_path=\"location_cache.json\")```\n",
        "* ```def load_cache_to_json(cache, file_path=\"location_cache.json\")```\n",
        "* ```def geocode_location(location)```\n",
        "* ```def batch_geocode(locations)```\n",
        "* ```def extract_word(location, position=\"first\")```\n",
        "* ```def split_geocoded_location(location)```\n",
        "* ```def get_coordinates(input_type, name, output_as='center', retries=3, delay=5)```\n",
        "* ```def add_coordinates_with_progress(df, city_col='city', state_col='state', country_col='country')```\n",
        "* ```def color(magnitude)```\n",
        "* ```def generateBaseMap(input_type, df, default_location=[40.693943, -73.985880], default_zoom_start=2)```\n",
        "* ```def extract_html_source(source_text)```\n",
        "* ```def replace_emoticons_with_emojis(text)```\n",
        "* ```def process_tweet_data(tweet, emoji_list=None)```\n",
        "* ``` def clean_tweets_with_progress_parallel(df, text_col='original_text', num_processes=6)```\n",
        "* ```def preprocess_text(df, text_column)```\n",
        "* ```def compute_ngrams(df, text_column, ngram_range=(2, 3), max_features=5000)```\n",
        "* ```def bert_embedding(sentence, tokenizer, model)```\n",
        "* ```def plot_normalized_confusion_matrix(y_true, y_pred, class_labels, title=\"Confusion Matrix\", cmap=\"Blues\")```\n",
        "\n",
        " **Optional**\n",
        "\n",
        "* ``` def fitness_function(selected_features, X, y, model, cv=3)```\n",
        "* ```def update_gwo(population, alpha, beta, delta,a)```\n",
        "* ```def hybrid_gwo_abc(X, y, model, n_wolves=10, n_iter=20)```\n",
        "\n"
      ],
      "metadata": {
        "id": "auiqucUetmLC"
      },
      "id": "auiqucUetmLC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplemental function to determine data directory\n",
        "# Input: none\n",
        "# Output: Data directory, depending on runtime environment.\n",
        "\n",
        "def determine_data_dir():\n",
        "    \"\"\"\n",
        "    Determines the data directory based on the execution environment:\n",
        "    - Local: Uses 'Data' directory in the current working directory.\n",
        "    - Cloud (e.g., Google Colab): Uses '/content' as the data directory.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the appropriate data directory.\n",
        "    \"\"\"\n",
        "    if 'COLAB_GPU' in os.environ:  # Check if running in Google Colab\n",
        "        data_dir = \"/content/drive/MyDrive/Colab_Notebooks/Data\"\n",
        "        print(f\"Running in Google Colab. Using data directory: {data_dir}\")\n",
        "    else:\n",
        "        data_dir = os.path.join(os.getcwd(), \"Data\")\n",
        "        print(f\"Running locally. Using data directory: {data_dir}\")\n",
        "\n",
        "        # Ensure the 'Data' directory exists locally\n",
        "        if not os.path.isdir(data_dir):\n",
        "            print(f\"The directory '{data_dir}' does not exist. Please create it and place the data files there.\")\n",
        "            raise FileNotFoundError(f\"'{data_dir}' directory is required for local execution.\")\n",
        "\n",
        "    return data_dir"
      ],
      "metadata": {
        "id": "dLmUhcBtfxFS"
      },
      "id": "dLmUhcBtfxFS",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_glove(data_dir):\n",
        "    \"\"\"\n",
        "    Downloads GloVe embeddings from Stanford, extracts them, and copies the specified file to data_dir.\n",
        "    \"\"\"\n",
        "    # Check if the file is already present in the data_dir, only download if it's not there\n",
        "    glove_file = Path(data_dir) / \"glove.twitter.27B.200d.txt\"\n",
        "    if not glove_file.is_file():\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        # Define GloVe URLs\n",
        "        glove_zip_url = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
        "        zip_file_path = os.path.join(data_dir, \"glove.twitter.27B.zip\")\n",
        "\n",
        "        # Download GloVe\n",
        "        os.system(f\"wget {glove_zip_url} -O {zip_file_path}\")\n",
        "\n",
        "        # Extract GloVe\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "\n",
        "        # Remove the downloaded ZIP file\n",
        "        os.remove(zip_file_path)\n",
        "\n",
        "        print(f\"GloVe embeddings downloaded and extracted to {data_dir}\")\n",
        "    else:\n",
        "        print(\"GloVe embeddings are already present in the data directory.\")"
      ],
      "metadata": {
        "id": "lpstvZMPRzd9"
      },
      "id": "lpstvZMPRzd9",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplemental function will display unique values for all categorical columns in a dataframe.\n",
        "def display_categorical_vals(df):\n",
        "    # select categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # print categorical columns and their unique values\n",
        "    for col in categorical_columns:\n",
        "        unique_values = df[col].unique()\n",
        "        print(f\"Column '{col}' has unique values: {unique_values}\")"
      ],
      "metadata": {
        "id": "jamXGPHa6A9n"
      },
      "execution_count": 7,
      "outputs": [],
      "id": "jamXGPHa6A9n"
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to check for special characters\n",
        "def has_special_chars(location):\n",
        "    return bool(re.search(r'[^\\w\\s,.-]', location))  # check for non-alphanumeric and non-space chars"
      ],
      "metadata": {
        "id": "713FrV2huiiK"
      },
      "id": "713FrV2huiiK",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_cache_to_json(cache, file_path=\"location_cache.json\"):\n",
        "    \"\"\"\n",
        "    Saves the location cache to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        cache (dict): The cache dictionary to save.\n",
        "        file_path (str): The file path where the cache will be saved.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(cache, f)\n",
        "    print(f\"Cache saved to {file_path}\")"
      ],
      "metadata": {
        "id": "jWjMn4eshLyv"
      },
      "id": "jWjMn4eshLyv",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cache_from_json(file_path=\"location_cache.json\"):\n",
        "    \"\"\"\n",
        "    Loads the location cache from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The file path from where the cache will be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded cache dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            cache = json.load(f)\n",
        "        print(f\"Cache loaded from {file_path}\")\n",
        "        return cache\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No cache file found at {file_path}. Starting with an empty cache.\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "Za01dz-NhMjF"
      },
      "id": "Za01dz-NhMjF",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to cache geocoding results\n",
        "try:\n",
        "    # Initialize cache from file\n",
        "    data_dir = determine_data_dir()\n",
        "    location_cache_file = os.path.join(data_dir, \"location_cache.json\")  # Path to cache file\n",
        "    location_cache = load_cache_from_json(location_cache_file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, initialize an empty cache\n",
        "    location_cache = {}\n",
        "\n",
        "# Supplemental function to use a geocoding API for location resolution\n",
        "def geocode_location(location):\n",
        "    \"\"\"\n",
        "    Resolve location using a geocoding API with caching.\n",
        "    Returns results in City, State, Country format.\n",
        "    \"\"\"\n",
        "    # Check cache first\n",
        "    if location in location_cache:\n",
        "        return location_cache[location]\n",
        "\n",
        "    try:\n",
        "        # Add a delay to respect API rate limits\n",
        "        geo = geolocator.geocode(location, addressdetails=True, exactly_one=True, timeout=10)\n",
        "        if geo:\n",
        "            # Default: Extract the address components\n",
        "            address = geo.raw.get('address', {})\n",
        "            city = address.get('city') or address.get('town') or address.get('village') or address.get('hamlet')\n",
        "            state = address.get('state')\n",
        "            country = address.get('country')\n",
        "\n",
        "            # Fallback: Parse city and country from display_name if missing\n",
        "            if not city:\n",
        "                try:\n",
        "                    city = geo.raw['display_name'].split(',')[0].strip()\n",
        "                except (KeyError, IndexError):\n",
        "                    city = \"Unknown\"\n",
        "            if not country:\n",
        "                try:\n",
        "                    country = geo.raw['display_name'].split(',')[-1].strip()\n",
        "                except (KeyError, IndexError):\n",
        "                    country = \"Unknown\"\n",
        "            # Fallback: Parse state dynamically from display_name if missing or ambiguous\n",
        "            if not state:\n",
        "                try:\n",
        "                    components = geo.raw['display_name'].split(',')\n",
        "                    components = [comp.strip() for comp in components]\n",
        "                    for i in range(len(components) - 1, -1, -1):  # Iterate backwards\n",
        "                        if 'County' not in components[i] and \"United States\" not in components[i]:\n",
        "                            state = components[i]\n",
        "                            break\n",
        "                except (KeyError, IndexError):\n",
        "                    state = \"Unknown\"\n",
        "\n",
        "            # Avoid redundancy: \"Country, Unknown, Country\"\n",
        "            if city == country:\n",
        "                city = \"Unknown\"\n",
        "            if state == country:\n",
        "                state = \"Unknown\"\n",
        "\n",
        "            # Construct the result in the desired format\n",
        "            result = f\"{city}, {state}, {country}\"\n",
        "        else:\n",
        "            result = \"Unknown\"\n",
        "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
        "        result = f\"Error: {e}\"\n",
        "\n",
        "    # Cache the result\n",
        "    location_cache[location] = result\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvEwHw9ut7F",
        "outputId": "0ffd5bfd-ed0e-4e01-81e1-dc773ad5b9d3"
      },
      "id": "rbvEwHw9ut7F",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/drive/MyDrive/Colab_Notebooks/Data\n",
            "No cache file found at /content/drive/MyDrive/Colab_Notebooks/Data/location_cache.json. Starting with an empty cache.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# geocode multiple locations in parallel using ThreadPoolExecutor\n",
        "def batch_geocode(locations):\n",
        "    \"\"\"\n",
        "    Geocode multiple locations in parallel using ThreadPoolExecutor.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:  # max_workers can be adjusted as needed\n",
        "        # Use tqdm to wrap the executor's map method for progress tracking\n",
        "        for result in tqdm(executor.map(geocode_location, locations), \\\n",
        "                           total=len(locations), desc=\"Geocoding Progress\"):\n",
        "            results.append(result)\n",
        "    return results"
      ],
      "metadata": {
        "id": "dX7unUcQvWrg"
      },
      "id": "dX7unUcQvWrg",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the first or last word from a location string\n",
        "def extract_word(location, position=\"first\"):\n",
        "    \"\"\"\n",
        "    Extract the first or last word from a location string.\n",
        "\n",
        "    Args:\n",
        "        location (str): The location string to process.\n",
        "        position (str): 'first' to extract the first word, 'last' to extract the last word.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted word or 'Unknown' if the location is empty or invalid.\n",
        "    \"\"\"\n",
        "    words = location.split()\n",
        "    if words:\n",
        "        return words[0] if position == \"first\" else words[-1]\n",
        "    return \"Unknown\""
      ],
      "metadata": {
        "id": "OojXElKzvuWp"
      },
      "id": "OojXElKzvuWp",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to split geocoded_location into City, State, and Country\n",
        "def split_geocoded_location(location):\n",
        "    if pd.notna(location):\n",
        "        parts = location.split(\",\")\n",
        "        parts = [p.strip() for p in parts]  # remove extra whitespace\n",
        "        city = parts[0] if len(parts) > 0 else \"Unknown\"\n",
        "        state = parts[1] if len(parts) > 1 else \"Unknown\"\n",
        "        country = parts[2] if len(parts) > 2 else \"Unknown\"\n",
        "        return city, state, country\n",
        "    return \"Unknown\", \"Unknown\", \"Unknown\""
      ],
      "metadata": {
        "id": "qCARRM5Mv5R4"
      },
      "id": "qCARRM5Mv5R4",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to cache coordinates\n",
        "try:\n",
        "    # Initialize cache from file\n",
        "    data_dir = determine_data_dir()\n",
        "    coordinate_cache_file = os.path.join(data_dir, \"coordinate_cache.json\")  # Path to cache file\n",
        "    coordinate_cache = load_cache_from_json(coordinate_cache_file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, initialize an empty cache\n",
        "    coordinate_cache = {}\n",
        "\n",
        "# Update get_coordinates function to include caching\n",
        "def get_coordinates(input_type, name, output_as='center', retries=3, delay=5):\n",
        "    \"\"\"\n",
        "    Fetch coordinates of a city/state/country using Nominatim API with caching and retry logic.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): 'country', 'state', or 'city' to specify the type of input.\n",
        "        name (str): Name of the location.\n",
        "        output_as (str): 'center' or 'boundingbox' for coordinate type.\n",
        "        retries (int): Number of retry attempts.\n",
        "        delay (int): Delay between retries in seconds.\n",
        "\n",
        "    Returns:\n",
        "        list: [latitude, longitude]. Returns [0, 0] on failure.\n",
        "    \"\"\"\n",
        "    # Check the cache first\n",
        "    if name in coordinate_cache:\n",
        "        return coordinate_cache[name]\n",
        "\n",
        "    url = f\"http://nominatim.openstreetmap.org/search\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"ColabGeocoder/1.0 (leksea@gmail.com)\"\n",
        "    }\n",
        "    params = {\n",
        "        input_type: name,\n",
        "        \"format\": \"json\",\n",
        "        \"polygon\": 0\n",
        "    }\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if data:\n",
        "                if output_as == 'center':\n",
        "                    result = [float(data[0]['lat']), float(data[0]['lon'])]\n",
        "                elif output_as == 'boundingbox':\n",
        "                    result = [float(coord) for coord in data[0]['boundingbox']]\n",
        "                else:\n",
        "                    result = [0, 0]\n",
        "                # Cache the result\n",
        "                coordinate_cache[name] = result\n",
        "                return result\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching coordinates for {name}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Retrying in {delay} seconds... ({attempt + 1}/{retries})\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Failed to fetch coordinates for {name} after {retries} attempts.\")\n",
        "                return [0, 0]\n",
        "\n",
        "    # Cache failed attempt as [0, 0] to avoid repeated retries\n",
        "    coordinate_cache[name] = [0, 0]\n",
        "    return [0, 0]"
      ],
      "metadata": {
        "id": "DfgwG_Z_xB7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb6c5c9-b93b-44e8-8702-8127f8a76e30"
      },
      "id": "DfgwG_Z_xB7w",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/drive/MyDrive/Colab_Notebooks/Data\n",
            "No cache file found at /content/drive/MyDrive/Colab_Notebooks/Data/coordinate_cache.json. Starting with an empty cache.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add_coordinates_with_progress function\n",
        "def add_coordinates_with_progress(df, city_col='city', state_col='state', country_col='country'):\n",
        "    \"\"\"\n",
        "    Add latitude and longitude coordinates to a DataFrame based on unique combinations\n",
        "    of City, State, and Country, only for rows where these are not 'Unknown'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        city_col (str): Column name for city.\n",
        "        state_col (str): Column name for state.\n",
        "        country_col (str): Column name for country.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated DataFrame with 'Latitude' and 'Longitude' columns.\n",
        "    \"\"\"\n",
        "    # Create a unique DataFrame of City, State, Country combinations\n",
        "    unique_locations = df[(df[city_col] != 'Unknown') &\n",
        "                          (df[state_col] != 'Unknown') &\n",
        "                          (df[country_col] != 'Unknown')][[city_col, state_col, country_col]].drop_duplicates()\n",
        "\n",
        "    # Define a helper function to fetch coordinates\n",
        "    def fetch_coords(row):\n",
        "        location_name = f\"{row[city_col]}, {row[state_col]}, {row[country_col]}\"\n",
        "        return get_coordinates('city', location_name)\n",
        "\n",
        "    # Add Latitude and Longitude columns to the unique locations\n",
        "    unique_locations[['latitude', 'longitude']] = unique_locations.progress_apply(fetch_coords, axis=1, result_type='expand')\n",
        "\n",
        "    # Create a mapping dictionary for efficient lookup\n",
        "    location_to_coords = unique_locations.set_index([city_col, state_col, country_col])[['latitude', 'longitude']].to_dict('index')\n",
        "\n",
        "    # Initialize Latitude and Longitude in the main DataFrame\n",
        "    df['latitude'], df['longitude'] = 0.0, 0.0\n",
        "\n",
        "    # Map coordinates back to the original DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        key = (row[city_col], row[state_col], row[country_col])\n",
        "        if key in location_to_coords:\n",
        "            df.at[index, 'latitude'] = location_to_coords[key]['latitude']\n",
        "            df.at[index, 'longitude'] = location_to_coords[key]['longitude']\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "7I6beD0fqZyw"
      },
      "id": "7I6beD0fqZyw",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine marker color based on tweet count\n",
        "def color(magnitude):\n",
        "    \"\"\"\n",
        "    Returns a color based on the magnitude using a hot-to-cool color map.\n",
        "    \"\"\"\n",
        "    if magnitude >= 2000:\n",
        "        return 'red'  # Hot color for high magnitude\n",
        "    elif 500 <= magnitude < 2000:\n",
        "        return 'orange'  # Medium-hot color\n",
        "    elif 100 <= magnitude < 500:\n",
        "        return 'yellow'  # Neutral color\n",
        "    elif 50 <= magnitude < 100:\n",
        "        return 'lightblue'  # Medium-cool color\n",
        "    else:\n",
        "        return 'blue'  # Cool color for low magnitude\n",
        "\n",
        "# Function to generate the heatmap\n",
        "def generateBaseMap(input_type, df, default_location=[37.774929, -122.419416], default_zoom_start=2):\n",
        "    \"\"\"\n",
        "    Function to generate a heatmap with markers for tweet distribution.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): 'country' or 'city' to specify the type of heatmap.\n",
        "        df (pd.DataFrame): DataFrame containing latitude, longitude, tweet count, and name.\n",
        "        default_location (list): Default map center location as [latitude, longitude].\n",
        "        default_zoom_start (int): Default zoom level for the map.\n",
        "\n",
        "    Returns:\n",
        "        folium.Map: A folium map object with heatmap and markers.\n",
        "    \"\"\"\n",
        "    # Initialize the base map\n",
        "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
        "    marker_cluster = plugins.MarkerCluster().add_to(base_map)\n",
        "\n",
        "    # Add the heatmap\n",
        "    HeatMap(data=df[['latitude', 'longitude']].values.tolist(), radius=20, max_zoom=13).add_to(base_map)\n",
        "\n",
        "    # Add markers with popups\n",
        "    for lat, lon, tweet_count, name in zip(df['latitude'], df['longitude'], df['tweet_count'], df.iloc[:, 0]):\n",
        "        popup_content = folium.Popup(f\"{name}<br>{tweet_count} tweets\", max_width=300)\n",
        "        folium.Marker(\n",
        "            location=[lat, lon],\n",
        "            popup=popup_content,\n",
        "            icon=folium.Icon(color=color(tweet_count), icon='twitter', prefix='fa')\n",
        "        ).add_to(marker_cluster)\n",
        "\n",
        "    # Add a colormap legend\n",
        "    min_val, max_val = df['tweet_count'].min(), df['tweet_count'].max()\n",
        "    colormap = cm.LinearColormap(colors=['blue', 'yellow', 'red'], vmin=min_val, vmax=max_val)\n",
        "    colormap.caption = f\"{input_type.title()} Distribution of COVID-19 Tweets\"\n",
        "    colormap.add_to(base_map)\n",
        "\n",
        "    return base_map"
      ],
      "metadata": {
        "id": "KS8rd-tnyU53"
      },
      "id": "KS8rd-tnyU53",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract readable source from HTML content\n",
        "def extract_html_source(source_text):\n",
        "    \"\"\"\n",
        "    Extracts the readable text (e.g., 'Twitter for Android') from the source HTML string.\n",
        "\n",
        "    Args:\n",
        "        source_text (str): The raw HTML string in the source column.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned, readable source text.\n",
        "    \"\"\"\n",
        "    return re.sub(r'<.*?>', '', str(source_text)).strip()  # remove HTML tags"
      ],
      "metadata": {
        "id": "wGVwTKwrwBN8"
      },
      "id": "wGVwTKwrwBN8",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a dictionary mapping emoticons to emojis\n",
        "emoticon_to_emoji = {\n",
        "    \":)\": \"😊\",\n",
        "    \":D\": \"😃\",\n",
        "    \":(\": \"☹️\",\n",
        "    \":/\": \"😕\",\n",
        "    \":P\": \"😛\",\n",
        "    \";)\": \"😉\",\n",
        "    \":'(\": \"😢\",\n",
        "    \":o\": \"😮\",\n",
        "    \":|\": \"😐\",\n",
        "    \":))\": \"😂\",\n",
        "    \":*\": \"😘\",\n",
        "    \"xD\": \"😆\"\n",
        "}\n",
        "\n",
        "def replace_emoticons_with_emojis(text):\n",
        "    \"\"\"\n",
        "    Replaces emoticons in the text with corresponding emojis.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with emoticons replaced by emojis.\n",
        "    \"\"\"\n",
        "    # Use regex to find and replace emoticons\n",
        "    for emoticon, emoji in emoticon_to_emoji.items():\n",
        "        text = re.sub(re.escape(emoticon), emoji, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "clWtFweFUJir"
      },
      "id": "clWtFweFUJir",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# big cleaning function\n",
        "def process_tweet_data(tweet, emoji_list=None):\n",
        "    \"\"\"\n",
        "    Processes a tweet to extract mentions, hashtags, retweets, emojis, hyperlinks, and cleaned text.\n",
        "\n",
        "    Args:\n",
        "        tweet (str): The raw tweet text.\n",
        "        emoji_list (list): List of emojis to extract. Defaults to keys of UNICODE_EMOJI.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with extracted components and cleaned text.\n",
        "    \"\"\"\n",
        "    # ensure input is a string\n",
        "    tweet = str(tweet)\n",
        "    # default emoji list if not provided\n",
        "    if emoji_list is None:\n",
        "        emoji_list = list(UNICODE_EMOJI.keys())\n",
        "\n",
        "    # extract mentions\n",
        "    mentions = re.findall(r'@\\w+', tweet)\n",
        "\n",
        "    # extract hashtags\n",
        "    hashtags = re.findall(r'#\\w+', tweet)\n",
        "\n",
        "    # check for retweets and extract username after RT\n",
        "    retweets = re.findall(r'^RT @(\\w+)', tweet)\n",
        "    retweet_user = retweets[0] if retweets else None\n",
        "\n",
        "    # extract hyperlinks before emoji\n",
        "    hyperlinks = re.findall(r'https?://[^\\s]+|www\\.[^\\s]+', tweet)\n",
        "    tweet = re.sub(r'https?://[^\\s]+|www\\.[^\\s]+', '', tweet) #Remove URL\n",
        "\n",
        "    # extract emojis\n",
        "    tweet = replace_emoticons_with_emojis(tweet)\n",
        "    emojis = ''.join([char for char in tweet if char in emoji_list])\n",
        "\n",
        "    # replace emojis with text\n",
        "    tweet = emoji.demojize(tweet).replace('_', ' ')\n",
        "\n",
        "    # remove mentions, hashtags, retweets, emojis, and hyperlinks from the tweet\n",
        "    cleaned_text = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
        "    cleaned_text = re.sub(r'#\\w+', '', cleaned_text)  # Remove hashtags\n",
        "    cleaned_text = re.sub(r'^RT', '', cleaned_text)   # Remove retweets\n",
        "    cleaned_text = ''.join([char for char in cleaned_text if char not in emoji_list])  # Remove emojis\n",
        "\n",
        "    # remove special characters, extra spaces, numbers\n",
        "    cleaned_text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
        "\n",
        "    #lowercase the text, remove numbers\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return {\n",
        "        'mentions': mentions,\n",
        "        'hashtags': hashtags,\n",
        "        'retweets': retweets,\n",
        "        'emojis': emojis,\n",
        "        'hyperlinks': hyperlinks,\n",
        "        'cleaned_text': cleaned_text\n",
        "    }"
      ],
      "metadata": {
        "id": "RvxDCHurw1Lg"
      },
      "id": "RvxDCHurw1Lg",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure process_tweet_data is modified to utilize GPU if applicable\n",
        "def extract_cleaned_text(tweet):\n",
        "    \"\"\"Helper function to extract 'cleaned_text' from process_tweet_data.\"\"\"\n",
        "    return process_tweet_data(tweet)['cleaned_text']\n",
        "\n",
        "# GPU-optimized batch processing for tweet cleaning\n",
        "def clean_tweets_with_progress_parallel(df, text_col='original_text', batch_size=256, num_processes=6):\n",
        "    \"\"\"\n",
        "    Cleans tweet data in parallel using ThreadPoolExecutor and GPU acceleration.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing tweets.\n",
        "        text_col (str): Column name containing the tweet text.\n",
        "        batch_size (int): Number of tweets to process in a batch.\n",
        "        num_processes (int): Number of processes to use for parallel execution.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Updated Series with cleaned text.\n",
        "    \"\"\"\n",
        "    texts = df[text_col].tolist()\n",
        "    cleaned_texts = []\n",
        "\n",
        "    # Process texts in batches\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Cleaning Tweets in Batches\"):\n",
        "        batch = texts[i:i + batch_size]\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
        "            # Submit tasks to executor\n",
        "            futures = {executor.submit(extract_cleaned_text, tweet): tweet for tweet in batch}\n",
        "\n",
        "            # Collect results\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    cleaned_texts.append(future.result())\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing tweet: {e}\")\n",
        "                    cleaned_texts.append(None)  # Append None for failed tweets\n",
        "\n",
        "    # Return as a pandas Series\n",
        "    return pd.Series(cleaned_texts, index=df.index, name='cleaned_text')"
      ],
      "metadata": {
        "id": "novHso6vyxEy"
      },
      "id": "novHso6vyxEy",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words and lemmatizer initialization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(df, text_column):\n",
        "    \"\"\"\n",
        "    Preprocesses text data by tokenizing, removing stop words, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the text column.\n",
        "        text_column (str): Name of the column with raw text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with processed text columns.\n",
        "    \"\"\"\n",
        "    # Convert 'text_column' to string type to ensure it contains only string values\n",
        "    df[text_column] = df[text_column].astype(str)\n",
        "    # Tokenization\n",
        "    df['tokenized_text'] = df[text_column].progress_apply(word_tokenize)\n",
        "\n",
        "    # Stop word removal\n",
        "    df['filtered_tokens'] = df['tokenized_text'].progress_apply(\n",
        "        lambda tokens: [word for word in tokens if word.lower() not in stop_words]\n",
        "    )\n",
        "\n",
        "    # Lemmatization\n",
        "    df['lemmatized_tokens'] = df['filtered_tokens'].progress_apply(\n",
        "        lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    )\n",
        "\n",
        "    # Combine tokens back into text for n-gram computation\n",
        "    df['lemmatized_text'] = df['lemmatized_tokens'].progress_apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "E0GoMgylvvTK"
      },
      "id": "E0GoMgylvvTK",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ngrams(df, text_column, ngram_range=(2, 3), max_features=5000):\n",
        "    \"\"\"\n",
        "    Computes n-grams from preprocessed text data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the processed text column.\n",
        "        text_column (str): Name of the column with preprocessed text.\n",
        "        ngram_range (tuple): Range of n-grams to compute (e.g., (2, 3) for bi-grams and tri-grams).\n",
        "        max_features (int): Maximum number of n-gram features.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Sparse matrix of n-grams and fitted CountVectorizer.\n",
        "    \"\"\"\n",
        "    # Compute n-grams using CountVectorizer\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    ngram_matrix = vectorizer.fit_transform(df[text_column])\n",
        "\n",
        "    return ngram_matrix, vectorizer"
      ],
      "metadata": {
        "id": "vVqQglx_uZJA"
      },
      "id": "vVqQglx_uZJA",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute BERT embeddings for a single text\n",
        "def bert_embedding(text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Compute BERT embeddings for a single text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text.\n",
        "        tokenizer (BertTokenizer): Tokenizer for BERT.\n",
        "        model (BertModel): Pre-trained BERT model.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Sentence embedding (average of token embeddings).\n",
        "    \"\"\"\n",
        "    # Move tokenizer inputs to GPU if available\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Average token embeddings to get sentence embedding\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "# Parallelized function to compute BERT embeddings\n",
        "def compute_bert_embeddings_parallel(df, text_column, tokenizer, model, max_workers=4):\n",
        "    \"\"\"\n",
        "    Compute BERT embeddings for all texts in a DataFrame using ThreadPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the text column.\n",
        "        text_column (str): Name of the column with text to embed.\n",
        "        tokenizer (BertTokenizer): Tokenizer for BERT.\n",
        "        model (BertModel): Pre-trained BERT model.\n",
        "        max_workers (int): Maximum number of threads.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of BERT embeddings.\n",
        "    \"\"\"\n",
        "    # Move model to GPU if available\n",
        "    model.to(device)\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    texts = df[text_column].tolist()\n",
        "    embeddings = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit tasks to the executor\n",
        "        futures = {executor.submit(bert_embedding, text, tokenizer, model): text for text in texts}\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing BERT embeddings\"):\n",
        "            try:\n",
        "                embeddings.append(future.result())\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing text: {e}\")\n",
        "                embeddings.append(None)  # Append None for failed embeddings\n",
        "\n",
        "    # Filter out None values and convert to PyTorch tensor\n",
        "    embeddings = [emb for emb in embeddings if emb is not None]\n",
        "    return torch.tensor(embeddings)"
      ],
      "metadata": {
        "id": "q_yhHxcjtGg6"
      },
      "id": "q_yhHxcjtGg6",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings into a dictionary.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the GloVe file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping words to their vector embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "7gl069T-ASgi"
      },
      "id": "7gl069T-ASgi",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, embeddings_dict, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Retrieve the embedding for a given word or a zero vector if the word is not in the embeddings.\n",
        "\n",
        "    Args:\n",
        "        word (str): Input word.\n",
        "        embeddings_dict (dict): Dictionary of pre-trained embeddings.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Embedding vector for the word.\n",
        "    \"\"\"\n",
        "    return embeddings_dict.get(word, np.zeros(embedding_dim))"
      ],
      "metadata": {
        "id": "J3ddMTmSAlUE"
      },
      "id": "J3ddMTmSAlUE",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentence_embedding(sentence, embeddings_dict, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Compute sentence embedding by averaging word embeddings.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): Input sentence.\n",
        "        embeddings_dict (dict): Dictionary of pre-trained embeddings.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Sentence embedding.\n",
        "    \"\"\"\n",
        "    tokens = sentence.split()  # Tokenize the sentence\n",
        "    token_embeddings = [get_embedding(token, embeddings_dict, embedding_dim) for token in tokens]\n",
        "    return np.mean(token_embeddings, axis=0) if token_embeddings else np.zeros(embedding_dim)"
      ],
      "metadata": {
        "id": "5D10sbgTA1AJ"
      },
      "id": "5D10sbgTA1AJ",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting confusion matrix\n",
        "def plot_normalized_confusion_matrix(y_true, y_pred, class_labels, title=\"Confusion Matrix\", cmap=\"Blues\"):\n",
        "    \"\"\"\n",
        "    Plots a normalized confusion matrix (percentages).\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): Ground truth labels.\n",
        "        y_pred (array-like): Predicted labels.\n",
        "        class_labels (list): List of class label names.\n",
        "        title (str): Title of the confusion matrix plot.\n",
        "        cmap (str): Colormap for the heatmap.\n",
        "    \"\"\"\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true class)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # Plot the normalized confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_labels)\n",
        "    disp.plot(cmap=cmap, ax=ax, colorbar=True)\n",
        "\n",
        "    plt.title(f'{title} (in %)')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3dpH8uXq9e48"
      },
      "id": "3dpH8uXq9e48",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supplemental GWO-ABC feature selection\n",
        "def fitness_function(selected_features, X, y, model, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Fitness function to evaluate feature subset.\n",
        "\n",
        "    Args:\n",
        "        selected_features (array-like): Binary array representing selected features.\n",
        "        X (array-like): Feature matrix (NumPy or CuPy).\n",
        "        y (array-like): Target labels (NumPy or CuPy).\n",
        "        model (scikit-learn or cuML model): Classifier to evaluate.\n",
        "        use_gpu (bool): Whether to use GPU-compatible arrays.\n",
        "\n",
        "    Returns:\n",
        "        float: Negative cross-validation accuracy (to minimize).\n",
        "    \"\"\"\n",
        "    X_selected = X[:, selected_features == 1]\n",
        "    if X_selected.shape[1] == 0:\n",
        "        return 1.0  # High error for invalid subsets\n",
        "\n",
        "    # Split the data\n",
        "    if use_gpu:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_selected, y, test_size=0.3, random_state=42, shuffle=True)\n",
        "    else:\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_selected.get(), y.get(), test_size=0.3, random_state=42)\n",
        "\n",
        "    # Fit the model and compute accuracy\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    score = accuracy_score(y_val, y_pred)\n",
        "    return -score\n",
        "\n",
        "\n",
        "def initialize_population(n_wolves, n_features, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Initialize the population of wolves.\n",
        "    \"\"\"\n",
        "    random_func = cp.random if use_gpu else np.random\n",
        "    return random_func.randint(2, size=(n_wolves, n_features))\n",
        "\n",
        "\n",
        "def update_gwo(population, alpha, beta, delta, a, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Update wolves' positions using GWO.\n",
        "    \"\"\"\n",
        "    array_lib = cp if use_gpu else np\n",
        "    n_wolves, n_features = population.shape\n",
        "    new_population = array_lib.copy(population)\n",
        "\n",
        "    for i in range(n_wolves):\n",
        "        for j in range(n_features):\n",
        "            r1, r2 = array_lib.random.rand(), array_lib.random.rand()\n",
        "            A = 2 * a * r1 - a\n",
        "            C = 2 * r2\n",
        "            D_alpha = array_lib.abs(C * alpha[j] - population[i, j])\n",
        "            X1 = alpha[j] - A * D_alpha\n",
        "\n",
        "            D_beta = array_lib.abs(C * beta[j] - population[i, j])\n",
        "            X2 = beta[j] - A * D_beta\n",
        "\n",
        "            D_delta = array_lib.abs(C * delta[j] - population[i, j])\n",
        "            X3 = delta[j] - A * D_delta\n",
        "\n",
        "            # Update position\n",
        "            new_population[i, j] = (X1 + X2 + X3) / 3\n",
        "\n",
        "    return (new_population > 0.5).astype(int)\n",
        "\n",
        "\n",
        "def hybrid_gwo_abc(X, y, model, n_wolves=10, n_iter=20, use_gpu=False):\n",
        "    \"\"\"\n",
        "    Hybrid GWO-ABC optimizer.\n",
        "\n",
        "    Args:\n",
        "        X (array-like): Feature matrix (NumPy or CuPy).\n",
        "        y (array-like): Target labels (NumPy or CuPy).\n",
        "        model: Classifier to evaluate.\n",
        "        n_wolves (int): Number of wolves in the population.\n",
        "        n_iter (int): Number of iterations.\n",
        "        use_gpu (bool): Whether to use GPU-compatible arrays.\n",
        "\n",
        "    Returns:\n",
        "        array-like: Best feature subset (binary array).\n",
        "    \"\"\"\n",
        "    array_lib = cp if use_gpu else np\n",
        "    n_features = X.shape[1]\n",
        "    population = initialize_population(n_wolves, n_features, use_gpu=use_gpu)\n",
        "\n",
        "    a = 2  # Linear reduction coefficient\n",
        "\n",
        "    for t in range(n_iter):\n",
        "        # Compute fitness\n",
        "        fitness = array_lib.array([fitness_function(wolf, X, y, model, use_gpu=use_gpu) for wolf in population])\n",
        "\n",
        "        # Sort wolves\n",
        "        sorted_indices = array_lib.argsort(fitness)\n",
        "        population = population[sorted_indices]\n",
        "\n",
        "        # Update alpha, beta, delta\n",
        "        alpha, beta, delta = population[0], population[1], population[2]\n",
        "\n",
        "        # Update positions using GWO\n",
        "        population = update_gwo(population, alpha, beta, delta, a, use_gpu=use_gpu)\n",
        "\n",
        "        # Local ABC optimization\n",
        "        for i in range(3):\n",
        "            local_search_wolf = array_lib.copy(population[i])\n",
        "            for _ in range(array_lib.random.randint(1, 5)):\n",
        "                feature_idx = array_lib.random.randint(0, n_features)\n",
        "                local_search_wolf[feature_idx] = 1 - local_search_wolf[feature_idx]\n",
        "            if fitness_function(local_search_wolf, X, y, model, use_gpu=use_gpu) < fitness[i]:\n",
        "                population[i] = local_search_wolf\n",
        "\n",
        "        # Reduce exploration coefficient\n",
        "        a -= 2 / n_iter\n",
        "\n",
        "    return population[0]"
      ],
      "metadata": {
        "id": "Cwk8177mDDDw"
      },
      "id": "Cwk8177mDDDw",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Data Loading\n",
        "\n",
        "This section focuses on loading the COVID-19 Twitter dataset into a Pandas DataFrame for analysis. We will perform the following steps:\n",
        "\n",
        "1. **Data Directory Determination:** Identify the appropriate directory where the data files are stored, considering both local and cloud (Colab) environments.\n",
        "\n",
        "2. **File Identification:** Locate all CSV files within the determined data directory using the `glob` library.\n",
        "\n",
        "3. **Data Loading and Concatenation:**\n",
        "    - Read each CSV file into a separate Pandas DataFrame using `pd.read_csv`.\n",
        "    - Concatenate all the individual DataFrames into a single DataFrame named `data` using `pd.concat`.\n",
        "    - Print information about the loaded data, including its dimensions and a preview of the first few rows.\n",
        "\n",
        "4. **Error Handling:** Implement error handling mechanisms to address potential issues during file loading, such as missing files or incorrect file formats."
      ],
      "metadata": {
        "id": "hnoqoMM-gr0F"
      },
      "id": "hnoqoMM-gr0F"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8Ir_LkSJww",
        "outputId": "654aee82-8972-49f2-ea7e-e0d0768aa3bd"
      },
      "id": "xm8Ir_LkSJww",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the files\n",
        "# determine the data directory\n",
        "# data_dir would be global variable\n",
        "data_dir = determine_data_dir()\n",
        "\n",
        "# locate all CSV files in the determined directory\n",
        "files_pattern = os.path.join(data_dir, \"*.csv\")\n",
        "files = glob.glob(files_pattern)\n",
        "\n",
        "# check if files are found\n",
        "if not files:\n",
        "    print(f\"No CSV files found in directory: {data_dir}\")\n",
        "else:\n",
        "     # load and inspect each file\n",
        "    dfs = []  # to store valid DataFrames\n",
        "    for file in files:\n",
        "        try:\n",
        "            # load the DataFrame\n",
        "            df = pd.read_csv(file)\n",
        "            rows, cols = df.shape\n",
        "            print(f\"File: {file} | Rows: {rows}, Columns: {cols}\")\n",
        "\n",
        "            # skip empty files or files with no columns\n",
        "            if rows == 0 or cols == 0:\n",
        "                print(f\"Skipping empty or invalid file: {file}\")\n",
        "                continue\n",
        "\n",
        "            # append to list if valid\n",
        "            dfs.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file}: {e}\")\n",
        "\n",
        "    # concatenate all valid DataFrames\n",
        "    if dfs:\n",
        "        data = pd.concat(dfs, ignore_index=True)\n",
        "        print(f\"Data loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
        "        print(data.head())\n",
        "    else:\n",
        "        print(\"No valid DataFrames to concatenate.\")"
      ],
      "metadata": {
        "id": "Gn7kjSy5DgOe",
        "outputId": "689676a0-b809-40b5-9fa5-b897f9980309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Gn7kjSy5DgOe",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/drive/MyDrive/Colab_Notebooks/Data\n",
            "File: /content/drive/MyDrive/Colab_Notebooks/Data/Covid-19 Twitter Dataset (Aug-Sep 2020).csv | Rows: 120509, Columns: 17\n",
            "File: /content/drive/MyDrive/Colab_Notebooks/Data/Covid-19 Twitter Dataset (Apr-Jun 2020).csv | Rows: 143903, Columns: 17\n",
            "File: /content/drive/MyDrive/Colab_Notebooks/Data/Covid-19 Twitter Dataset (Apr-Jun 2021).csv | Rows: 147475, Columns: 17\n",
            "Data loaded successfully with 411887 rows and 17 columns.\n",
            "             id  created_at  \\\n",
            "0  1.300000e+18  2020-08-20   \n",
            "1  1.300000e+18  2020-08-20   \n",
            "2  1.300000e+18  2020-08-20   \n",
            "3  1.300000e+18  2020-08-20   \n",
            "4  1.300000e+18  2020-08-20   \n",
            "\n",
            "                                              source  \\\n",
            "0  <a href=\"http://twitter.com/download/android\" ...   \n",
            "1  <a href=\"http://twitter.com/download/android\" ...   \n",
            "2  <a href=\"http://twitter.com/download/android\" ...   \n",
            "3  <a href=\"https://about.twitter.com/products/tw...   \n",
            "4  <a href=\"http://twitter.com/download/android\" ...   \n",
            "\n",
            "                                       original_text lang  favorite_count  \\\n",
            "0  RT @RobertAlai: 91-year-old Ex-Vice President ...   en             0.0   \n",
            "1  RT @cnnphilippines: BREAKING: The Department o...   en             0.0   \n",
            "2  RT @latestly: #SidharthShukla Helps Out Fan Wh...   en             0.0   \n",
            "3  Lending Club loan originations down 90% ... bu...   en             0.0   \n",
            "4  RT @OpIndia_com: Curious case of ‘United Natio...   en             0.0   \n",
            "\n",
            "   retweet_count original_author                  hashtags  \\\n",
            "0          100.0       kvn_kegan                       NaN   \n",
            "1           38.0      puTOPinamo                       NaN   \n",
            "2            0.0     DevSidheart  SidharthShukla, Covid_19   \n",
            "3           13.0   Chris_Skinner                       NaN   \n",
            "4          286.0  Yashodhara1010                       NaN   \n",
            "\n",
            "               user_mentions                   place  \\\n",
            "0                 RobertAlai          Nairobi, Kenya   \n",
            "1             cnnphilippines                     NaN   \n",
            "2                   latestly                Sidheart   \n",
            "3                        NaN  ÜT: 51.511924,-0.22414   \n",
            "4  OpIndia_com, LekhakAnurag                   India   \n",
            "\n",
            "                                         clean_tweet  compound  neg    neu  \\\n",
            "0  year old ex vice presid moodi awori land inter...    0.0000  0.0  1.000   \n",
            "1  break depart health report peopl caught covid1...    0.0000  0.0  1.000   \n",
            "2  help fan request help arrang bed posit father ...    0.7717  0.0  0.476   \n",
            "3                     lend club loan origin hey bank    0.0000  0.0  1.000   \n",
            "4  curiou case unit nation ngo appreci kingdom ma...    0.0000  0.0  1.000   \n",
            "\n",
            "     pos sentiment  \n",
            "0  0.000       neu  \n",
            "1  0.000       neu  \n",
            "2  0.524       pos  \n",
            "3  0.000       neu  \n",
            "4  0.000       neu  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading GloVe embeddings (large file, might take a few minutes)\n",
        "download_and_extract_glove(data_dir)"
      ],
      "metadata": {
        "id": "S4iThCR6amwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93c8897-5276-4ae6-c82d-44a6e70f8cd2"
      },
      "id": "S4iThCR6amwh",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe embeddings are already present in the data directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Basic Data Understanding\n",
        "\n",
        "This section focuses on gaining an initial understanding of the dataset's structure and contents.\n",
        "We will perform the following steps:\n",
        "1. **Data Overview:** Examine the basic information about the dataset, including the number of rows, columns, and data types.\n",
        "2. **Column Selection:** Identify and select the relevant columns for the analysis.\n"
      ],
      "metadata": {
        "id": "75eceDA73wLf"
      },
      "id": "75eceDA73wLf"
    },
    {
      "cell_type": "code",
      "source": [
        "# get general info about the dataset\n",
        "data.info()"
      ],
      "metadata": {
        "id": "tsrDMCYiEfUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f6e9af-34b2-4d36-ed06-19124bf3978d"
      },
      "id": "tsrDMCYiEfUL",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 411887 entries, 0 to 411886\n",
            "Data columns (total 17 columns):\n",
            " #   Column           Non-Null Count   Dtype  \n",
            "---  ------           --------------   -----  \n",
            " 0   id               411883 non-null  float64\n",
            " 1   created_at       411885 non-null  object \n",
            " 2   source           411587 non-null  object \n",
            " 3   original_text    411885 non-null  object \n",
            " 4   lang             411884 non-null  object \n",
            " 5   favorite_count   411884 non-null  float64\n",
            " 6   retweet_count    411884 non-null  float64\n",
            " 7   original_author  411884 non-null  object \n",
            " 8   hashtags         97775 non-null   object \n",
            " 9   user_mentions    295207 non-null  object \n",
            " 10  place            293775 non-null  object \n",
            " 11  clean_tweet      409915 non-null  object \n",
            " 12  compound         411887 non-null  float64\n",
            " 13  neg              411887 non-null  float64\n",
            " 14  neu              411887 non-null  float64\n",
            " 15  pos              411887 non-null  float64\n",
            " 16  sentiment        411887 non-null  object \n",
            "dtypes: float64(7), object(10)\n",
            "memory usage: 53.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# info about the numeric columns\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "l8cIT_Ib3_UI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "db65d846-b87a-42dc-bee3-19616d58c08d"
      },
      "id": "l8cIT_Ib3_UI",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  favorite_count  retweet_count       compound  \\\n",
              "count  4.118830e+05   411884.000000  411884.000000  411887.000000   \n",
              "mean   1.324197e+18        0.216726    1585.174163       0.008415   \n",
              "std    5.902218e+16        6.332250    9423.896052       0.370853   \n",
              "min    1.250000e+18        0.000000       0.000000      -0.992500   \n",
              "25%    1.260000e+18        0.000000       1.000000      -0.102700   \n",
              "50%    1.310000e+18        0.000000      15.000000       0.000000   \n",
              "75%    1.395011e+18        0.000000     243.000000       0.226300   \n",
              "max    1.409140e+18     2923.000000  416923.000000       0.980500   \n",
              "\n",
              "                 neg            neu            pos  \n",
              "count  411887.000000  411887.000000  411887.000000  \n",
              "mean        0.090920       0.807021       0.102052  \n",
              "std         0.152717       0.200474       0.157080  \n",
              "min         0.000000       0.000000       0.000000  \n",
              "25%         0.000000       0.667000       0.000000  \n",
              "50%         0.000000       0.819000       0.000000  \n",
              "75%         0.180000       1.000000       0.200000  \n",
              "max         1.000000       1.000000       1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-492b2399-c43c-4ca1-a5bc-8a6099d6b370\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.118830e+05</td>\n",
              "      <td>411884.000000</td>\n",
              "      <td>411884.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.324197e+18</td>\n",
              "      <td>0.216726</td>\n",
              "      <td>1585.174163</td>\n",
              "      <td>0.008415</td>\n",
              "      <td>0.090920</td>\n",
              "      <td>0.807021</td>\n",
              "      <td>0.102052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.902218e+16</td>\n",
              "      <td>6.332250</td>\n",
              "      <td>9423.896052</td>\n",
              "      <td>0.370853</td>\n",
              "      <td>0.152717</td>\n",
              "      <td>0.200474</td>\n",
              "      <td>0.157080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.250000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.992500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.260000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.102700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.310000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.819000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.395011e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>0.226300</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.409140e+18</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>416923.000000</td>\n",
              "      <td>0.980500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-492b2399-c43c-4ca1-a5bc-8a6099d6b370')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-492b2399-c43c-4ca1-a5bc-8a6099d6b370 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-492b2399-c43c-4ca1-a5bc-8a6099d6b370');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-00c87ae9-ca17-4aee-94ab-13915d4ad6ad\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-00c87ae9-ca17-4aee-94ab-13915d4ad6ad')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-00c87ae9-ca17-4aee-94ab-13915d4ad6ad button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.023959288097569e+17,\n        \"min\": 411883.0,\n        \"max\": 1.40914e+18,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.324197404747178e+18,\n          1.31e+18,\n          411883.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145478.61324083302,\n        \"min\": 0.0,\n        \"max\": 411884.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21672606850472464,\n          2923.0,\n          6.332250463859156\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 190993.20623830325,\n        \"min\": 0.0,\n        \"max\": 416923.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1585.1741631138864,\n          15.0,\n          411884.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"compound\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145624.02059975133,\n        \"min\": -0.9925,\n        \"max\": 411887.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.008414607647243052,\n          0.0,\n          411887.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145623.97348716456,\n        \"min\": 0.0,\n        \"max\": 411887.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          411887.0,\n          0.09092004360419241,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145623.8184359867,\n        \"min\": 0.0,\n        \"max\": 411887.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          411887.0,\n          0.8070205736039257,\n          0.819\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145623.9716943737,\n        \"min\": 0.0,\n        \"max\": 411887.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          411887.0,\n          0.10205184431652371,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select subset of tweets we'll be working with:\n",
        "cols_to_keep = ['id', 'source', 'created_at', 'original_text', \\\n",
        "                'lang', 'favorite_count', 'retweet_count', 'original_author', \\\n",
        "                'hashtags', 'user_mentions', 'place', 'sentiment', 'compound', 'pos', 'neu', 'neg']\n",
        "tweets_df = data[cols_to_keep].copy()\n",
        "# drop NaN ids\n",
        "tweets_df.dropna(subset=['id'], inplace=True)"
      ],
      "metadata": {
        "id": "p7qKCkF41B4j"
      },
      "id": "p7qKCkF41B4j",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_categorical_vals(tweets_df)"
      ],
      "metadata": {
        "id": "Lgl3O8AV0loS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c06f76-744d-4ee5-d8f8-de45d471e383"
      },
      "id": "Lgl3O8AV0loS",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'source' has unique values: ['<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>'\n",
            " '<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>'\n",
            " '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n",
            " ...\n",
            " '<a href=\"https://help.twitter.com/en/using-twitter/how-to-tweet#source-labels\" rel=\"nofollow\">VaxBlr</a>'\n",
            " '<a href=\"https://help.twitter.com/en/using-twitter/how-to-tweet#source-labels\" rel=\"nofollow\">PNI Publishing App Twitter</a>'\n",
            " '<a href=\"http://www.rugbylad.ie\" rel=\"nofollow\">RugbyLAD.ie</a>']\n",
            "Column 'created_at' has unique values: ['2020-08-20' '2020-08-21' '2020-08-22' '2020-08-23' '2020-08-24'\n",
            " '2020-08-26' '2020-08-28' '2020-08-29' '2020-08-30' '2020-08-31'\n",
            " '2020-09-01' '2020-09-02' '2020-09-03' '2020-09-06' '2020-09-12'\n",
            " '2020-09-13' '2020-09-15' '2020-09-17' '2020-09-19' '2020-09-20'\n",
            " '2020-09-22' '2020-09-23' '2020-09-24' '2020-09-25' '2020-09-26'\n",
            " '2020-09-27' '2020-09-28' '2020-09-29' '2020-09-30' '2020-10-01'\n",
            " '2020-10-02' '2020-10-03' '2020-10-04' nan '2020-10-05' '2020-10-06'\n",
            " '2020-10-07' '2020-10-08' '2020-10-09' '2020-10-10' '2020-10-11'\n",
            " '2020-10-12' '2020-10-13' '2020-10-14' '2020-10-15' '2020-10-16'\n",
            " '2020-10-17' '2020-10-18' '2020-10-19' '2020-10-20' '2020-04-19'\n",
            " '2020-04-22' '2020-04-23' '2020-04-24' '2020-04-25' '2020-04-26'\n",
            " '2020-04-27' '2020-04-28' '2020-04-29' '2020-04-30' '2020-05-01'\n",
            " '2020-05-02' '2020-05-03' '2020-05-04' '2020-05-05' '2020-05-06'\n",
            " '2020-05-07' '2020-05-08' '2020-05-09' '2020-05-10' '2020-05-11'\n",
            " '2020-05-12' '2020-05-13' '2020-05-14' '2020-05-15' '2020-05-16'\n",
            " '2020-05-17' '2020-05-18' '2020-05-19' '2020-05-20' '2020-05-22'\n",
            " '2020-05-23' '2020-05-24' '2020-05-25' '2020-05-26' '2020-05-27'\n",
            " '2020-05-28' '2020-05-29' '2020-05-30' '2020-05-31' '2020-06-01'\n",
            " '2020-06-03' '2020-06-04' '2020-06-05' '2020-06-06' '2020-06-07'\n",
            " '2020-06-08' '2020-06-09' '2020-06-10' '2020-06-11' '2020-06-12'\n",
            " '2020-06-13' '2020-06-14' '2020-06-15' '2020-06-17' '2020-06-19'\n",
            " '2020-06-20' '2021-04-26' '2021-04-27' '2021-04-28' '2021-04-29'\n",
            " '2021-04-30' '2021-05-01' '2021-05-03' '2021-05-04' '2021-05-05'\n",
            " '2021-05-07' '2021-05-08' '2021-05-09' '2021-05-10' '2021-05-11'\n",
            " '2021-05-13' '2021-05-14' '2021-05-15' '2021-05-16' '2021-05-18'\n",
            " '2021-05-19' '2021-05-20' '2021-05-21' '2021-05-23' '2021-05-25'\n",
            " '2021-05-26' '2021-05-27' '2021-05-28' '2021-05-29' '2021-05-30'\n",
            " '2021-06-01' '2021-06-02' '2021-06-03' '2021-06-05' '2021-06-07'\n",
            " '2021-06-08' '2021-06-09' '2021-06-11' '2021-06-12' '2021-06-13'\n",
            " '2021-06-14' '2021-06-15' '2021-06-16' '2021-06-17' '2021-06-18'\n",
            " '2021-06-19' '2021-06-24' '2021-06-26' '2021-06-27']\n",
            "Column 'original_text' has unique values: ['RT @RobertAlai: 91-year-old Ex-Vice President Moody Awori Lands Inter County Covid-19 Committee Role https://t.co/bslXxeMpGD'\n",
            " 'RT @cnnphilippines: BREAKING: The Department of Health reports 4,339 more people caught COVID-19, pushing the national case count to 178,02…'\n",
            " 'RT @latestly: #SidharthShukla Helps Out Fan Who Requested Him To Help Arrange A Bed For Her #Covid_19 Positive Father (View Tweet)\\n@sidhart…'\n",
            " ...\n",
            " 'Goal is reached: 40 per cent of Chinese get Covid-19 jab before end of June https://t.co/ssYlXXNsZJ via @scmpnews'\n",
            " 'Covid-19 and Uganda’s looming political crisis https://t.co/slV34Ntt8M'\n",
            " 'RT @MirzaNasara: Alhamdolillah, we got our second dose of COVID-19 vaccine may Allah keep everyone safe and healthy. Amin https://t.co/sE3l…']\n",
            "Column 'lang' has unique values: ['en' nan]\n",
            "Column 'original_author' has unique values: ['kvn_kegan' 'puTOPinamo' 'DevSidheart' ... 'CochingcoA' 'JOBBWIRE'\n",
            " 'Life_Devotee']\n",
            "Column 'hashtags' has unique values: [nan 'SidharthShukla, Covid_19' 'COVID19, Glasgow, Covid_19' ...\n",
            " 'Nagaland, Vaccination, Covidvaccination, covidemergencyindia, CovidResources, Covidindiahelp'\n",
            " 'Ibunisasi' 'Jawa42, SiriusWhite']\n",
            "Column 'user_mentions' has unique values: ['RobertAlai' 'cnnphilippines' 'latestly' ... 'samacharnews_in'\n",
            " 'sinonome_umi' 'MirzaNasara']\n",
            "Column 'place' has unique values: ['Nairobi, Kenya' nan 'Sidheart' ... 'Strangeland!'\n",
            " 'Airport Heights Campus, NUST.' 'قائدتحریک جدید۔تربیت ۔اشاعت  ']\n",
            "Column 'sentiment' has unique values: ['neu' 'pos' 'neg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Data Cleaning: Date"
      ],
      "metadata": {
        "id": "psy8TJ7r6Lit"
      },
      "id": "psy8TJ7r6Lit"
    },
    {
      "cell_type": "code",
      "source": [
        "#rename date column for clarity and convert to date\n",
        "tweets_df.rename(columns={'created_at': 'date'}, inplace=True)\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'])"
      ],
      "metadata": {
        "id": "bSzfHyP60pCc"
      },
      "id": "bSzfHyP60pCc",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow by exploratory data analysis:\n",
        "* What were daily tweet patters?\n",
        "* What were the top 20 days with most tweets?  "
      ],
      "metadata": {
        "id": "5wNyc5aZ8KYM"
      },
      "id": "5wNyc5aZ8KYM"
    },
    {
      "cell_type": "code",
      "source": [
        "# exploratory analysis: plot number of tweets per day\n",
        "# group by date and count tweets\n",
        "tweets_per_day = tweets_df.groupby(tweets_df['date'].dt.date)['id'].count()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(tweets_per_day.index, tweets_per_day.values)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.title(\"Number of Tweets Per Day\")\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KWByJUQb8HnZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9dcde106-880c-4ff0-a7a4-8be9b3cd9b8a"
      },
      "id": "KWByJUQb8HnZ",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exploratory analysis: list the top 20 days with the most tweets\n",
        "# rename columns for clarity\n",
        "tweets_per_day.columns = ['date', 'tweet_count']\n",
        "\n",
        "# sort by tweet count and get the top 20\n",
        "top_20_days = tweets_per_day.sort_values(ascending=False).head(20)\n",
        "\n",
        "# display the result\n",
        "print(top_20_days)"
      ],
      "metadata": {
        "id": "tKhB8-_k6oVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ef84e0c-a6f4-4c0d-ce8d-8fbad5d2b530"
      },
      "id": "tKhB8-_k6oVd",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date\n",
            "2021-06-03    7229\n",
            "2020-05-04    7165\n",
            "2021-05-25    7114\n",
            "2020-09-27    7040\n",
            "2020-09-30    7028\n",
            "2020-04-25    6178\n",
            "2020-10-01    6166\n",
            "2020-05-09    5857\n",
            "2020-05-23    5600\n",
            "2020-04-24    5323\n",
            "2021-06-01    5175\n",
            "2021-06-09    5158\n",
            "2020-05-22    5102\n",
            "2021-05-08    5032\n",
            "2020-05-05    4949\n",
            "2021-05-05    4886\n",
            "2020-05-25    4804\n",
            "2021-05-26    4750\n",
            "2021-05-07    4586\n",
            "2020-05-06    4575\n",
            "Name: id, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Data Cleaning: Language\n",
        "Before coming up with the strategy for each column, we'll check the contents of categorical data and the distributiuon of NaNs.\n",
        "\n",
        "* It would make sence that fields like ```hashtags``` and ```user_mentions``` would have missing values and we'll leave it as it is.\n",
        "* We'll check the ```lang``` and ```place``` columns.\n"
      ],
      "metadata": {
        "id": "Br52XIjx-PYG"
      },
      "id": "Br52XIjx-PYG"
    },
    {
      "cell_type": "code",
      "source": [
        "# number of NaNs in lang\n",
        "sum(tweets_df.lang.isna())"
      ],
      "metadata": {
        "id": "ZPUde-jG8BO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79b9752-23e5-4983-b38e-5226c29bc882"
      },
      "id": "ZPUde-jG8BO2",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the tweet text\n",
        "tweets_df[tweets_df.lang.isna()]"
      ],
      "metadata": {
        "id": "NjCQDN2y-0Ww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "5ed0392e-4a92-4aa7-b89f-e51cbc177d24"
      },
      "id": "NjCQDN2y-0Ww",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id                                             source  \\\n",
              "77157   1.310000e+18                                                NaN   \n",
              "357288  1.400490e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "\n",
              "             date                                      original_text lang  \\\n",
              "77157         NaT                                                NaN  NaN   \n",
              "357288 2021-06-03  @santoshmt7666 @globaltimesnews The COVID-19 d...  NaN   \n",
              "\n",
              "        favorite_count  retweet_count original_author hashtags user_mentions  \\\n",
              "77157              NaN            NaN             NaN      NaN           NaN   \n",
              "357288             NaN            NaN             NaN      NaN           NaN   \n",
              "\n",
              "       place sentiment  compound  pos    neu    neg  \n",
              "77157    NaN       neu    0.0000  0.0  1.000  0.000  \n",
              "357288   NaN       neg   -0.5994  0.0  0.606  0.394  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-510ca737-ad2e-4534-9dbc-4818a808b66e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>date</th>\n",
              "      <th>original_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>place</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>compound</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>77157</th>\n",
              "      <td>1.310000e+18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357288</th>\n",
              "      <td>1.400490e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2021-06-03</td>\n",
              "      <td>@santoshmt7666 @globaltimesnews The COVID-19 d...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.5994</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-510ca737-ad2e-4534-9dbc-4818a808b66e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-510ca737-ad2e-4534-9dbc-4818a808b66e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-510ca737-ad2e-4534-9dbc-4818a808b66e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-af0a9730-93ea-4878-8580-a937bdf0a1f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af0a9730-93ea-4878-8580-a937bdf0a1f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-af0a9730-93ea-4878-8580-a937bdf0a1f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only 3 rows where language is missing are missing the original text, so we'll discard them."
      ],
      "metadata": {
        "id": "peqUklAw_XG3"
      },
      "id": "peqUklAw_XG3"
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows where 'lang' is NaN\n",
        "tweets_df = tweets_df.dropna(subset=['lang'])\n",
        "\n",
        "# verify the changes\n",
        "print(f\"Number of NaNs in 'lang' after dropping: {sum(tweets_df.lang.isna())}\")\n",
        "\n",
        "# drop the lang column from the df\n",
        "tweets_df = tweets_df.drop(columns=['lang'])"
      ],
      "metadata": {
        "id": "UVXP7Mdl-_JG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86bbebc-9c7a-4de9-b989-149085e971a7"
      },
      "id": "UVXP7Mdl-_JG",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs in 'lang' after dropping: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df"
      ],
      "metadata": {
        "id": "BSX8Xlp4QYRL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "outputId": "62bc9cd5-5eb5-4b5f-879e-5bb38c2faf14"
      },
      "id": "BSX8Xlp4QYRL",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id                                             source  \\\n",
              "0       1.300000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "1       1.300000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "2       1.300000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "3       1.300000e+18  <a href=\"https://about.twitter.com/products/tw...   \n",
              "4       1.300000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "...              ...                                                ...   \n",
              "411882  1.409140e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "411883  1.409140e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "411884  1.409130e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "411885  1.409130e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "411886  1.409130e+18  <a href=\"http://twitter.com/download/iphone\" r...   \n",
              "\n",
              "             date                                      original_text  \\\n",
              "0      2020-08-20  RT @RobertAlai: 91-year-old Ex-Vice President ...   \n",
              "1      2020-08-20  RT @cnnphilippines: BREAKING: The Department o...   \n",
              "2      2020-08-20  RT @latestly: #SidharthShukla Helps Out Fan Wh...   \n",
              "3      2020-08-20  Lending Club loan originations down 90% ... bu...   \n",
              "4      2020-08-20  RT @OpIndia_com: Curious case of ‘United Natio...   \n",
              "...           ...                                                ...   \n",
              "411882 2021-06-27  Just as the northern hemisphere summer season ...   \n",
              "411883 2021-06-27  COVID 19 TRENDS 2020-2021 https://t.co/tjKMaht...   \n",
              "411884 2021-06-27  Goal is reached: 40 per cent of Chinese get Co...   \n",
              "411885 2021-06-27  Covid-19 and Uganda’s looming political crisis...   \n",
              "411886 2021-06-27  RT @MirzaNasara: Alhamdolillah, we got our sec...   \n",
              "\n",
              "        favorite_count  retweet_count  original_author  \\\n",
              "0                  0.0          100.0        kvn_kegan   \n",
              "1                  0.0           38.0       puTOPinamo   \n",
              "2                  0.0            0.0      DevSidheart   \n",
              "3                  0.0           13.0    Chris_Skinner   \n",
              "4                  0.0          286.0   Yashodhara1010   \n",
              "...                ...            ...              ...   \n",
              "411882             0.0            0.0   jmhamiltonblog   \n",
              "411883             0.0            0.0       CochingcoA   \n",
              "411884             0.0            0.0  DavidGr07837209   \n",
              "411885             0.0            0.0         JOBBWIRE   \n",
              "411886             0.0          221.0     Life_Devotee   \n",
              "\n",
              "                        hashtags              user_mentions  \\\n",
              "0                            NaN                 RobertAlai   \n",
              "1                            NaN             cnnphilippines   \n",
              "2       SidharthShukla, Covid_19                   latestly   \n",
              "3                            NaN                        NaN   \n",
              "4                            NaN  OpIndia_com, LekhakAnurag   \n",
              "...                          ...                        ...   \n",
              "411882                       NaN                        NaN   \n",
              "411883                       NaN                    YouTube   \n",
              "411884                       NaN                   SCMPNews   \n",
              "411885                       NaN                        NaN   \n",
              "411886                       NaN                MirzaNasara   \n",
              "\n",
              "                                 place sentiment  compound    pos    neu  \\\n",
              "0                       Nairobi, Kenya       neu    0.0000  0.000  1.000   \n",
              "1                                  NaN       neu    0.0000  0.000  1.000   \n",
              "2                             Sidheart       pos    0.7717  0.524  0.476   \n",
              "3               ÜT: 51.511924,-0.22414       neu    0.0000  0.000  1.000   \n",
              "4                                India       neu    0.0000  0.000  1.000   \n",
              "...                                ...       ...       ...    ...    ...   \n",
              "411882                        🇨🇦🇺🇸🏳️‍🌈       neu    0.0000  0.000  1.000   \n",
              "411883   Airport Heights Campus, NUST.       neu    0.0000  0.000  1.000   \n",
              "411884  Melbourne, Victoria, Australia       pos    0.0258  0.099  0.901   \n",
              "411885                 Kampala, Uganda       neg   -0.2263  0.000  0.678   \n",
              "411886   قائدتحریک جدید۔تربیت ۔اشاعت         pos    0.4404  0.195  0.805   \n",
              "\n",
              "          neg  \n",
              "0       0.000  \n",
              "1       0.000  \n",
              "2       0.000  \n",
              "3       0.000  \n",
              "4       0.000  \n",
              "...       ...  \n",
              "411882  0.000  \n",
              "411883  0.000  \n",
              "411884  0.000  \n",
              "411885  0.322  \n",
              "411886  0.000  \n",
              "\n",
              "[411881 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5f2cd5f-0c57-4fe1-9997-59ee70abc4b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>date</th>\n",
              "      <th>original_text</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>place</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>compound</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.300000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-08-20</td>\n",
              "      <td>RT @RobertAlai: 91-year-old Ex-Vice President ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>kvn_kegan</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RobertAlai</td>\n",
              "      <td>Nairobi, Kenya</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.300000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-08-20</td>\n",
              "      <td>RT @cnnphilippines: BREAKING: The Department o...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>puTOPinamo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cnnphilippines</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.300000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-08-20</td>\n",
              "      <td>RT @latestly: #SidharthShukla Helps Out Fan Wh...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>DevSidheart</td>\n",
              "      <td>SidharthShukla, Covid_19</td>\n",
              "      <td>latestly</td>\n",
              "      <td>Sidheart</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.7717</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.300000e+18</td>\n",
              "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
              "      <td>2020-08-20</td>\n",
              "      <td>Lending Club loan originations down 90% ... bu...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>Chris_Skinner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ÜT: 51.511924,-0.22414</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.300000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-08-20</td>\n",
              "      <td>RT @OpIndia_com: Curious case of ‘United Natio...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>286.0</td>\n",
              "      <td>Yashodhara1010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OpIndia_com, LekhakAnurag</td>\n",
              "      <td>India</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411882</th>\n",
              "      <td>1.409140e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2021-06-27</td>\n",
              "      <td>Just as the northern hemisphere summer season ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>jmhamiltonblog</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>🇨🇦🇺🇸🏳️‍🌈</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411883</th>\n",
              "      <td>1.409140e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2021-06-27</td>\n",
              "      <td>COVID 19 TRENDS 2020-2021 https://t.co/tjKMaht...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>CochingcoA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>Airport Heights Campus, NUST.</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411884</th>\n",
              "      <td>1.409130e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2021-06-27</td>\n",
              "      <td>Goal is reached: 40 per cent of Chinese get Co...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>DavidGr07837209</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SCMPNews</td>\n",
              "      <td>Melbourne, Victoria, Australia</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.0258</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.901</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411885</th>\n",
              "      <td>1.409130e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2021-06-27</td>\n",
              "      <td>Covid-19 and Uganda’s looming political crisis...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>JOBBWIRE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Kampala, Uganda</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.2263</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411886</th>\n",
              "      <td>1.409130e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>2021-06-27</td>\n",
              "      <td>RT @MirzaNasara: Alhamdolillah, we got our sec...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>Life_Devotee</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MirzaNasara</td>\n",
              "      <td>قائدتحریک جدید۔تربیت ۔اشاعت</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.4404</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.805</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>411881 rows × 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5f2cd5f-0c57-4fe1-9997-59ee70abc4b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5f2cd5f-0c57-4fe1-9997-59ee70abc4b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5f2cd5f-0c57-4fe1-9997-59ee70abc4b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7baba3e4-8028-4ba0-a630-5e4509df9967\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7baba3e4-8028-4ba0-a630-5e4509df9967')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7baba3e4-8028-4ba0-a630-5e4509df9967 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7abbeeee-5637-4c79-9f6c-eb7721d29b82\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tweets_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7abbeeee-5637-4c79-9f6c-eb7721d29b82 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tweets_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweets_df"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rename place column to location for clarity\n",
        "tweets_df.rename(columns={'place': 'location'}, inplace=True)"
      ],
      "metadata": {
        "id": "VDUco94l_msI"
      },
      "id": "VDUco94l_msI",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count of NaNs\n",
        "sum(tweets_df.location.isna())"
      ],
      "metadata": {
        "id": "IRZGktsdBCU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f344b04-bb47-44ad-e9f9-6dbab4d5e8ce"
      },
      "id": "IRZGktsdBCU8",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118109"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fill location NaNs with Unknown\n",
        "tweets_df.fillna(value={'location':'Unknown'}, inplace=True)"
      ],
      "metadata": {
        "id": "DElV9lreBbuR"
      },
      "id": "DElV9lreBbuR",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9 Data Cleaning: Location\n",
        "\n",
        "Here we'll see if location can be cleaned up for further visualization. We'll start with checking the locations with at least 10 tweets.  "
      ],
      "metadata": {
        "id": "QltD9yKyKM2i"
      },
      "id": "QltD9yKyKM2i"
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate value counts for the 'location' column\n",
        "location_counts = tweets_df.location.value_counts()\n",
        "\n",
        "# filter to keep locations with at least 20 occurrences\n",
        "filtered_locations = location_counts[location_counts >= 10]\n",
        "\n",
        "# fisplay the filtered results\n",
        "print(filtered_locations)"
      ],
      "metadata": {
        "id": "v7mKo2s6BNWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369ec3ec-11bb-44ac-f1ef-a639f4b9d5bc"
      },
      "id": "v7mKo2s6BNWZ",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "location\n",
            "Unknown                                   118146\n",
            "United States                               4539\n",
            "India                                       4283\n",
            "London                                      2646\n",
            "London, England                             2638\n",
            "                                           ...  \n",
            "Riyadh, Saudi Arabia                          10\n",
            "Agra                                          10\n",
            "Welland/Pelham/Port Colborne/Wainfleet        10\n",
            "Utrecht                                       10\n",
            "Gotham                                        10\n",
            "Name: count, Length: 2748, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll check the locations with the special characters (non-alphanumeric that are not the ```-,.```)."
      ],
      "metadata": {
        "id": "yJbEJxdeTk6I"
      },
      "id": "yJbEJxdeTk6I"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to create a boolean mask\n",
        "special_chars_mask = tweets_df['location'].apply(has_special_chars)\n",
        "\n",
        "# filter the DataFrame and get value counts\n",
        "locations_with_special_chars = tweets_df.loc[special_chars_mask, \\\n",
        "                                             'location'].value_counts()\n",
        "\n",
        "# display the result\n",
        "print(locations_with_special_chars)"
      ],
      "metadata": {
        "id": "q7Kf4O_JSdrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f90199-3b6c-4cfd-c019-bf176fdaeef0"
      },
      "id": "q7Kf4O_JSdrL",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "location\n",
            "भारत                              211\n",
            "കേരളം                             155\n",
            "नई दिल्ली, भारत                    87\n",
            "she/her                            81\n",
            "मुंबई, भारत                        80\n",
            "                                 ... \n",
            "God’s Country....                   1\n",
            "Oakville/Liberty Village/FMTY✈      1\n",
            "Photo : Blue Mountains              1\n",
            "#ConstitutionalSweatEquity 🗽        1\n",
            "USA  🇺🇲                             1\n",
            "Name: count, Length: 16221, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll try to salvage the locations by using the first or the last word in a multi-word location. Otherwise, set it to the 'Unknown'."
      ],
      "metadata": {
        "id": "nXn706c4VKH0"
      },
      "id": "nXn706c4VKH0"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a boolean mask for one-word locations, those we won't be able to automatically ID\n",
        "one_word_mask = locations_with_special_chars.index.str.split().str.len() == 1\n",
        "one_word_locations = locations_with_special_chars[one_word_mask].index\n",
        "\n",
        "# replace those locations in the original DataFrame with 'Unknown'\n",
        "tweets_df['location'] = tweets_df['location'].replace(one_word_locations, 'Unknown')"
      ],
      "metadata": {
        "id": "NzY9yF_YUIKv"
      },
      "id": "NzY9yF_YUIKv",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The brief explanation for ```geocode_location()``` function:\n",
        "* It utilizes the geocoding API. We give it a location (string), and it returns a best(first) guess for the location. For example, if I give it ```Paris```, it will return ```Paris, France``` and not ```Paris, Texas```.\n",
        "* It executes slowly."
      ],
      "metadata": {
        "id": "Xd9iepNmWjnz"
      },
      "id": "Xd9iepNmWjnz"
    },
    {
      "cell_type": "code",
      "source": [
        "# test it on few entries\n",
        "print(geocode_location('New York, NY'))             # Expected: \"New York, New York, United States\"\n",
        "print(geocode_location('Toronto, Ontario'))         # Expected: \"Toronto, Ontario, Canada\"\n",
        "print(geocode_location('India'))                    # Expected: \"Unknown, Unknown, India\"\n",
        "print(geocode_location('USA'))                      # Expected: \"Unknown, Unknown, United States\""
      ],
      "metadata": {
        "id": "J7zg7xziM9yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e854d9-2b60-40cd-d507-53a5d7e408ec"
      },
      "id": "J7zg7xziM9yi",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "City of New York, New York, United States\n",
            "Toronto, Ontario, Canada\n",
            "Unknown, Unknown, India\n",
            "Unknown, None, United States\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we implemented parallel optimization in ```batch_geocode()```."
      ],
      "metadata": {
        "id": "ygHbOge8X0EU"
      },
      "id": "ygHbOge8X0EU"
    },
    {
      "cell_type": "code",
      "source": [
        "# before applying, do minor cleaning: replace 'unknown' with 'Unknown'\n",
        "tweets_df.location.replace('unknown', 'Unknown')"
      ],
      "metadata": {
        "id": "TIdeQFu-PxRd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "b5136c22-b652-4df7-e493-2f80ac994722"
      },
      "id": "TIdeQFu-PxRd",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         Nairobi, Kenya\n",
              "1                                Unknown\n",
              "2                               Sidheart\n",
              "3                 ÜT: 51.511924,-0.22414\n",
              "4                                  India\n",
              "                       ...              \n",
              "411882                           Unknown\n",
              "411883     Airport Heights Campus, NUST.\n",
              "411884    Melbourne, Victoria, Australia\n",
              "411885                   Kampala, Uganda\n",
              "411886     قائدتحریک جدید۔تربیت ۔اشاعت  \n",
              "Name: location, Length: 411881, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nairobi, Kenya</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sidheart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ÜT: 51.511924,-0.22414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>India</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411882</th>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411883</th>\n",
              "      <td>Airport Heights Campus, NUST.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411884</th>\n",
              "      <td>Melbourne, Victoria, Australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411885</th>\n",
              "      <td>Kampala, Uganda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411886</th>\n",
              "      <td>قائدتحریک جدید۔تربیت ۔اشاعت</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>411881 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate value counts for the 'location' column\n",
        "location_counts = tweets_df.location.value_counts()\n",
        "\n",
        "# filter to keep locations with at least 10 occurrences\n",
        "filtered_locations = location_counts[location_counts >= 10]\n",
        "\n",
        "# remove 'Unknown' from filtered_locations\n",
        "filtered_locations_known = filtered_locations[filtered_locations.index != 'Unknown']\n",
        "\n",
        "# create a boolean mask for locations in filtered_locations_known\n",
        "mask = tweets_df['location'].isin(filtered_locations_known.index)\n",
        "\n",
        "# get unique locations from the filtered DataFrame\n",
        "unique_locations = tweets_df.loc[mask, 'location'].unique()"
      ],
      "metadata": {
        "id": "bNgR_KIOWndU"
      },
      "id": "bNgR_KIOWndU",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will apply batch geocoding requests. To respect the Nominatim API, we put a sleep(0.5) before each request and longer timeout (8s).\n",
        "The code would run ~40 minutes."
      ],
      "metadata": {
        "id": "j6l1lw7GfENf"
      },
      "id": "j6l1lw7GfENf"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply geocode_location only to the selected locations in parallel\n",
        "geocoded_results = dict(zip(unique_locations, batch_geocode(unique_locations)))\n",
        "\n",
        "# map the results back to the DataFrame\n",
        "tweets_df.loc[mask, 'geocoded_location'] = tweets_df.loc[mask, 'location'].map(geocoded_results)"
      ],
      "metadata": {
        "id": "aKl7CyUDNBMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "894a7e97-b89a-4dcf-8d43-0b13ee6c8234"
      },
      "id": "aKl7CyUDNBMf",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Geocoding Progress:  32%|███▏      | 857/2713 [14:12<30:46,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-459c82b06e7a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# apply geocode_location only to the selected locations in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgeocoded_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_geocode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# map the results back to the DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'geocoded_location'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeocoded_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-adf30d47d103>\u001b[0m in \u001b[0;36mbatch_geocode\u001b[0;34m(locations)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# max_workers can be adjusted as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Use tqdm to wrap the executor's map method for progress tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         for result in tqdm(executor.map(geocode_location, locations), \\\n\u001b[0m\u001b[1;32m     10\u001b[0m                            total=len(locations), desc=\"Geocoding Progress\"):\n\u001b[1;32m     11\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save location cache after processing\n",
        "save_cache_to_json(location_cache)"
      ],
      "metadata": {
        "id": "6pTfHQSQiun3"
      },
      "id": "6pTfHQSQiun3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check value counts\n",
        "tweets_df['geocoded_location'].value_counts()"
      ],
      "metadata": {
        "id": "4HJk5wdYNxyJ"
      },
      "id": "4HJk5wdYNxyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue location data cleaning. For all undecoded ```geocoded_location```, try to apply first then last words of ```location```."
      ],
      "metadata": {
        "id": "rmtVoVogWUIG"
      },
      "id": "rmtVoVogWUIG"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mask for rows where 'geocoded_location' is 'Unknown' and where its known\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "known_mask = tweets_df['geocoded_location'] != 'Unknown'\n",
        "# extract FIRST word from 'location' for 'Unknown' geocoded rows and batch process\n",
        "if unknown_mask.any():\n",
        "    first_words = tweets_df.loc[unknown_mask, 'location'].apply(lambda loc: extract_word(loc, position=\"first\"))\n",
        "\n",
        "    unique_first_words = first_words.unique()\n",
        "    unique_first_words_set = set(unique_first_words)\n",
        "    # extract known locations where geocoded_location is not 'Unknown'\n",
        "    known_locations = tweets_df.loc[known_mask, 'location']\n",
        "    # tokenize each location into words and find intersection with unique_first_words_set\n",
        "    known_words_in_locations = known_locations.str.split().apply(lambda words: unique_first_words_set.intersection(words))\n",
        "\n",
        "    # extract matches\n",
        "    matched_words = {word for words in known_words_in_locations for word in words}\n",
        "\n",
        "    # perform geocoding for matched first words\n",
        "    first_word_results = batch_geocode(matched_words)\n",
        "    # create a mapping of unique first words to geocoded results\n",
        "    first_word_mapping = dict(zip(matched_words, first_word_results))\n",
        "\n",
        "    # map the results back to the DataFrame\n",
        "    tweets_df.loc[unknown_mask, 'geocoded_location'] = first_words.map(first_word_mapping)"
      ],
      "metadata": {
        "id": "m2GOQnD6R2uc"
      },
      "id": "m2GOQnD6R2uc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update the mask for remaining 'Unknown' rows\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "known_mask = tweets_df['geocoded_location'] != 'Unknown'\n",
        "\n",
        "# extract LAST word from 'location' for remaining 'Unknown' rows and batch process\n",
        "if unknown_mask.any():\n",
        "    last_words = tweets_df.loc[unknown_mask, 'location'].apply(lambda loc: extract_word(loc, position=\"last\"))\n",
        "    unique_last_words = last_words.unique()\n",
        "    unique_last_words_set = set(last_words)\n",
        "\n",
        "    known_locations = tweets_df.loc[known_mask, 'location']\n",
        "\n",
        "     # tokenize each location into words and find intersection with unique_first_words_set\n",
        "    known_words_in_locations = known_locations.str.split().apply(lambda words: unique_last_words_set.intersection(words))\n",
        "\n",
        "    # extract matches\n",
        "    matched_words = {word for words in known_words_in_locations for word in words}\n",
        "\n",
        "    # perform geocoding for unique last words\n",
        "    last_word_results = batch_geocode(matched_words)  # Ensure batch_geocode is defined\n",
        "    # create a mapping of unique last words to geocoded results\n",
        "    last_word_mapping = dict(zip(matched_words, last_word_results))\n",
        "\n",
        "    # map the results back to the DataFrame\n",
        "    tweets_df.loc[unknown_mask, 'geocoded_location'] = last_words.map(last_word_mapping)"
      ],
      "metadata": {
        "id": "l5uLD4NlRufX"
      },
      "id": "l5uLD4NlRufX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if still 'Unknown', give up\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "if unknown_mask.any():\n",
        "    print(f\"Giving up on {unknown_mask.sum()} locations. Could not geocode these entries.\")"
      ],
      "metadata": {
        "id": "gRDg2uJDRxtq"
      },
      "id": "gRDg2uJDRxtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update location cache after processing\n",
        "save_cache_to_json(location_cache)"
      ],
      "metadata": {
        "id": "k_SZ2FctjEb9"
      },
      "id": "k_SZ2FctjEb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill location NaNs with Unknown\n",
        "tweets_df.fillna(value={'geocoded_location':'Unknown'}, inplace=True)"
      ],
      "metadata": {
        "id": "2orl_xq7Yazz"
      },
      "id": "2orl_xq7Yazz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll use ```split_geocoded_location()``` to split the geocoded_location into three columns: ```country```, ```state```, and ```city```."
      ],
      "metadata": {
        "id": "6qrp8WChaMCI"
      },
      "id": "6qrp8WChaMCI"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to create separate columns\n",
        "tweets_df[['city', 'state', 'country']] = tweets_df['geocoded_location'].apply(\n",
        "    lambda loc: pd.Series(split_geocoded_location(loc))\n",
        ")\n",
        "# display the DataFrame with new columns\n",
        "tweets_df[['geocoded_location', 'city', 'state', 'country']]"
      ],
      "metadata": {
        "id": "9virTNaVavK_"
      },
      "id": "9virTNaVavK_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minor fix: capitalization\n",
        "# Capitalize all entries in 'City', 'State', and 'Country' columns\n",
        "tweets_df['city'] = tweets_df['city'].str.title()\n",
        "tweets_df['state'] = tweets_df['state'].str.title()\n",
        "tweets_df['country'] = tweets_df['country'].str.title()"
      ],
      "metadata": {
        "id": "f3iWZ3sxbXCA"
      },
      "id": "f3iWZ3sxbXCA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll perform location EDA: tweet histograms by Country (Worldwide), by City (Worldwide), By  State(US), followed by additional cleaning."
      ],
      "metadata": {
        "id": "G3d4li4aCFQo"
      },
      "id": "G3d4li4aCFQo"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Country EDA: Remove 'Unknown' countries and plot histogram for top 20 (ordered)\n",
        "known_countries = tweets_df[tweets_df['country'] != 'Unknown']\n",
        "top_20_countries = known_countries['country'].value_counts().nlargest(20)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=top_20_countries.index,\n",
        "    x=top_20_countries.values,\n",
        "    hue=top_20_countries.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 Countries (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Country')\n",
        "for i, v in enumerate(top_20_countries.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KLMPIu6CBVMT"
      },
      "id": "KLMPIu6CBVMT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Remove 'Unknown' cities and plot histogram for top 50 (ordered)\n",
        "known_cities = tweets_df[tweets_df['city'] != 'Unknown']\n",
        "top_50_cities = known_cities['city'].value_counts().nlargest(50)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    y=top_50_cities.index,\n",
        "    x=top_50_cities.values,\n",
        "    hue=top_50_cities.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 50 Cities (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('City')\n",
        "for i, v in enumerate(top_50_cities.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4RIdk7xSCbBV"
      },
      "id": "4RIdk7xSCbBV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out 'Unknown' states and keep only US entries\n",
        "us_states = tweets_df[(tweets_df['country'] == 'United States') & (tweets_df['state'] != 'Unknown')]\n",
        "\n",
        "# Count occurrences of each state and get the top 20\n",
        "state_counts = us_states['state'].value_counts().nlargest(20)\n",
        "\n",
        "# Plot the distribution of tweets by the top 20 US states\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=state_counts.index,\n",
        "    x=state_counts.values,\n",
        "    hue=state_counts.index,  # Use state as hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Prevent splitting bars\n",
        "    legend=False  # Suppress the legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 US States (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('US State')\n",
        "\n",
        "# Add labels to the bars\n",
        "for i, v in enumerate(state_counts.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UKlTA2rvD9FJ"
      },
      "id": "UKlTA2rvD9FJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minor fixes: Move Florida, California, Texas, Michigan  from City to State and rename None in state to Unknown\n",
        "states_as_cities = ['Florida', 'California', 'Texas', 'Michigan']\n",
        "mask_states_as_cities = (tweets_df['city'].isin(states_as_cities)) & (tweets_df['country'] == 'United States')\n",
        "\n",
        "# update State and City columns for these entries\n",
        "tweets_df.loc[mask_states_as_cities, 'state'] = tweets_df.loc[mask_states_as_cities, 'city']\n",
        "tweets_df.loc[mask_states_as_cities, 'city'] = 'Unknown'\n",
        "\n",
        "# rename all 'None' in State to 'Unknown'\n",
        "tweets_df['state'] = tweets_df['state'].replace('None', 'Unknown')\n",
        "\n",
        "# fix \"Alba/Scotland\" entries\n",
        "mask_alba_scotland = tweets_df['city'].str.contains('Alba/Scotland', case=False, na=False)\n",
        "\n",
        "# update City and Country for Alba/Scotland\n",
        "tweets_df.loc[mask_alba_scotland, 'city'] = 'Unknown'\n",
        "tweets_df.loc[mask_alba_scotland, 'country'] = 'Scotland'"
      ],
      "metadata": {
        "id": "CpKOlC0mEcFv"
      },
      "id": "CpKOlC0mEcFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# redo the city plot:\n",
        "# 2. Remove 'Unknown' cities and plot histogram for top 50 (ordered)\n",
        "known_cities = tweets_df[tweets_df['city'] != 'Unknown']\n",
        "top_50_cities = known_cities['city'].value_counts().nlargest(50)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    y=top_50_cities.index,\n",
        "    x=top_50_cities.values,\n",
        "    hue=top_50_cities.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 50 Cities (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('City')\n",
        "for i, v in enumerate(top_50_cities.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zpEycPkWcHrz"
      },
      "id": "zpEycPkWcHrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# redo the states plot:\n",
        "# Filter out 'Unknown' states and keep only US entries\n",
        "us_states = tweets_df[(tweets_df['country'] == 'United States') & (tweets_df['state'] != 'Unknown')]\n",
        "\n",
        "# Count occurrences of each state and get the top 20\n",
        "state_counts = us_states['state'].value_counts().nlargest(20)\n",
        "\n",
        "# Plot the distribution of tweets by the top 20 US states\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=state_counts.index,\n",
        "    x=state_counts.values,\n",
        "    hue=state_counts.index,  # Use state as hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Prevent splitting bars\n",
        "    legend=False  # Suppress the legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 US States (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('US State')\n",
        "\n",
        "# Add labels to the bars\n",
        "for i, v in enumerate(state_counts.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iN_jW3o1b_M4"
      },
      "id": "iN_jW3o1b_M4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USA tweets per day\n",
        "usa_tweets = tweets_df[tweets_df['country'] == 'United States']\n",
        "\n",
        "# group by date and count tweets per day\n",
        "tweets_per_day = usa_tweets.groupby(usa_tweets['date'].dt.date).size().reset_index(name='Total Tweets')\n",
        "tweets_per_day.columns = ['Date', 'Total Tweets']\n",
        "\n",
        "# bar plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(\n",
        "    data=tweets_per_day,\n",
        "    x='Date',\n",
        "    y='Total Tweets',\n",
        "    hue='Date',  # Assign the x variable to hue\n",
        "    palette='viridis',\n",
        "    legend=False  # Disable the legend\n",
        ")\n",
        "\n",
        "# customize x-axis ticks to show every third date\n",
        "xticks = plt.gca().get_xticks()\n",
        "xtick_labels = tweets_per_day['Date'].astype(str).values\n",
        "plt.xticks(\n",
        "    ticks=xticks[::3],  # show every third tick\n",
        "    labels=xtick_labels[::3],  # use corresponding labels\n",
        "    rotation=90  # rotate 45 deg for better visibility\n",
        ")\n",
        "\n",
        "plt.xticks(rotation=90)  # rotate x-axis labels for better readability\n",
        "plt.title('Tweets Per Day in the USA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.tight_layout()  # Adjust layout to prevent label overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZiT_qKnDdz5K"
      },
      "id": "ZiT_qKnDdz5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Optional] Adding lattitude, longitude (long runtime)**. This piece of code will run for about 1 hr, it's going to retrieve geo coordinates (latitude, longitude) for all cities in the dataset. Strongly recommend executing the following ```save_cache_to_json()``` to save the work and downloading the file if you're running the notebook from the cloud.\n",
        "\n",
        "For this particular dataset or scope of the business problem it might not worth it, but for advanced analysis or futire work it might be a useful EDA section to utilize.  "
      ],
      "metadata": {
        "id": "fo8-mFt8rMaz"
      },
      "id": "fo8-mFt8rMaz"
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = add_coordinates_with_progress(tweets_df)"
      ],
      "metadata": {
        "id": "F23LeybSxdp2"
      },
      "id": "F23LeybSxdp2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save coordinate cache to json\n",
        "save_cache_to_json(coordinate_cache, \"coordinate_cache.json\")"
      ],
      "metadata": {
        "id": "i4f-X-xWq1CE"
      },
      "id": "i4f-X-xWq1CE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the map if it's not found\n",
        "try:\n",
        "    # Try to open the file\n",
        "    heatmap_file = os.path.join(data_dir, \"heatmap_city.html\")\n",
        "    with open(heatmap_file, \"r\") as file:\n",
        "        print(\"heatmap_city.html already exists. No need to recompute.\")\n",
        "        heatmap_city = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"heatmap.html does not exist. Generating the heatmap...\")\n",
        "    # Call the function to compute the heatmap\n",
        "    # Filter rows where both latitude and longitude are not 0.0\n",
        "    filtered_tweets_df = tweets_df[(tweets_df['latitude'] != 0.0) & (tweets_df['longitude'] != 0.0)]\n",
        "    # Create tweet_count column\n",
        "    filtered_tweets_df.loc[:, 'tweet_count'] = filtered_tweets_df.groupby('city')['city'].transform('count')\n",
        "    heatmap_city = generateBaseMap(input_type=\"city\", df=filtered_tweets_df)\n",
        "    heatmap_city.save(heatmap_file)"
      ],
      "metadata": {
        "id": "vTzcb11aBfOn"
      },
      "id": "vTzcb11aBfOn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the map directly in the notebook (might not work)\n",
        "display(HTML(heatmap_city))"
      ],
      "metadata": {
        "id": "4odEWaXzHfPI"
      },
      "id": "4odEWaXzHfPI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.10 Data Cleaning: Source\n",
        "Next, we'll replace the ```source``` values with readable meaningful values using ```extract_html_source()``` function that will extract the value between the HTML tags."
      ],
      "metadata": {
        "id": "w4PgWcupwYB6"
      },
      "id": "w4PgWcupwYB6"
    },
    {
      "cell_type": "code",
      "source": [
        "# replace the source with the meaningful value\n",
        "tweets_df.source.value_counts()"
      ],
      "metadata": {
        "id": "XWS6nPsxn2UX"
      },
      "id": "XWS6nPsxn2UX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to the 'source' column\n",
        "tweets_df['source'] = tweets_df['source'].apply(extract_html_source)\n",
        "# identify sources with counts less than 100\n",
        "source_counts = tweets_df['source'].value_counts()\n",
        "low_count_sources = source_counts[source_counts < 100].index\n",
        "\n",
        "# replace low-count sources with 'Other'\n",
        "tweets_df['source'] = tweets_df['source'].replace(low_count_sources, 'Other')\n",
        "#replace NaNs with 'Other'\n",
        "tweets_df['source'].fillna('Other', inplace=True)\n",
        "tweets_df['source'].value_counts()"
      ],
      "metadata": {
        "id": "GvPFCp0voW5f"
      },
      "id": "GvPFCp0voW5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the top 20 sources\n",
        "top_20_sources = tweets_df['source'].value_counts().nlargest(20)\n",
        "\n",
        "# Convert to a DataFrame for plotting\n",
        "top_20_df = top_20_sources.reset_index()\n",
        "top_20_df.columns = ['Source', 'Count']\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=top_20_df,\n",
        "    y='Source',\n",
        "    x='Count',\n",
        "    hue='Source',  # Assign the x variable to hue\n",
        "    palette='viridis',\n",
        "    legend=False  # Disable the legend\n",
        ")\n",
        "plt.title('Top 20 Twitter Sources')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Source')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AJmCePQOoSb5"
      },
      "id": "AJmCePQOoSb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.11 Data Cleaning: Sentiment"
      ],
      "metadata": {
        "id": "asvDQcaVaZQp"
      },
      "id": "asvDQcaVaZQp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distribution of tweet sentiments\n",
        "sentiment_counts = tweets_df.sentiment.value_counts()\n",
        "print(sentiment_counts)\n",
        "# is there any NaNs\n",
        "print(tweets_df.sentiment.isna().sum())"
      ],
      "metadata": {
        "id": "95I77q_xoCPX"
      },
      "id": "95I77q_xoCPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom colors for the sentiments\n",
        "custom_colors = {\n",
        "    'pos': '#90EE90',  # Light Green\n",
        "    'neu': '#ADD8E6',   # Light Blue\n",
        "    'neg': '#FFB6C1'   # Light Red\n",
        "}\n",
        "\n",
        "# Ensure the colors map correctly to the sentiment categories\n",
        "print(\"Sentiment Categories:\", sentiment_counts.index)  # Debugging step\n",
        "colors = [custom_colors.get(sentiment, '#D3D3D3') for sentiment in sentiment_counts.index]\n",
        "\n",
        "# Check the colors being applied\n",
        "print(\"Applied Colors:\", colors)  # Debugging step\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sentiment_counts.values,\n",
        "    labels=sentiment_counts.index,\n",
        "    autopct=lambda p: f'{p:.1f}% ({int(p * sum(sentiment_counts.values) / 100)})',  # Percentage and count\n",
        "    colors=colors,  # Use custom colors\n",
        "    startangle=90\n",
        ")\n",
        "plt.title('Sentiment Distribution', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OkmtHZGsoE0N"
      },
      "id": "OkmtHZGsoE0N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiments and their corresponding colors\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
        "\n",
        "# Scatter plots for the relationship between sentiment scores and compound value\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (sentiment, color) in enumerate(zip(sentiments, colors), start=1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.scatterplot(\n",
        "        data=tweets_df,\n",
        "        x=sentiment,\n",
        "        y='compound',\n",
        "        alpha=0.6,\n",
        "        color=color\n",
        "    )\n",
        "    plt.title(f'Compound Score vs {sentiment.capitalize()}')\n",
        "    plt.xlabel(sentiment.capitalize())\n",
        "    plt.ylabel('Compound Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EQnrTxdO4TWj"
      },
      "id": "EQnrTxdO4TWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Kaggle's data card:\n",
        "Algorithm Sentiment Classification of Tweets (compound, sentiment):\n",
        "\n",
        "if tweet[compound] < 0:\n",
        "tweet[sentiment] = 0.0 # assigned 0.0 for Negative Tweets\n",
        "elif tweet[compound] > 0:\n",
        "tweet[sentiment] = 1.0 # assigned 1.0 for Positive Tweets\n",
        "else:\n",
        "tweet[sentiment] = 0.5 # assigned 0.5 for Neutral Tweets\n",
        "end```"
      ],
      "metadata": {
        "id": "FHOVEGR4FPx6"
      },
      "id": "FHOVEGR4FPx6"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "positive_tweets = tweets_df[tweets_df['sentiment'] == 'pos']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = positive_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ],
      "metadata": {
        "id": "VsDAMyKDFj6Z"
      },
      "id": "VsDAMyKDFj6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "neutral_tweets = tweets_df[tweets_df['sentiment'] == 'neu']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = neutral_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ],
      "metadata": {
        "id": "N-Ozs-UCHEl9"
      },
      "id": "N-Ozs-UCHEl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "negative_tweets = tweets_df[tweets_df['sentiment'] == 'neg']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = negative_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ],
      "metadata": {
        "id": "KfLiQNJuHRSC"
      },
      "id": "KfLiQNJuHRSC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatterplot for value distribution in neutral sentiment column."
      ],
      "metadata": {
        "id": "Kflp-YXVIGFk"
      },
      "id": "Kflp-YXVIGFk"
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df['neu'].value_counts()"
      ],
      "metadata": {
        "id": "vdy3Su78IFcX"
      },
      "id": "vdy3Su78IFcX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding sentiment compound distribution between positive, neutral, negative score values.\n",
        "\n",
        "Here we have a good distribution for positive, negative tweets. We're unable to plot neutral tweets's distribution because the variance is almost zero (most variables are around 0)."
      ],
      "metadata": {
        "id": "leAjLtZpzE2W"
      },
      "id": "leAjLtZpzE2W"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.kdeplot(\n",
        "    data=tweets_df,\n",
        "    x='compound',\n",
        "    hue='sentiment',\n",
        "    fill=True,\n",
        "    alpha=0.6,\n",
        "    palette={'neg': 'lightcoral', 'neu': 'lightblue', 'pos': 'lightgreen'}\n",
        ")\n",
        "plt.title('KDE of Compoundby by Sentiment')\n",
        "plt.xlabel('Compound Weighted')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VVtJyTzSzDqd"
      },
      "id": "VVtJyTzSzDqd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-computing the compound score using weighted formula: Compound Score=(pos−neg)×(1−neu)**\n",
        "\n",
        "We'll also exclude rows where all 3 values for `pos`, `neu`, `neg` are either 0 or 1."
      ],
      "metadata": {
        "id": "78HibLzkLK0S"
      },
      "id": "78HibLzkLK0S"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where all three values are either 0 or 1\n",
        "valid_rows = ~(\n",
        "    ((tweets_df['pos'] == 0) & (tweets_df['neu'] == 0) & (tweets_df['neg'] == 0)) |\n",
        "    ((tweets_df['pos'] == 1) & (tweets_df['neu'] == 1) & (tweets_df['neg'] == 1))\n",
        ")\n",
        "\n",
        "# Apply the filter explicitly with .loc[]\n",
        "tweets_df = tweets_df.loc[valid_rows]\n",
        "\n",
        "# Compute the weighted compound score using .loc[]\n",
        "tweets_df.loc[:, 'compound_weighted'] = (tweets_df['pos'] - tweets_df['neg']) * (1 - tweets_df['neu'])\n",
        "\n",
        "# Scale it to [0, 1]\n",
        "tweets_df['compound_weighted'] = (tweets_df['compound_weighted'] - tweets_df['compound_weighted'].min()) / (\n",
        "    tweets_df['compound_weighted'].max() - tweets_df['compound_weighted'].min()\n",
        ")\n",
        "\n",
        "# Display a preview of the updated DataFrame\n",
        "print(tweets_df[['pos', 'neu', 'neg', 'compound', 'compound_weighted', 'sentiment']].head())"
      ],
      "metadata": {
        "id": "6akAIPcRMnTV"
      },
      "id": "6akAIPcRMnTV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replot scores sentiments\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
        "\n",
        "# Scatter plots for the relationship between sentiment scores and compound value\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (sentiment, color) in enumerate(zip(sentiments, colors), start=1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.scatterplot(\n",
        "        data=tweets_df,\n",
        "        x=sentiment,\n",
        "        y='compound_weighted',\n",
        "        alpha=0.6,\n",
        "        color=color\n",
        "    )\n",
        "    plt.title(f'Compound Score vs {sentiment.capitalize()}')\n",
        "    plt.xlabel(sentiment.capitalize())\n",
        "    plt.ylabel('Compound Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-iy7Olu1NiZM"
      },
      "id": "-iy7Olu1NiZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting new and old compound score distributions.\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Plot weighted compound distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(tweets_df['compound_weighted'], bins=30, kde=False, color='purple')\n",
        "plt.title('Distribution of Weighted Compound Scores')\n",
        "plt.xlabel('Weighted Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Plot compound distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(tweets_df['compound'], bins=30, kde=False, color='orange')\n",
        "plt.title('Distribution of Original Compound Scores')\n",
        "plt.xlabel('Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wYpFMoLrOZdc"
      },
      "id": "wYpFMoLrOZdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing new sentiments:\n",
        "\n",
        "# Define conditions and labels for new_simple_sentiment\n",
        "simple_conditions = [\n",
        "    (tweets_df['compound_weighted'] <= 0.49),  # Negative sentiment\n",
        "    (tweets_df['compound_weighted'] >0.49) & (tweets_df['compound_weighted'] <= 0.51),  # Neutral sentiment\n",
        "    (tweets_df['compound_weighted'] > 0.51)  # Positive sentiment\n",
        "]\n",
        "simple_labels = ['neg', 'neu', 'pos']\n",
        "\n",
        "# Define conditions and labels for new_advanced_sentiment\n",
        "advanced_conditions = [\n",
        "    (tweets_df['compound_weighted'] < 0.35),  # Strongly Negative\n",
        "    (tweets_df['compound_weighted'] >= 0.35) & (tweets_df['compound_weighted'] <= 0.49),  # Weakly Negative (neu-neg)\n",
        "    (tweets_df['compound_weighted'] >0.49) & (tweets_df['compound_weighted'] <= 0.51),  # Neutral sentiment\n",
        "    (tweets_df['compound_weighted'] > 0.51) & (tweets_df['compound_weighted'] <= 0.75),  # Weakly Positive (neu-pos)\n",
        "    (tweets_df['compound_weighted'] > 0.75)  # Strongly Positive\n",
        "]\n",
        "advanced_labels = ['neg', 'neu-neg', 'neu', 'neu-pos', 'pos']\n",
        "\n",
        "# Assign new_simple_sentiment\n",
        "tweets_df['new_simple_sentiment'] = pd.cut(\n",
        "    tweets_df['compound_weighted'],\n",
        "    bins=[-float('inf'), 0.49, 0.51, float('inf')],  # Define boundaries\n",
        "    labels=['neg', 'neu', 'pos']\n",
        ")\n",
        "\n",
        "# Assign new_advanced_sentiment\n",
        "tweets_df['new_advanced_sentiment'] = pd.cut(\n",
        "    tweets_df['compound_weighted'],\n",
        "    bins=[-float('inf'), 0.35, 0.49, 0.51, 0.75, float('inf')],  # Define boundaries\n",
        "    labels=['neg', 'neu-neg', 'neu', 'neu-pos', 'pos']\n",
        ")\n",
        "\n",
        "# Check the distribution of the new sentiment columns\n",
        "print(tweets_df['new_simple_sentiment'].value_counts())\n",
        "print(tweets_df['new_advanced_sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "-3jWXTSASXj8"
      },
      "id": "-3jWXTSASXj8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate conclusions:\n",
        "* We don't know whow assigned scores are computed (it's not regular or weighted computation).\n",
        "* We can guess the prescence of a weights placed on the negative or positive parts of the score, when one dominates the other. That would explain the difference in compound score distribution, skewing tweets with more positive or negative scores urther away from neutral.\n",
        "* We should also consider that back in 2020 twitter was a better moderated environment, and many tweets would originate from official accounts, contributing to their neutral sentiment.  "
      ],
      "metadata": {
        "id": "3nq9SmWDWk1Z"
      },
      "id": "3nq9SmWDWk1Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.12 Exploratory Data Analysis (EDA): Social Connections\n",
        "Vanity fair. Lookng at social engagement, connections, popularity."
      ],
      "metadata": {
        "id": "sn9Hm_WY84o3"
      },
      "id": "sn9Hm_WY84o3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hashtags word cloud."
      ],
      "metadata": {
        "id": "_rsWWdIhAniD"
      },
      "id": "_rsWWdIhAniD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the list of hashtags\n",
        "all_hashtags = [\n",
        "    ''.join(hashtags)  # Join characters into a string\n",
        "    for hashtags in tweets_df['hashtags']\n",
        "    if isinstance(hashtags, (list, tuple, str, np.ndarray))  # Check if iterable\n",
        "]\n",
        "# Convert the list of hashtags into a single string, separated by spaces\n",
        "hashtag_text = ' '.join(all_hashtags)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, \\\n",
        "                      background_color='white', colormap='viridis').generate(hashtag_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Hashtag Word Cloud\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYYTMdrq9nNs"
      },
      "id": "zYYTMdrq9nNs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Histograms for favorite tweets, retweets.\n",
        "Convert both to log scale because of the value range (wide range, outliers).\n"
      ],
      "metadata": {
        "id": "C1Lt_TCCArYL"
      },
      "id": "C1Lt_TCCArYL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-transform favorite_count (add 1 to avoid log(0))\n",
        "tweets_df['log_favorite_count'] = np.log1p(tweets_df['favorite_count'])\n",
        "\n",
        "# Histogram for log-transformed favorite_count\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(tweets_df['log_favorite_count'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribution of Log-Transformed Favorite Counts')\n",
        "plt.xlabel('Log(Favorite Count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Log-transform retweet_count (add 1 to avoid log(0))\n",
        "tweets_df['log_retweet_count'] = np.log1p(tweets_df['retweet_count'])\n",
        "\n",
        "# Histogram for log-transformed retweet_count\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(tweets_df['log_retweet_count'], bins=30, kde=True, color='green')\n",
        "plt.title('Distribution of Log-Transformed Retweet Counts')\n",
        "plt.xlabel('Log(Retweet Count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZfLetef9-DkN"
      },
      "id": "ZfLetef9-DkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-transform favorite_count and retweet_count (add 1 to avoid log(0))\n",
        "tweets_df['log_favorite_count'] = np.log1p(tweets_df['favorite_count'])\n",
        "tweets_df['log_retweet_count'] = np.log1p(tweets_df['retweet_count'])\n",
        "\n",
        "# Scatter plot with log-transformed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='log_favorite_count', y='log_retweet_count', data=tweets_df, alpha=0.6)\n",
        "plt.title('Log(Favorites) vs Log(Retweets)')\n",
        "plt.xlabel('Log(Favorite Count)')\n",
        "plt.ylabel('Log(Retweet Count)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EnxKnDBe-c7q"
      },
      "id": "EnxKnDBe-c7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Show trends in favorites and retweets over time.\n",
        "aggregated = tweets_df.groupby('date')[['favorite_count', 'retweet_count']].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='date', y='favorite_count', data=aggregated, label='Favorites', color='blue')\n",
        "plt.yscale('log')\n",
        "sns.lineplot(x='date', y='retweet_count', data=aggregated, label='Retweets', color='green')\n",
        "plt.title('Log Trends in Favorites and Retweets Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NNUqkPi5-mXH"
      },
      "id": "NNUqkPi5-mXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top contributors by the number of tweets\n",
        "top_authors = tweets_df['original_author'].value_counts().head(10)\n",
        "# Bar Plot for Top Authors\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    x=top_authors.values,\n",
        "    y=top_authors.index,\n",
        "    hue=top_authors.index,  # Assign hue to the y variable\n",
        "    palette='viridis',\n",
        "    dodge=False,            # Ensure no splitting\n",
        "    legend=False            # Disable the legend\n",
        ")\n",
        "plt.title('Top 10 Authors by Number of Tweets')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Author')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YgsLPGyT-uc6"
      },
      "id": "YgsLPGyT-uc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot authors based on their average favorite_count and retweet_count\n",
        "author_stats = tweets_df.groupby('original_author')[['favorite_count', \\\n",
        "                                                     'retweet_count']].mean().reset_index()\n",
        "\n",
        "author_stats['log_favorite_count'] = np.log1p(author_stats['favorite_count'])\n",
        "author_stats['log_retweet_count'] = np.log1p(author_stats['retweet_count'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='log_favorite_count', y='log_retweet_count', data=author_stats, alpha=0.6\n",
        ")\n",
        "plt.title('Author Popularity: Log-Transformed Average Favorites vs Retweets')\n",
        "plt.xlabel('Log(Average Favorite Count)')\n",
        "plt.ylabel('Log(Average Retweet Count)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWQxhA-u_GOW"
      },
      "id": "YWQxhA-u_GOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the most mentioned users."
      ],
      "metadata": {
        "id": "ndi-I9-9__Hx"
      },
      "id": "ndi-I9-9__Hx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Process user_mentions: split by comma and remove empty/malformed entries\n",
        "tweets_df['user_mentions'] = tweets_df['user_mentions'].fillna('')  # Replace None with empty strings\n",
        "tweets_df['user_mentions_clean'] = tweets_df['user_mentions'].apply(lambda x: [mention.strip() for mention in x.split(',') if mention.strip()])\n",
        "\n",
        "# Flatten mentions and count frequencies\n",
        "all_mentions = [mention for mentions in tweets_df['user_mentions_clean'] for mention in mentions]\n",
        "mention_counts = Counter(all_mentions).most_common(10)\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "mention_df = pd.DataFrame(mention_counts, columns=['user', 'count'])\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    x='count',\n",
        "    y='user',\n",
        "    data=mention_df,\n",
        "    hue='user',        # Assign hue to the y variable\n",
        "    palette='magma',\n",
        "    dodge=False,       # Prevent bar splitting\n",
        "    legend=False       # Disable the legend\n",
        ")\n",
        "plt.title('Top 10 Mentioned Users', fontsize=14)\n",
        "plt.xlabel('Number of Mentions', fontsize=12)\n",
        "plt.ylabel('User', fontsize=12)\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53_X2kEx_Rlf"
      },
      "id": "53_X2kEx_Rlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Show the relationships between authors and mentioned users. With a subset that big, we won't have computational resources to compute a full graph.  "
      ],
      "metadata": {
        "id": "fZvsOhFo_6JZ"
      },
      "id": "fZvsOhFo_6JZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter nodes with degree above a certain threshold (200 connections)\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "# Populate the graph with edges from tweets_df\n",
        "for _, row in tweets_df.iterrows():\n",
        "    original_author = row['original_author']\n",
        "    mentioned_users = row['user_mentions_clean']  # List of mentioned users\n",
        "\n",
        "    # Add edges: author -> mentioned user\n",
        "    for mention in mentioned_users:\n",
        "        G.add_edge(original_author, mention)\n",
        "\n",
        "degree_threshold = 200\n",
        "high_degree_nodes = [node for node, degree in G.degree() if degree > degree_threshold]\n",
        "subgraph = G.subgraph(high_degree_nodes)\n",
        "\n",
        "# Draw the subgraph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw_networkx(\n",
        "    subgraph,\n",
        "    node_size=50,\n",
        "    alpha=0.7,\n",
        "    font_size=8,\n",
        "    edge_color='gray'\n",
        ")\n",
        "plt.title(f'User Mentions Network (Nodes with Degree > {degree_threshold})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ZXy4hax_cYf"
      },
      "id": "5ZXy4hax_cYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Display correlation between different metrics (e.g., favorites, retweets, mentions).*italicized text*"
      ],
      "metadata": {
        "id": "zNKuJ_Nh_jDi"
      },
      "id": "zNKuJ_Nh_jDi"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(tweets_df[['favorite_count', 'retweet_count']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Between Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aUI5MgVT_lwr"
      },
      "id": "aUI5MgVT_lwr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize authors with bubbles representing their popularity."
      ],
      "metadata": {
        "id": "EUTwTTrb_w8t"
      },
      "id": "EUTwTTrb_w8t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate favorite_count and retweet_count for each author\n",
        "author_stats = tweets_df.groupby('original_author').agg({\n",
        "    'favorite_count': 'mean',\n",
        "    'retweet_count': 'mean',\n",
        "    'original_author': 'count'  # Count tweets for each author\n",
        "}).rename(columns={'original_author': 'tweet_count'}).reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='favorite_count',\n",
        "    y='retweet_count',\n",
        "    size='tweet_count',\n",
        "    data=author_stats,\n",
        "    sizes=(50, 500),\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Author Popularity: Favorites vs Retweets with Tweet Count')\n",
        "plt.xlabel('Average Favorite Count')\n",
        "plt.ylabel('Average Retweet Count')\n",
        "plt.legend(title='Tweet Count', loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "of54h0nm_0Ku"
      },
      "id": "of54h0nm_0Ku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Text Preprocessing and Feature Engineering\n",
        "\n",
        "In this section we'll be transforming the contents of the `original_tweet` column to the text format suitable for classification. This will include the following steps:\n",
        "\n",
        "**Cleaning**\n",
        "\n",
        "This step removes noise and unnecessary elements from the raw tweet text, such as special characters, links, mentions (@usernames), and hashtags (#topic), ii also detects emoji and converts it to plain text, (😀 to smile face).\n",
        "\n",
        "**Preprocessing**\n",
        "This section prepares the cleaned text for sentiment analysis by breaking it down into individual words (tokenization), removing common words that don't carry much meaning (stop word removal), and reducing words to their base form (lemmatization).\n",
        "**Feature Extraction**\n",
        "\n",
        "This stage involves creating numerical representations of the text that can be used as input for machine learning models. We'll be extracting\n",
        "* Bag of Words (BoW): This technique represents text as a collection of individual words and their frequencies, ignoring grammar and word order. It creates a numerical vector for each document, where each element represents the count of a specific word in the document's vocabulary.\n",
        "* N-grams: Creating combinations of words (e.g., \"Covid cases\" as a bigram, or \"Die of Covid\") to capture more context.\n",
        "* TF-IDF: Calculating the importance of words in a document relative to a collection of documents.\n",
        "* Embeddings: Using pre-trained models to create vector representations of words that capture semantic meaning.\n"
      ],
      "metadata": {
        "id": "MWCtcPLd3q8z"
      },
      "id": "MWCtcPLd3q8z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Text Cleaning\n",
        "We wrote ```process_tweet_data()```to do the following:\n",
        "* Remove and store emoji as a separate column(using default UNICODE_EMOJI).\n",
        "* Remove and store mentions @ as a separate column.\n",
        "* Remove and store retweets RT @ as a separate column.\n",
        "* Remove and store hashtags # as a separate column.\n",
        "* Remove and store URLs www. or t. or bit. as a separate column.\n",
        "* Remove special characters, whitespaces, numbers.\n",
        "* Lowercase and store text in ```cleaned_text``` column  \n",
        "\n",
        "\n",
        "For debugging purposes, we'll create a small dataframe with URLs, mentions, retweents, and emojis."
      ],
      "metadata": {
        "id": "Vkjyw35VopKU"
      },
      "id": "Vkjyw35VopKU"
    },
    {
      "cell_type": "code",
      "source": [
        "# example df\n",
        "df_test = {\n",
        "    'tweet_text': [\n",
        "        \"WOW!!! Check out this link: https://example.com :) @user123 #hashtag #fun\",\n",
        "        \"RT @user456: Another day in paradise! 😃 #sunnyday\",\n",
        "        \"Why so serious? :(( Visit www.example.org for details! #serious\"\n",
        "    ]\n",
        "}\n",
        "df_test = pd.DataFrame(df_test)\n",
        "\n",
        "# Clean the text data\n",
        "df_test = pd.concat([\n",
        "    df_test,\n",
        "    df_test['tweet_text'].apply(lambda x: pd.Series(process_tweet_data(x)))\n",
        "], axis=1)\n",
        "df_test"
      ],
      "metadata": {
        "id": "Hu094gmiwvFz"
      },
      "id": "Hu094gmiwvFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll apply text cleaning with progress to the subset of entire twitter df (takes about 2 hours minutes)"
      ],
      "metadata": {
        "id": "zG2ge4CK27kf"
      },
      "id": "zG2ge4CK27kf"
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_slice_df = tweets_df[['id', 'date', 'source', 'original_text', 'sentiment', \\\n",
        "                             'pos', 'neu', 'neg', 'compound', 'compound_weighted', \\\n",
        "                             'new_simple_sentiment', 'new_advanced_sentiment']].copy()"
      ],
      "metadata": {
        "id": "XvYD8vhPju9c"
      },
      "id": "XvYD8vhPju9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for NaNs before text pre-processing.\n",
        "tweets_slice_df.isna().sum()\n",
        "# drop NaNs from ID\n",
        "tweets_slice_df = tweets_slice_df.dropna(subset=['id'])\n",
        "#re-check for NaNs:\n",
        "tweets_slice_df.isna().sum()"
      ],
      "metadata": {
        "id": "j_q9NBBTeSbM"
      },
      "id": "j_q9NBBTeSbM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all retweets\n",
        "# Detect retweets: Create a mask for rows that are retweets\n",
        "retweet_mask = tweets_slice_df['original_text'].str.startswith('RT @')\n",
        "\n",
        "# Filter out retweets\n",
        "tweets_df_no_retweets = tweets_slice_df[~retweet_mask].copy()\n",
        "\n",
        "# Print results\n",
        "print(f\"Original DataFrame size: {tweets_df.shape[0]} rows\")\n",
        "print(f\"DataFrame size after removing retweets: {tweets_df_no_retweets.shape[0]} rows\")"
      ],
      "metadata": {
        "id": "JQp5P0qtgWjr"
      },
      "id": "JQp5P0qtgWjr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to each tweet and expand the results into separate columns\n",
        "cleaned_text_column = clean_tweets_with_progress_parallel(tweets_df_no_retweets, text_col='original_text')"
      ],
      "metadata": {
        "id": "rtFLmGL6kb_r"
      },
      "id": "rtFLmGL6kb_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the cleaned text column to dataframe:\n",
        "tweets_df_no_retweets['cleaned_text'] = cleaned_text_column"
      ],
      "metadata": {
        "id": "JCGX7QGtPly3"
      },
      "id": "JCGX7QGtPly3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df_no_retweets = tweets_df_no_retweets.dropna(subset=['cleaned_text'])\n",
        "tweets_df_no_retweets.isna().sum()"
      ],
      "metadata": {
        "id": "JcPLYI2XPqb9"
      },
      "id": "JcPLYI2XPqb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save as inrermideate processing step\n",
        "path_to_saved_file = os.path.join(data_dir, \"tweets_df_no_retweets.csv\")  # Path to saved file\n",
        "tweets_df_no_retweets.to_csv(path_to_saved_file, index=False)\n",
        "print(\"tweets_df_no_retweets has been saved as 'tweets_df_no_retweets.csv'.\")"
      ],
      "metadata": {
        "id": "XdN06fpqOAP-"
      },
      "id": "XdN06fpqOAP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Text Preprocessing\n",
        "\n",
        "This section focuses on preparing the tweet text data for sentiment analysis by applying essential preprocessing techniques. The following steps are performed:\n",
        "\n",
        "* Tokenization: The text of each tweet is broken down into individual words or tokens using the word_tokenize function from the nltk library.\n",
        "\n",
        "* Stop Word Removal: Common words (like \"the,\" \"a,\" \"is\") that don't carry much meaning are removed from the tokenized text to reduce noise and improve analysis accuracy.\n",
        "\n",
        "* Lemmatization: Words are reduced to their base or root form (e.g., \"running\" becomes \"run\") using the WordNetLemmatizer to standardize the vocabulary and group similar words together.\n",
        "\n",
        "* Text Recombination: The preprocessed tokens are combined back into a single text string for further analysis."
      ],
      "metadata": {
        "id": "o_Kzz-ygoslI"
      },
      "id": "o_Kzz-ygoslI"
    },
    {
      "cell_type": "code",
      "source": [
        "# load tweets_slice_df if memory crashes\n",
        "path_to_saved_file = os.path.join(data_dir, \"tweets_df_no_retweets.csv\")  # Path to saved file\n",
        "tweets_df_no_retweets = pd.read_csv(path_to_saved_file, low_memory=False)\n",
        "tweets_df_no_retweets.dropna(subset=['cleaned_text'], inplace=True)"
      ],
      "metadata": {
        "id": "vi3waIylg09B"
      },
      "id": "vi3waIylg09B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df_no_retweets.head()"
      ],
      "metadata": {
        "id": "zw8XhSX1fzIc"
      },
      "id": "zw8XhSX1fzIc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text in the 'cleaned_text' column\n",
        "preprocessed_df_test = preprocess_text(df_test, text_column='cleaned_text')\n",
        "# Display the preprocessed DataFrame\n",
        "preprocessed_df_test[['cleaned_text', 'lemmatized_text']]"
      ],
      "metadata": {
        "id": "uhWbdBrPrTvy"
      },
      "id": "uhWbdBrPrTvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply lemmatization to entire dataframe\n",
        "lemmatized_df = preprocess_text(tweets_df_no_retweets, text_column='cleaned_text')"
      ],
      "metadata": {
        "id": "T9567GjY2sBe"
      },
      "id": "T9567GjY2sBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_df.isna().sum()"
      ],
      "metadata": {
        "id": "OEpUVykLWiP5"
      },
      "id": "OEpUVykLWiP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_df= lemmatized_df.dropna(subset=['lemmatized_text'])\n",
        "# checking NaNs\n",
        "lemmatized_df.isna().sum()"
      ],
      "metadata": {
        "id": "NmExG5RhtM73"
      },
      "id": "NmExG5RhtM73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save lemmatized_df if memory crashes\n",
        "path_to_saved_lemmatized_file = os.path.join(data_dir, \"lemmatized_df.csv\")  # Path to saved file\n",
        "lemmatized_df.to_csv(path_to_saved_lemmatized_file, index=False)\n",
        "print(\"lemmatized_df has been saved as 'lemmatized_df.csv'.\")"
      ],
      "metadata": {
        "id": "8mmpr-vBy2pv"
      },
      "id": "8mmpr-vBy2pv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Feature Extraction.\n",
        "In this section, we'll transform the preprocessed text data into numerical representations suitable for machine learning models. We'll explore techniques like:\n",
        "\n",
        "N-grams: Extracting sequences of adjacent words (e.g., \"social distancing,\" \"stay home\") to capture contextual information.\n",
        "TF-IDF: Calculating word importance based on their frequency within a document and across the entire corpus.\n",
        "Embeddings (optional): Representing words as dense vectors to capture semantic relationships.\n",
        "These extracted features will serve as input for the sentiment analysis models in the next stage."
      ],
      "metadata": {
        "id": "1uQsQ8CI5rTG"
      },
      "id": "1uQsQ8CI5rTG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll run test datafram through the N-grams generation, Bag of Words, TF-IDF."
      ],
      "metadata": {
        "id": "BqK3b_82vt6b"
      },
      "id": "BqK3b_82vt6b"
    },
    {
      "cell_type": "code",
      "source": [
        "# load lemmatized_df if memory crashes\n",
        "# path_to_saved_lemmatized_file = os.path.join(data_dir, \"lemmatized_df.csv\")  # Path to saved file\n",
        "# lemmatized_df = pd.read_csv(path_to_saved_lemmatized_file, low_memory=False)"
      ],
      "metadata": {
        "id": "XsyUUQMXzlnD"
      },
      "id": "XsyUUQMXzlnD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply N-gram computation to test lemmatized twitter df\n",
        "ngram_matrix_0, vectorizer_0 = compute_ngrams(preprocessed_df_test, \\\n",
        "                                          text_column='lemmatized_text', ngram_range=(2, 3))\n",
        "\n",
        "# Feature names (n-grams)\n",
        "ngram_features_0 = vectorizer_0.get_feature_names_out()\n",
        "\n",
        "print(f\"Top 10 n-grams: {ngram_features_0[:10]}\")\n",
        "print(f\"Shape of n-gram matrix: {ngram_matrix_0.shape}\")"
      ],
      "metadata": {
        "id": "HpqpRxgzwG6X"
      },
      "id": "HpqpRxgzwG6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # can be adjusted as needed"
      ],
      "metadata": {
        "id": "yS_svUDn5O3R"
      },
      "id": "yS_svUDn5O3R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer to test lemmatized twitter df\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_df_test['lemmatized_text'])\n",
        "\n",
        "## Visualize the results\n",
        "# tfidf_matrix and tfidf_vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.toarray().sum(axis=0)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(zip(feature_names, tfidf_scores)))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9uvTwVdbnxI"
      },
      "id": "L9uvTwVdbnxI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words for the test matrix\n",
        "bow_vectorizer = TfidfVectorizer(max_features=5000)  #  can be adjusted as needed"
      ],
      "metadata": {
        "id": "xBm257Ts5Ub4"
      },
      "id": "xBm257Ts5Ub4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer to test lemmatized twitter df\n",
        "bow_matrix = bow_vectorizer.fit_transform(preprocessed_df_test['lemmatized_text'])\n",
        "\n",
        "# Plot heatmap\n",
        "subset_matrix = bow_matrix.toarray()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(subset_matrix, cmap=\"viridis\", annot=True, fmt=\".2g\",\n",
        "            xticklabels=bow_vectorizer.get_feature_names_out(),\n",
        "            yticklabels=range(bow_matrix.shape[0]))\n",
        "plt.title(\"Bag of Words Heatmap (Test Data)\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Tweets\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uyZF7DKM5C-W"
      },
      "id": "uyZF7DKM5C-W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting N-grams for the entire datset"
      ],
      "metadata": {
        "id": "3f9XdGf2fEqI"
      },
      "id": "3f9XdGf2fEqI"
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_matrix_twitter, vectorizer_twitter = compute_ngrams(lemmatized_df, \\\n",
        "                                          text_column='lemmatized_text', ngram_range=(2, 3))\n",
        "\n",
        "# Feature names (n-grams)\n",
        "ngram_features_twitter = vectorizer_twitter.get_feature_names_out()\n",
        "\n",
        "print(f\"Top 10 n-grams: {ngram_features_twitter[:10]}\")\n",
        "print(f\"Shape of n-gram matrix: {ngram_features_twitter.shape}\")"
      ],
      "metadata": {
        "id": "UHccUBALfEQj"
      },
      "id": "UHccUBALfEQj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How N-grams are distributed by sentiments?"
      ],
      "metadata": {
        "id": "T1tFg4UZnwOb"
      },
      "id": "T1tFg4UZnwOb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the n-gram matrix to a DataFrame\n",
        "ngram_df = pd.DataFrame(\n",
        "    ngram_matrix_twitter.toarray(),\n",
        "    columns=ngram_features_twitter\n",
        ")\n",
        "ngram_df['sentiment'] = lemmatized_df['sentiment']  # Add the sentiment column\n",
        "\n",
        "# Aggregate n-grams by sentiment\n",
        "sentiment_ngrams = ngram_df.groupby('sentiment').sum()\n",
        "\n",
        "# Step 3: Select top n-grams for each sentiment\n",
        "top_ngrams_per_sentiment = {}\n",
        "n_top = 10\n",
        "for sentiment in sentiment_ngrams.index:\n",
        "    top_ngrams = sentiment_ngrams.loc[sentiment].nlargest(n_top)\n",
        "    top_ngrams_per_sentiment[sentiment] = top_ngrams\n",
        "\n",
        "# Step 4: Visualize the top n-grams for each sentiment\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "for i, (sentiment, top_ngrams) in enumerate(top_ngrams_per_sentiment.items(), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.barplot(\n",
        "    x=top_ngrams.values,\n",
        "    y=top_ngrams.index,\n",
        "    hue=top_ngrams.index,  # Assign hue to the y variable\n",
        "    dodge=False,           # Ensure no splitting\n",
        "    legend=False,          # Disable the legend\n",
        "    palette='viridis'\n",
        "    )\n",
        "    plt.title(f\"Top {n_top} N-grams ({sentiment})\", fontsize=14)\n",
        "    plt.xlabel('Frequency', fontsize=12)\n",
        "    plt.ylabel('N-Gram', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zkB0FY6knpP-"
      },
      "id": "zkB0FY6knpP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the BoW vectorizer to full twitter data\n",
        "bow_matrix = bow_vectorizer.fit_transform(lemmatized_df['lemmatized_text'])"
      ],
      "metadata": {
        "id": "FeqTbIb4wtQs"
      },
      "id": "FeqTbIb4wtQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum frequencies for each term\n",
        "term_frequencies = bow_matrix.sum(axis=0).A1  # Sum along columns\n",
        "top_indices = term_frequencies.argsort()[-20:][::-1]  # Indices of top 20 terms\n",
        "\n",
        "# Subset matrix for these terms\n",
        "subset_matrix = bow_matrix[:, top_indices][:20].toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(subset_matrix, cmap='viridis', annot=True, fmt=\".2f\",\n",
        "            xticklabels=bow_vectorizer.get_feature_names_out()[top_indices],  # Feature names for top terms\n",
        "            yticklabels=range(20))\n",
        "plt.title('Heatmap of Top 20 Terms (BoW)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cryPP72seY0l"
      },
      "id": "cryPP72seY0l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the tf-idf vectorizer to full twitter data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_df['lemmatized_text'])"
      ],
      "metadata": {
        "id": "c7iZuFzTwwwm"
      },
      "id": "c7iZuFzTwwwm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualize the results\n",
        "# Compute term frequencies\n",
        "tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
        "\n",
        "# Get feature names from the tfidf_vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Select top 200 terms (adjust if needed)\n",
        "top_indices = np.argsort(tfidf_scores)[-200:]\n",
        "top_features = feature_names[top_indices] # Now, feature_names is from tfidf_vectorizer\n",
        "top_scores = tfidf_scores[top_indices]\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(\n",
        "    dict(zip(top_features, top_scores))\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9dvQ36XgeZB3"
      },
      "id": "9dvQ36XgeZB3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings (BERT)."
      ],
      "metadata": {
        "id": "-jzHSx2ntiBt"
      },
      "id": "-jzHSx2ntiBt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "# bert_model_name = \"bert-base-uncased\"\n",
        "# tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "# model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# Apply BERT embeddings to dataset with progress bar\n",
        "# lemmatized_df['bert_embedding'] = compute_bert_embeddings_parallel(lemmatized_df, text_column='lemmatized_text')"
      ],
      "metadata": {
        "id": "F_jx83r6tc1g"
      },
      "id": "F_jx83r6tc1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save lemmatize_df\n",
        "#lemmatized_df.to_csv(\"lemmatized_df_with_bert.csv\", index=False)\n",
        "#print(\"lemmatized_df has been saved as 'lemmatized_df_with_bert.csv'.\")"
      ],
      "metadata": {
        "id": "bjVGl_MBvIiI"
      },
      "id": "bjVGl_MBvIiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "glove_file = \"glove.twitter.27B.200d.txt\"  # 200-dimensional embeddings\n",
        "glove_path = os.path.join(data_dir, glove_file)\n",
        "embeddings_dict = load_glove_embeddings(glove_path)\n",
        "\n",
        "#Compute embeddings for a dataset\n",
        "lemmatized_df['sentence_embedding'] = lemmatized_df['lemmatized_text'].apply(\n",
        "    lambda x: compute_sentence_embedding(x, embeddings_dict, embedding_dim=200)\n",
        ")"
      ],
      "metadata": {
        "id": "IKb5Y14j_-Cf"
      },
      "id": "IKb5Y14j_-Cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting features ready for use:\n",
        "# Convert embeddings into a 2D NumPy array\n",
        "embeddings_matrix = np.vstack(lemmatized_df['sentence_embedding'].values)\n",
        "embeddings_sparse = csr_matrix(embeddings_matrix)"
      ],
      "metadata": {
        "id": "DCV2GAjXuJU3"
      },
      "id": "DCV2GAjXuJU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sentiment Classification\n",
        "\n",
        "This section aims to analyze the sentiment expressed in the preprocessed tweets using machine learning models. The goal is to classify tweets into different sentiment categories (e.g., positive, negative, neutral) and to understand the overall sentiment trends within the dataset. This involves:\n",
        "\n",
        "* Model Selection and Training: Choosing a suitable classification model and training it on the preprocessed tweet data. We'll start with a baseline simple model.\n",
        "\n",
        "* Model Evaluation: Assessing the performance of the trained model using appropriate metrics.\n",
        "* Insights and Visualization: Presenting the results of the sentiment analysis, including visualizations and insights derived from the model's predictions.\n",
        "\n",
        "\n",
        "**Input Overview**\n",
        "\n",
        "* Our input features `X` will be a combination (concatenation) of computed text features:\n",
        "`bow_matrix`, `tfidf_matrix`, `ngram_matrix_twitter`, `embeddings_sparse`.\n",
        "* Our target variable `y` is the sentiment : `{neg, neu, pos}`.\n",
        "* Our performance metric will be `f1-score`.   \n",
        "* X dimentions: `170675x10200`.\n",
        "* Sentiment distribution (class imbalance):  \n",
        "`neu:76894, pos:50239, neg:43542`.\n"
      ],
      "metadata": {
        "id": "7ZLUlvn-57Z_"
      },
      "id": "7ZLUlvn-57Z_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and labels\n",
        "X_bow = hstack([bow_matrix, ngram_matrix_twitter, embeddings_sparse])\n",
        "# X_tfidf = hstack([tfidf_matrix, ngram_matrix_twitter, embeddings_sparse])\n",
        "X_bow.shape"
      ],
      "metadata": {
        "id": "VuioIhl14jJS"
      },
      "id": "VuioIhl14jJS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check sentiment distribution between all 3 versions:\n",
        "print(lemmatized_df['sentiment'].value_counts())\n",
        "print(lemmatized_df['new_simple_sentiment'].value_counts())\n",
        "print(lemmatized_df['new_advanced_sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "4gIrwKh2vT0A"
      },
      "id": "4gIrwKh2vT0A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  4.1 Train-Test Data Split.\n",
        " Split proportions: 70% training, 15% testing, 15% validation."
      ],
      "metadata": {
        "id": "m7djBMSQ4oHO"
      },
      "id": "m7djBMSQ4oHO"
    },
    {
      "cell_type": "code",
      "source": [
        "y = lemmatized_df['sentiment']     # Sentiment labels\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the sentiment labels\n",
        "y_encoded = label_encoder.fit_transform(y)  # y contains 'pos', 'neg', 'neu'\n",
        "\n",
        "# Check the mapping\n",
        "print(\"Label Mapping:\", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))"
      ],
      "metadata": {
        "id": "pT6GL1sdvTAO"
      },
      "id": "pT6GL1sdvTAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_bow, y_encoded, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Cc2jlpJO5vAk"
      },
      "id": "Cc2jlpJO5vAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# size of each set\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Number of features\n",
        "print(f\"Number of features: {X_train.shape[1]}\")"
      ],
      "metadata": {
        "id": "SkoWwP420l-9"
      },
      "id": "SkoWwP420l-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save features and variables for future use\n",
        "# pd.to_pickle(X_train, os.path.join(data_dir, \"X_train.pkl\"))\n",
        "# pd.to_pickle(X_val, os.path.join(data_dir, \"X_val.pkl\"))\n",
        "# pd.to_pickle(X_test, os.path.join(data_dir, \"X_test.pkl\"))\n",
        "# pd.to_pickle(y_train, os.path.join(data_dir, \"y_train.pkl\"))\n",
        "# pd.to_pickle(y_val, os.path.join(data_dir, \"y_val.pkl\"))\n",
        "# pd.to_pickle(y_test, os.path.join(data_dir, \"y_test.pkl\"))\n",
        "# pd.to_pickle(label_encoder, os.path.join(data_dir, \"label_encoder.pkl\"))\n",
        "# pd.to_pickle(bow_vectorizer, os.path.join(data_dir, \"bow_vectorizer.pkl\"))\n",
        "# pd.to_pickle(ngram_features_twitter, os.path.join(data_dir, \"vectorizer_twitter.pkl\"))\n",
        "# pd.to_pickle(embeddings_dict, os.path.join(data_dir, \"embeddings_dict.pkl\"))"
      ],
      "metadata": {
        "id": "DWZcqR4EYD4m"
      },
      "id": "DWZcqR4EYD4m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Sentiment Classification with Simple Logistic Regression Model."
      ],
      "metadata": {
        "id": "5C0RTXca1uq3"
      },
      "id": "5C0RTXca1uq3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Model\n",
        "log_reg = LogisticRegression(\n",
        "    penalty='l2',  # L2 regularization (Ridge regression penalty)\n",
        "    C=1.0,  # Regularization strength (smaller = stronger regularization)\n",
        "    max_iter=1000,  # Ensure convergence\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5iyfqcVVYZ83"
      },
      "id": "5iyfqcVVYZ83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "y_val_log = log_reg.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_val\n",
        "\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_log_decoded = label_encoder.inverse_transform(y_pred_log)\n",
        "y_val_log_decoded = label_encoder.inverse_transform(y_val_log)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Base Model LogReg Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_log_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Base Model LogReg Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_log_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "7S8e1VYnZVYm"
      },
      "id": "7S8e1VYnZVYm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_decoded, y_pred_log_decoded, labels=label_encoder.classes_)\n",
        "\n",
        "# Normalize confusion matrix by rows to show percentages\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages\n",
        "\n",
        "# Display the normalized confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=label_encoder.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(cmap='Blues', colorbar=True, ax=ax)\n",
        "\n",
        "plt.title(\"LogReg Confusion Matrix (Percentages) on Test Set\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vGymIuwFaIkn"
      },
      "id": "vGymIuwFaIkn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Tuning Hyperparameters for Base Logisstic Regression Model"
      ],
      "metadata": {
        "id": "1l4AxpFoa8BU"
      },
      "id": "1l4AxpFoa8BU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Grid search with F1 scoring\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    scoring=make_scorer(f1_score, average='weighted'),  # Weighted F1 score\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and F1 score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best F1 Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "kQNYPilwa5M2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "9fd1adc6-762e-4164-812a-13e42098769d"
      },
      "id": "kQNYPilwa5M2",
      "execution_count": 142,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-142-348c0be5a7a3>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Fit on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Best parameters and F1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1021\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    967\u001b[0m                     )\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    970\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    971\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;31m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Improved Model Performance and Feature Interpretation."
      ],
      "metadata": {
        "id": "0pXuFA03glp2"
      },
      "id": "0pXuFA03glp2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Feature Reduction (Experimental)"
      ],
      "metadata": {
        "id": "QeqthMaZEDoh"
      },
      "id": "QeqthMaZEDoh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X_bow is dense\n",
        "X_dense = csr_matrix(X_bow).toarray()\n",
        "\n",
        "# Use the hybrid GWO + ABC optimizer\n",
        "best_features = hybrid_gwo_abc(X_dense, y_encoded, model=SVC(kernel='linear'))"
      ],
      "metadata": {
        "id": "X7_z--b3BfSu"
      },
      "id": "X7_z--b3BfSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the best feature mask\n",
        "selected_indices = np.where(best_features == 1)[0]\n",
        "X_bow_selected = X_bow[:, selected_indices]\n",
        "\n",
        "print(f\"Reduced feature set from {X_bow.shape[1]} to {X_bow_selected.shape[1]}\")"
      ],
      "metadata": {
        "id": "1T0nWRwGBfnT"
      },
      "id": "1T0nWRwGBfnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test-val split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_bow_selected, y_encoded, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "# Train SVM\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "Ry6eJEEtEmMU"
      },
      "id": "Ry6eJEEtEmMU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Other Tested Classification Models."
      ],
      "metadata": {
        "id": "3VReoTUA71g3"
      },
      "id": "3VReoTUA71g3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.1 SDG Classifier Performance."
      ],
      "metadata": {
        "id": "Ywzy8538Qz2E"
      },
      "id": "Ywzy8538Qz2E"
    },
    {
      "cell_type": "code",
      "source": [
        "# SGDClassifier with logistic regression loss\n",
        "sgd_model = SGDClassifier(\n",
        "    loss='log_loss',          # Logistic regression loss\n",
        "    penalty='l2',             # L2 regularization\n",
        "    alpha=1e-4,               # Regularization strength\n",
        "    max_iter=1000,            # Maximum number of iterations\n",
        "    tol=1e-3,                 # Convergence tolerance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate the SGD model\n",
        "sgd_model.fit(X_train, y_train)\n",
        "y_pred_sgd = sgd_model.predict(X_test)\n",
        "y_val_sgd = sgd_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_sgd\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_sgd_decoded = label_encoder.inverse_transform(y_pred_sgd)\n",
        "y_val_sgd_decoded = label_encoder.inverse_transform(y_val_sgd)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_sgd_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_sgd_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "3w8cBoZHre_5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "89b44e99-b08b-462a-f80f-f758efa25c0e"
      },
      "id": "3w8cBoZHre_5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SGDClassifier' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6d21677919a7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SGDClassifier with logistic regression loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m sgd_model = SGDClassifier(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log_loss'\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# Logistic regression loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# L2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# Regularization strength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SGDClassifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = pipeline_sgd.named_steps['sgd'].coef_\n",
        "\n",
        "# Get feature names and align them with the combined feature matrix\n",
        "bow_features = bow_vectorizer.get_feature_names_out()\n",
        "ngram_features = vectorizer_twitter.get_feature_names_out()\n",
        "embedding_dim = 200\n",
        "embedding_feature_names = [f\"embedding_{i}\" for i in range(embedding_dim)]\n",
        "\n",
        "all_features = np.concatenate([bow_features, ngram_features, embedding_feature_names])  # Combine feature names\n",
        "\n",
        "# Ensure alignment between coefficients and feature names\n",
        "if coefficients.shape[1] != len(all_features):\n",
        "    raise ValueError(\"Mismatch between coefficients and feature names.\")\n",
        "\n",
        "# Analyze top features for each sentiment\n",
        "for i, sentiment in enumerate(label_encoder.classes_):\n",
        "    top_indices = np.argsort(coefficients[i])[-10:]  # Top 10 features for each sentiment\n",
        "    top_features = all_features[top_indices]  # Use combined features\n",
        "\n",
        "    print(f\"\\nTop Features for {sentiment.capitalize()} Sentiment:\")\n",
        "    for feature, coef in zip(top_features, coefficients[i][top_indices]):\n",
        "        if feature.startswith(\"embedding_\"):\n",
        "            # Extract the embedding index from the feature name (e.g., embedding_11 -> 11)\n",
        "            embedding_index = int(feature.split(\"_\")[1])\n",
        "            print(f\"{feature} (Embedding Value): {coef:.4f}\")\n",
        "        else:\n",
        "            print(f\"{feature}: {coef:.4f}\")"
      ],
      "metadata": {
        "id": "FYbw9_9X1aC7"
      },
      "id": "FYbw9_9X1aC7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.2. Random Forest"
      ],
      "metadata": {
        "id": "rs_Rk3ZsbySX"
      },
      "id": "rs_Rk3ZsbySX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model with specific parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "# Train and evaluate the RF model\n",
        "\n",
        "y_pred_rf= rf_model.predict(X_test)\n",
        "y_val_rf = rf_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_rf\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_rf_decoded = label_encoder.inverse_transform(y_pred_rf)\n",
        "y_val_rf_decoded = label_encoder.inverse_transform(y_val_rf)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Random Forest Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_rf_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Random Forest Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_rf_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "DgZUJdCkA513"
      },
      "id": "DgZUJdCkA513",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desicion Tree with specific parameters\n",
        "dt_model =DecisionTreeClassifier(criterion = 'gini',\n",
        "                                 max_depth = 20,\n",
        "                                 min_samples_leaf = 2,\n",
        "                                 min_samples_split = 10,\n",
        "                                 random_state=42)\n",
        "\n",
        "\n",
        "# Train and evaluate the DT model\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt= dt_model.predict(X_test)\n",
        "y_val_dt = dt_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_dt\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_dt_decoded = label_encoder.inverse_transform(y_pred_dt)\n",
        "y_val_dt_decoded = label_encoder.inverse_transform(y_val_dt)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Decision Trees Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_dt_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Decision Trees Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_dt_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "7uk4_l3g7HWE"
      },
      "id": "7uk4_l3g7HWE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.3 XGBoost"
      ],
      "metadata": {
        "id": "MSIJ0KwohOrB"
      },
      "id": "MSIJ0KwohOrB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the XGBoost model with specific parameters\n",
        "xgb_model = XGBClassifier(learning_rate=0.2, max_depth=10, n_estimators=200)\n",
        "\n",
        "# Train and evaluate the XGBoost model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_xgb= xgb_model.predict(X_test)\n",
        "y_val_xgb = xgb_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_xgb\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_xgb_decoded = label_encoder.inverse_transform(y_pred_xgb)\n",
        "y_val_xgb_decoded = label_encoder.inverse_transform(y_val_xgb)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"XGB Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_xgb_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"XGB Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_xgb_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "Y6zdbCy-afec"
      },
      "id": "Y6zdbCy-afec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Conclusion."
      ],
      "metadata": {
        "id": "iFbPQcpw74-o"
      },
      "id": "iFbPQcpw74-o"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
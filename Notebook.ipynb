{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70934233-4e52-4f0f-883b-56cbc48f299c",
      "metadata": {
        "id": "70934233-4e52-4f0f-883b-56cbc48f299c"
      },
      "source": [
        "# Twitter Sentiment Analysis of Public Reaction to COVID-19 News\n",
        "\n",
        "**Project Overview:**\n",
        "\n",
        "This project aims to analyze a large dataset of COVID-19-related tweets to understand how public sentiment evolves and spreads in response to news announcements and events. By leveraging natural language processing (NLP) techniques and sentiment analysis models, we seek to gain valuable insights into the dynamics of online conversations surrounding the pandemic.\n",
        "\n",
        "**Importance and Motivation:**\n",
        "\n",
        "Understanding public sentiment during a global crisis like the COVID-19 pandemic is crucial for various stakeholders, including:\n",
        "\n",
        "- **Public Health Officials:** To gauge public response to health policies and interventions.\n",
        "- **Media Outlets:** To assess the impact of their news coverage on public perception.\n",
        "- **Government Agencies:** To monitor public opinion and tailor communication strategies.\n",
        "- **Researchers:** To study the spread of information and misinformation on social media.\n",
        "\n",
        "This project contributes to this understanding by providing a comprehensive analysis of Twitter data, revealing trends and patterns in public sentiment related to COVID-19.\n",
        "\n",
        "Notebook Structure\n",
        "---\n",
        "<details>\n",
        "<summary><b>1. Business Problem and Objectives</b></summary>\n",
        "   Define the problem being addressed and its relevance to real-world scenarios.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>2. Data Acquisition and Preparation</b></summary>\n",
        "\n",
        "- ### **2.1 Data Source and Download**  \n",
        "  Explanation of the dataset source and how it was obtained.  \n",
        "\n",
        "- ### **2.2 Installing Required Modules**  \n",
        "  List and install the libraries needed for the project.  \n",
        "\n",
        "- ### **2.3 Importing Modules and Global Variables**  \n",
        "  Set up imports and define constants or global variables.  \n",
        "\n",
        "- ### **2.4 Defining Supplemental Functions**  \n",
        "  Helper functions to streamline data processing.  \n",
        "\n",
        "- ### **2.5 Data Loading**  \n",
        "  Load the dataset into a DataFrame or suitable data structure.  \n",
        "\n",
        "- ### **2.6 Basic Data Understanding**  \n",
        "  Perform initial data exploration, including shape, columns, and types.  \n",
        "\n",
        "- ### **2.7 Data Cleaning and EDA: Date**  \n",
        "  Extract and analyze temporal trends in the dataset.\n",
        "\n",
        "- ### **2.8 Data Cleaning and EDA: Language**  \n",
        "  Extract relevant language subset.\n",
        "\n",
        "- ### **2.9 Data Cleaning and EDA: Location**  \n",
        "  Process location data to standardize and extract insights.  \n",
        "\n",
        "- ### **2.10 Data Cleaning and EDA: Source**  \n",
        "  Analyze the platforms from which tweets were sent.  \n",
        "\n",
        "- ### **2.11 Data Cleaning and EDA: Sentiment**  \n",
        "  Explore sentiment labels and their distribution.  \n",
        "\n",
        "- ### **2.12 Exploratory Data Analysis (EDA): Social Connections**  \n",
        "  Investigate user mentions, retweets, and network connections.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>3. Text Preprocessing and Feature Engineering</b></summary>\n",
        "\n",
        "- ### **3.1 Cleaning**  \n",
        "  Remove noise, including special characters, links, mentions, and hashtags.  \n",
        "\n",
        "- ### **3.2 Preprocessing**  \n",
        "  Tokenize, lemmatize, and remove stop words from the text data.  \n",
        "\n",
        "- ### **3.3 Feature Extraction**  \n",
        "  Generate n-grams, TF-IDF features, or embeddings for model input.  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>4. Sentiment Analysis</b></summary>\n",
        "\n",
        "- ### **4.1 Train-Test Data Split**  \n",
        "- ### **4.2 Sentiment Classification with Simple Logistic Regression Model**   \n",
        "  Choose a simple base classification model and train it on the preprocessed data. Assess model performance using metrics like accuracy, precision, and recall.\n",
        "- ### **4.3 Selecting Best Model for Feature Reduction**\n",
        "  Deploy several advanced classification models with feature interpretability.\n",
        "- ### **4.4 Feature Reduction Using Best Advanced Model**\n",
        "  Reducing dataset to most important features from best performing model.        \n",
        "- ### **4.5 Tuning Hyperparameters for Logistic Regression Model**\n",
        "  Use parameter grid search to find best-performing model.  \n",
        "- ### **4.6 Improved Model Performance**\n",
        "  Assess model performance using metrics like accuracy, precision, and recall.\n",
        "- ### **4.7 Feature Interpretation**\n",
        "  Visualize results and discuss findings, including strengths and limitations.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>5. Conclusion</b></summary>\n",
        "Summarize work.\n",
        "Summarize findings, including strengths and limitations.\n",
        "Suggest future work.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc",
      "metadata": {
        "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc"
      },
      "source": [
        "## 1.1 Business Problem and Objectives\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "Media outlets and public health organizations need a better understanding of how their COVID-19-related news and announcements influence public sentiment on Twitter. This project addresses this need by analyzing a large dataset of tweets to identify and track sentiment trends in response to news events.\n",
        "\n",
        "**Key Questions:**\n",
        "\n",
        "- How do positive and negative sentiments spread among users following a COVID-19 news announcement?\n",
        "- What are the key topics and themes associated with different sentiment trends?\n",
        "- Can we identify any patterns or correlations between news events and changes in public sentiment?\n",
        "\n",
        "**Project Objectives:**\n",
        "\n",
        "- To develop a robust NLP pipeline for cleaning, preprocessing, and analyzing Twitter data.\n",
        "- To apply sentiment analysis models to classify tweets and track sentiment trends over time.\n",
        "- To visualize and interpret the sentiment analysis results to provide actionable insights.\n",
        "- To potentially identify key influencers and networks driving sentiment on Twitter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VveiYpccgaRY",
      "metadata": {
        "id": "VveiYpccgaRY"
      },
      "source": [
        "# 2. Data Acquisition and Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lSJyODVEaJZs",
      "metadata": {
        "id": "lSJyODVEaJZs"
      },
      "source": [
        "## 2.1. Data Understanding\n",
        "\n",
        "This section outlines the source of the data used in this project,  and provides instructions for downloading it.\n",
        "\n",
        "**Data Sources:**\n",
        "\n",
        "1. **Covid-19 Twitter Dataset:** The primary dataset for this Twitter sentiment analysis project is the \"Covid-19 Twitter Dataset\" available on Kaggle. This dataset contains a large collection of tweets related to COVID-19, including tweet text, user details, location, and sentiment labels.\n",
        "\n",
        "2. **GloVe Embeddings:** To enhance the analysis, we will utilize pre-trained GloVe embeddings from Stanford NLP. These word embeddings capture semantic relationships between words and can improve the performance of NLP models.\n",
        "\n",
        "**Data Relevance**:\n",
        "\n",
        "* The Covid-19 Twitter Dataset contains a vast collection of tweets related to the pandemic, providing a valuable source of public opinion and sentiment during this period.\n",
        "* This dataset is suitable for our project because it includes sentiment labels, allowing us to train and evaluate sentiment analysis models.\n",
        "\n",
        "\n",
        "**Data Limitations**\n",
        "\n",
        "* Recent changes to the Twitter API have significantly impacted the accessibility of tweet data for research and analysis.  Specifically, Twitter has severely restricted free API access. This means that retrieving the original dataset used in this project [(see Panacea's lab github page)](https://github.com/thepanacealab/covid19_twitter) is no longer possible without incurring substantial costs.\n",
        "* This dataset contains pre-computed sentiment labels, however, we don't know what method was used and how the accuracy of the sentiment was evaluated.\n",
        "\n",
        "**Download Instructions:**\n",
        "\n",
        "1. **Kaggle Dataset:** The dataset can be accessed and downloaded from the following Kaggle page:\n",
        "    [Covid-19 Twitter Dataset](https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset/data)\n",
        "\n",
        "2. ** GloVe Embeddings:** The pre-trained GloVe embeddings can be obtained from the [Stanford NLP website](https://nlp.stanford.edu/projects/glove/). For this project, we will use the \"glove.twitter.27B.zip\" file, which contains 10-200-dimensional embeddings trained on 27B Twitter tokens.\n",
        "\n",
        "**Data Storage:**\n",
        "\n",
        "   - **Local Execution:** If you are running the notebook locally, please download the dataset files and place them into a folder named `Data` within your project directory.\n",
        "  \n",
        "   - **Colab Environment:** If you are using Google Colab, use kaggle's API to download the data directly from Kaggle, or upload manually.\n",
        "\n",
        "**Default dataset directory should be `Data` if you're running locally, or `content/Data`, if you're running from Colab and just executing the notebook.**\n",
        "\n",
        "\n",
        "\n",
        "**Data Loading:**\n",
        "\n",
        "1. **Loading Datasets:** code would check runtime environment and for local environment set the `data_dir` to be `Data` folder on the same level as the notebook. If you're using Colab, default data directory would be `/content/Data`. Highly recommend to not use runtime-dependent directory.   \n",
        "2. **Loading GloVe Embeddings:** The script will automatically check if the GloVe embeddings are already present in the 'Data' directory. If not, it will download and extract them for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fQwFcnPjZpd9",
      "metadata": {
        "id": "fQwFcnPjZpd9"
      },
      "source": [
        "## 2.2 Installing Required Modules\n",
        "\n",
        "This section focuses on installing the necessary Python libraries and packages required for our Twitter sentiment analysis project. We accomplish this through the following steps:\n",
        "\n",
        "1. **Requirements File:**\n",
        "    - We retrieve the list of required packages from a `requirements.txt` file hosted on GitHub using `wget`. This file contains the names and versions of all the dependencies.\n",
        "    - This ensures that we install the correct versions of the libraries for compatibility and reproducibility.\n",
        "    - Here's the link to the requirements file on GitHub:\n",
        "       `https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt`\n",
        "\n",
        "2. **Installation using pip:**\n",
        "    - We use Python's `pip` package manager to install the libraries listed in the `requirements.txt` file.\n",
        "    - The `-r` flag instructs `pip` to read the requirements file and install all the packages listed within."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gDkE92vOyrD1",
      "metadata": {
        "id": "gDkE92vOyrD1"
      },
      "source": [
        "Getting Data from **Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "k9WcmlbSzpRd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9WcmlbSzpRd",
        "outputId": "51a93338-2179-4386-e9d1-bcc22b5c159d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading covid19-twitter-dataset.zip to /content\n",
            " 78% 38.0M/48.7M [00:00<00:00, 93.1MB/s]\n",
            "100% 48.7M/48.7M [00:00<00:00, 99.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d arunavakrchakraborty/covid19-twitter-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "q1NuVNtJ0ApD",
      "metadata": {
        "id": "q1NuVNtJ0ApD"
      },
      "outputs": [],
      "source": [
        "# Setup Module\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "#Checks if directory exists\n",
        "def ensure_directory(path):\n",
        "    \"\"\"\n",
        "    Ensure that a directory exists. If not, create it.\n",
        "    \"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Directory ensured: {path}\")\n",
        "# DOwnloads files\n",
        "def download_files(base_url, file_names, destination_dir):\n",
        "    \"\"\"\n",
        "    Download a list of files from a base URL to a specified directory.\n",
        "\n",
        "    Args:\n",
        "    - base_url (str): The base URL for the files.\n",
        "    - file_names (list): List of filenames to download.\n",
        "    - destination_dir (str): Directory to save the downloaded files.\n",
        "    \"\"\"\n",
        "    for file_name in file_names:\n",
        "        url = f\"{base_url}/{file_name}\"\n",
        "        dest_path = os.path.join(destination_dir, file_name)\n",
        "        if not os.path.exists(dest_path):\n",
        "            print(f\"Downloading {file_name}...\")\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            with open(dest_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded: {file_name}\")\n",
        "        else:\n",
        "            print(f\"File already exists: {file_name}\")\n",
        "# Unzips into directory\n",
        "def unzip_dataset(zip_path, destination_dir):\n",
        "    \"\"\"\n",
        "    Unzip a dataset into the specified directory.\n",
        "\n",
        "    Args:\n",
        "    - zip_path (str): Path to the zip file.\n",
        "    - destination_dir (str): Directory to extract the zip contents.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_dir)\n",
        "    print(f\"Unzipped: {zip_path} to {destination_dir}\")\n",
        "# Supplemental function to determine data directory\n",
        "# Input: none\n",
        "# Output: Data directory, depending on runtime environment.\n",
        "\n",
        "def determine_data_dir():\n",
        "    \"\"\"\n",
        "    Determines the data directory based on the execution environment:\n",
        "    - Local: Uses 'Data' directory in the current working directory.\n",
        "    - Cloud (e.g., Google Colab): Uses '/content' as the data directory.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the appropriate data directory.\n",
        "    \"\"\"\n",
        "    if 'COLAB_GPU' in os.environ:  # Check if running in Google Colab\n",
        "        data_dir = \"/content/Data\"\n",
        "        print(f\"Running in Google Colab. Using data directory: {data_dir}\")\n",
        "    else:\n",
        "        data_dir = os.path.join(os.getcwd(), \"Data\")\n",
        "        print(f\"Running locally. Using data directory: {data_dir}\")\n",
        "\n",
        "        # Ensure the 'Data' directory exists locally\n",
        "        if not os.path.isdir(data_dir):\n",
        "            print(f\"The directory '{data_dir}' does not exist. Please create it and place the data files there.\")\n",
        "            raise FileNotFoundError(f\"'{data_dir}' directory is required for local execution.\")\n",
        "\n",
        "    return data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ly7KPfvk6JTx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly7KPfvk6JTx",
        "outputId": "f9a1ef45-f39e-4efe-98e9-82adf129114c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/Data\n",
            "Directory ensured: /content/Data\n",
            "Directory ensured: /content/Model\n",
            "Directory ensured: /content/Images\n",
            "Unzipped: covid19-twitter-dataset.zip to /content/Data\n",
            "Downloading coordinate_cache.json...\n",
            "Downloaded: coordinate_cache.json\n",
            "Downloading location_cache.json...\n",
            "Downloaded: location_cache.json\n",
            "Downloading cleaned_text_df.csv...\n",
            "Downloaded: cleaned_text_df.csv\n",
            "Downloading trained_model.pkl...\n",
            "Downloaded: trained_model.pkl\n",
            "Downloading trained_base_model.pkl...\n",
            "Downloaded: trained_base_model.pkl\n",
            "Downloading trained_dt_model.pkl...\n",
            "Downloaded: trained_dt_model.pkl\n",
            "Downloading trained_sgd_model.pkl...\n",
            "Downloaded: trained_sgd_model.pkl\n",
            "Downloading trained_xgb_model.pkl...\n",
            "Downloaded: trained_xgb_model.pkl\n",
            "Downloading glove.twitter.27B.zip...\n",
            "Downloaded: glove.twitter.27B.zip\n",
            "Extracted glove.twitter.27B.200d.txt to /content/Data\n",
            "Removed ZIP file: /content/Data/glove.twitter.27B.zip\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "# Directories\n",
        "data_dir = determine_data_dir()\n",
        "# Get the parent directory of data_dir\n",
        "base_dir = os.path.dirname(data_dir)\n",
        "models_dir = os.path.join(base_dir, \"Model\")\n",
        "images_dir = os.path.join(base_dir, \"Images\")\n",
        "\n",
        "# Ensure directories exist\n",
        "ensure_directory(data_dir)\n",
        "ensure_directory(models_dir)\n",
        "ensure_directory(images_dir)\n",
        "\n",
        "# Unzip dataset\n",
        "unzip_dataset(\"covid19-twitter-dataset.zip\", data_dir)\n",
        "\n",
        "# Download supplemental data\n",
        "github_base_url = \"https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/Data\"\n",
        "supplemental_files = [\n",
        "    \"coordinate_cache.json\",\n",
        "    \"location_cache.json\",\n",
        "    \"cleaned_text_df.csv\"\n",
        "]\n",
        "download_files(github_base_url, supplemental_files, data_dir)\n",
        "\n",
        "# Download model into Models directory\n",
        "# \"trained_rf_model.pkl\" is too large to fit into github\n",
        "model_base_url = \"https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/Model\"\n",
        "model_files = [\n",
        "    \"trained_model.pkl\",\n",
        "    \"trained_base_model.pkl\",\n",
        "    \"trained_dt_model.pkl\",\n",
        "    \"trained_sgd_model.pkl\",\n",
        "    \"trained_xgb_model.pkl\"\n",
        "]\n",
        "\n",
        "download_files(model_base_url, model_files, models_dir)\n",
        "\n",
        "# GloVe URL and File Information\n",
        "glove_zip_url = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
        "glove_zip_name = \"glove.twitter.27B.zip\"\n",
        "glove_file = \"glove.twitter.27B.200d.txt\"  # The specific file to extract\n",
        "zip_file_path = os.path.join(data_dir, glove_zip_name)\n",
        "\n",
        "# Download GloVe embeddings (entire ZIP file)\n",
        "download_files(\n",
        "    base_url=\"http://nlp.stanford.edu/data\",\n",
        "    file_names=[glove_zip_name],\n",
        "    destination_dir=data_dir\n",
        ")\n",
        "\n",
        "# Extract only the specified GloVe file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Check if the desired file exists in the ZIP\n",
        "    if glove_file in zip_ref.namelist():\n",
        "        # Extract only the specified file\n",
        "        zip_ref.extract(glove_file, data_dir)\n",
        "        print(f\"Extracted {glove_file} to {data_dir}\")\n",
        "    else:\n",
        "        print(f\"{glove_file} not found in the ZIP archive.\")\n",
        "\n",
        "# Optional: Remove the ZIP file to save space\n",
        "os.remove(zip_file_path)\n",
        "print(f\"Removed ZIP file: {zip_file_path}\")\n",
        "\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uZMWk2nB5Xy6",
      "metadata": {
        "id": "uZMWk2nB5Xy6"
      },
      "source": [
        "Installing modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "st62fukEaHir",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st62fukEaHir",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "fe9e9714-6151-4f53-b1cb-6aa809c9a2e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-10 04:17:59--  https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     152  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-10 04:17:59 (6.48 MB/s) - ‘requirements.txt’ saved [152/152]\n",
            "\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.8.1)\n",
            "Collecting cartopy (from -r requirements.txt (line 7))\n",
            "  Downloading Cartopy-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.19.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.9.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (11.1.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
            "Collecting emot (from -r requirements.txt (line 14))\n",
            "  Downloading emot-3.1-py3-none-any.whl.metadata (396 bytes)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.4.1)\n",
            "Collecting emoji (from -r requirements.txt (line 18))\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.4.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (7.34.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (3.7.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
            "Requirement already satisfied: jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from branca->-r requirements.txt (line 6)) (3.1.5)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (2.0.6)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2024.12.14)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/dist-packages (from folium->-r requirements.txt (line 9)) (2024.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy->-r requirements.txt (line 15)) (2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 20))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (4.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (0.15.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (2.10.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->-r requirements.txt (line 21)) (3.5.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 20)) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3->branca->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 21)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 20)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 20)) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 21)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 21)) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 21)) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 21)) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 21)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 21)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 21)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 21)) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 21)) (7.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 21)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 21)) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 21)) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 21)) (0.1.2)\n",
            "Downloading Cartopy-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot, jedi, emoji, cartopy\n",
            "Successfully installed cartopy-0.24.1 emoji-2.14.0 emot-3.1 jedi-0.19.2\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
        "!pip install -r 'requirements.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zd0e2IvvfuDJ",
      "metadata": {
        "id": "Zd0e2IvvfuDJ"
      },
      "source": [
        "## 2.3 Importing Modules and Global Variables\n",
        "\n",
        "This section focuses on setting up the necessary environment for our analysis by importing the required Python modules and declaring global variables. We perform the following:\n",
        "\n",
        "1. **Module Imports:** We import a variety of modules that will be essential for data manipulation, analysis, visualization, and natural language processing tasks. These modules include:\n",
        "\n",
        "    - **Built-in Modules:** `os`, `string`, `re`, `glob`, `time`, `datetime`, `concurrent.futures`, `json`, `collections`, `concurrent`.\n",
        "    - **Data Processing and Analysis:** `numpy`, `pandas`.\n",
        "    - **Visualization:** `matplotlib.pyplot`, `seaborn`, `networkx`, `folium`, `branca.colormap`, `cartopy`.\n",
        "    - **Natural Language Processing (NLP):** `nltk`, `emot`, `emoji`.\n",
        "    - **Machine Learning:** `sklearn`.\n",
        "\n",
        "2. **Global Variable Declarations:**\n",
        "    - We define and initialize global variables that will be used throughout the analysis. These include:\n",
        "        - `geolocator`: An instance of the `Nominatim` geolocator from the `geopy` library for location standardization.\n",
        "        - `tqdm`: Enabling progress bars for long computations using `tqdm.pandas()`.\n",
        "        - `stop_words`: A set of English stop words from `nltk.corpus` for text preprocessing.\n",
        "        - `lemmatizer`: An instance of the `WordNetLemmatizer` from `nltk.stem` for lemmatization.\n",
        "\n",
        "3. **Downloading NLP Resources:**\n",
        "    - We download necessary resources for NLP tasks, such as stopwords, wordnet, and punkt using `nltk.download()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "py58BnTUdbsd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py58BnTUdbsd",
        "outputId": "ae140019-c8ae-474b-a61a-c8f7cd7b5bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# built-in modules\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "# Optimization with Parallel Computing:\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "# url processing for extracting the coordinates\n",
        "import requests\n",
        "# progress bar monitoring\n",
        "from tqdm import tqdm\n",
        "# data manupulation, analysis, sparce matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import rand, randint\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "# general data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# networkx for user connection visualization\n",
        "import networkx as nx\n",
        "# world maps\n",
        "import folium\n",
        "from folium import plugins\n",
        "from folium.plugins import HeatMap\n",
        "import branca.colormap as cm\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
        "# displaying the folium heatmap\n",
        "from IPython.display import display, HTML\n",
        "#for location standartization\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "# world cloud\n",
        "from wordcloud import WordCloud\n",
        "# Natural Language Processing (NLP)\n",
        "from emot.emo_unicode import UNICODE_EMOJI\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# save python-specific file format\n",
        "import pickle\n",
        "# Classification\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score, make_scorer, precision_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "%matplotlib inline\n",
        "## GLOBAL VARIABLES\n",
        "# stop words for tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Initialize geolocator globally for efficient geocoding cache\n",
        "geolocator = Nominatim(user_agent=\"batch-geocoding\")\n",
        "# Enable tqdm for pandas, progress bar for long computations\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "auiqucUetmLC",
      "metadata": {
        "id": "auiqucUetmLC"
      },
      "source": [
        "## 2.4 Defining Supplemental Functions\n",
        "\n",
        "* ```def display_categorical_vals(df)```\n",
        "* ```def has_special_chars(location) ```\n",
        "* ```def save_cache_to_json(cache, file_path=\"location_cache.json\")```\n",
        "* ```def load_cache_to_json(cache, file_path=\"location_cache.json\")```\n",
        "* ```def geocode_location(location)```\n",
        "* ```def batch_geocode(locations)```\n",
        "* ```def extract_word(location, position=\"first\")```\n",
        "* ```def split_geocoded_location(location)```\n",
        "* ```def get_coordinates(input_type, name, output_as='center', retries=3, delay=5)```\n",
        "* ```def add_coordinates_with_progress(df, city_col='city', state_col='state', country_col='country')```\n",
        "* ```def color(magnitude)```\n",
        "* ```def generateBaseMap(input_type, df, default_location=[40.693943, -73.985880], default_zoom_start=2)```\n",
        "* ```def extract_html_source(source_text)```\n",
        "* ```def replace_emoticons_with_emojis(text)```\n",
        "* ```def process_tweet_data(tweet, emoji_list=None)```\n",
        "* ``` def clean_tweets_with_progress_parallel(df, text_col='original_text', num_processes=6)```\n",
        "* ```def preprocess_text(df, text_column)```\n",
        "* ```def compute_ngrams(df, text_column, ngram_range=(2, 3), max_features=5000)```\n",
        "\n",
        " **Optional**\n",
        "\n",
        "* ``` def fitness_function(selected_features, X, y, model, cv=3)```\n",
        "* ```def update_gwo(population, alpha, beta, delta,a)```\n",
        "* ```def hybrid_gwo_abc(X, y, model, n_wolves=10, n_iter=20)```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "jamXGPHa6A9n",
      "metadata": {
        "id": "jamXGPHa6A9n"
      },
      "outputs": [],
      "source": [
        "# Supplemental function will display unique values for all categorical columns in a dataframe.\n",
        "def display_categorical_vals(df):\n",
        "    # select categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # print categorical columns and their unique values\n",
        "    for col in categorical_columns:\n",
        "        unique_values = df[col].unique()\n",
        "        print(f\"Column '{col}' has unique values: {unique_values}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "713FrV2huiiK",
      "metadata": {
        "id": "713FrV2huiiK"
      },
      "outputs": [],
      "source": [
        "# define a function to check for special characters\n",
        "def has_special_chars(location):\n",
        "    return bool(re.search(r'[^\\w\\s,.-]', location))  # check for non-alphanumeric and non-space chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "jWjMn4eshLyv",
      "metadata": {
        "id": "jWjMn4eshLyv",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def save_cache_to_json(cache, file_path=\"location_cache.json\"):\n",
        "    \"\"\"\n",
        "    Saves the location cache to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        cache (dict): The cache dictionary to save.\n",
        "        file_path (str): The file path where the cache will be saved.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(cache, f)\n",
        "    print(f\"Cache saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Za01dz-NhMjF",
      "metadata": {
        "id": "Za01dz-NhMjF",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def load_cache_from_json(file_path=\"location_cache.json\"):\n",
        "    \"\"\"\n",
        "    Loads the location cache from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The file path from where the cache will be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded cache dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            cache = json.load(f)\n",
        "        print(f\"Cache loaded from {file_path}\")\n",
        "        return cache\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No cache file found at {file_path}. Starting with an empty cache.\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "rbvEwHw9ut7F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvEwHw9ut7F",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "6d2a36cb-ed5a-436e-8b02-f2f987edc3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/Data\n",
            "Cache loaded from /content/Data/location_cache.json\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to cache geocoding results\n",
        "try:\n",
        "    # Initialize cache from file\n",
        "    data_dir = determine_data_dir()\n",
        "    location_cache_file = os.path.join(data_dir, \"location_cache.json\")  # Path to cache file\n",
        "    location_cache = load_cache_from_json(location_cache_file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, initialize an empty cache\n",
        "    location_cache = {}\n",
        "\n",
        "# Supplemental function to use a geocoding API for location resolution\n",
        "def geocode_location(location):\n",
        "    \"\"\"\n",
        "    Resolve location using a geocoding API with caching.\n",
        "    Returns results in City, State, Country format.\n",
        "    \"\"\"\n",
        "    # Check cache first\n",
        "    if location in location_cache:\n",
        "        return location_cache[location]\n",
        "\n",
        "    try:\n",
        "        # Add a delay to respect API rate limits\n",
        "        geo = geolocator.geocode(location, addressdetails=True, exactly_one=True, timeout=10)\n",
        "        if geo:\n",
        "            # Default: Extract the address components\n",
        "            address = geo.raw.get('address', {})\n",
        "            city = address.get('city') or address.get('town') or address.get('village') or address.get('hamlet')\n",
        "            state = address.get('state')\n",
        "            country = address.get('country')\n",
        "\n",
        "            # Fallback: Parse city and country from display_name if missing\n",
        "            if not city:\n",
        "                try:\n",
        "                    city = geo.raw['display_name'].split(',')[0].strip()\n",
        "                except (KeyError, IndexError):\n",
        "                    city = \"Unknown\"\n",
        "            if not country:\n",
        "                try:\n",
        "                    country = geo.raw['display_name'].split(',')[-1].strip()\n",
        "                except (KeyError, IndexError):\n",
        "                    country = \"Unknown\"\n",
        "            # Fallback: Parse state dynamically from display_name if missing or ambiguous\n",
        "            if not state:\n",
        "                try:\n",
        "                    components = geo.raw['display_name'].split(',')\n",
        "                    components = [comp.strip() for comp in components]\n",
        "                    for i in range(len(components) - 1, -1, -1):  # Iterate backwards\n",
        "                        if 'County' not in components[i] and \"United States\" not in components[i]:\n",
        "                            state = components[i]\n",
        "                            break\n",
        "                except (KeyError, IndexError):\n",
        "                    state = \"Unknown\"\n",
        "\n",
        "            # Avoid redundancy: \"Country, Unknown, Country\"\n",
        "            if city == country:\n",
        "                city = \"Unknown\"\n",
        "            if state == country:\n",
        "                state = \"Unknown\"\n",
        "\n",
        "            # Construct the result in the desired format\n",
        "            result = f\"{city}, {state}, {country}\"\n",
        "        else:\n",
        "            result = \"Unknown\"\n",
        "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
        "        result = f\"Error: {e}\"\n",
        "\n",
        "    # Cache the result\n",
        "    location_cache[location] = result\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "dX7unUcQvWrg",
      "metadata": {
        "id": "dX7unUcQvWrg",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# geocode multiple locations in parallel using ThreadPoolExecutor\n",
        "def batch_geocode(locations):\n",
        "    \"\"\"\n",
        "    Geocode multiple locations in parallel using ThreadPoolExecutor.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:  # max_workers can be adjusted as needed\n",
        "        # Use tqdm to wrap the executor's map method for progress tracking\n",
        "        for result in tqdm(executor.map(geocode_location, locations), \\\n",
        "                           total=len(locations), desc=\"Geocoding Progress\"):\n",
        "            results.append(result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "OojXElKzvuWp",
      "metadata": {
        "id": "OojXElKzvuWp",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# function to extract the first or last word from a location string\n",
        "def extract_word(location, position=\"first\"):\n",
        "    \"\"\"\n",
        "    Extract the first or last word from a location string.\n",
        "\n",
        "    Args:\n",
        "        location (str): The location string to process.\n",
        "        position (str): 'first' to extract the first word, 'last' to extract the last word.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted word or 'Unknown' if the location is empty or invalid.\n",
        "    \"\"\"\n",
        "    words = location.split()\n",
        "    if words:\n",
        "        return words[0] if position == \"first\" else words[-1]\n",
        "    return \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "qCARRM5Mv5R4",
      "metadata": {
        "id": "qCARRM5Mv5R4",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# function to split geocoded_location into City, State, and Country\n",
        "def split_geocoded_location(location):\n",
        "    if pd.notna(location):\n",
        "        parts = location.split(\",\")\n",
        "        parts = [p.strip() for p in parts]  # remove extra whitespace\n",
        "        city = parts[0] if len(parts) > 0 else \"Unknown\"\n",
        "        state = parts[1] if len(parts) > 1 else \"Unknown\"\n",
        "        country = parts[2] if len(parts) > 2 else \"Unknown\"\n",
        "        return city, state, country\n",
        "    return \"Unknown\", \"Unknown\", \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "DfgwG_Z_xB7w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfgwG_Z_xB7w",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "295672d1-7ce6-487b-8926-f3442a083b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/Data\n",
            "Cache loaded from /content/Data/coordinate_cache.json\n"
          ]
        }
      ],
      "source": [
        "# Dictionary to cache coordinates\n",
        "try:\n",
        "    # Initialize cache from file\n",
        "    data_dir = determine_data_dir()\n",
        "    coordinate_cache_file = os.path.join(data_dir, \"coordinate_cache.json\")  # Path to cache file\n",
        "    coordinate_cache = load_cache_from_json(coordinate_cache_file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, initialize an empty cache\n",
        "    coordinate_cache = {}\n",
        "\n",
        "# Update get_coordinates function to include caching\n",
        "def get_coordinates(input_type, name, output_as='center', retries=3, delay=5):\n",
        "    \"\"\"\n",
        "    Fetch coordinates of a city/state/country using Nominatim API with caching and retry logic.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): 'country', 'state', or 'city' to specify the type of input.\n",
        "        name (str): Name of the location.\n",
        "        output_as (str): 'center' or 'boundingbox' for coordinate type.\n",
        "        retries (int): Number of retry attempts.\n",
        "        delay (int): Delay between retries in seconds.\n",
        "\n",
        "    Returns:\n",
        "        list: [latitude, longitude]. Returns [0, 0] on failure.\n",
        "    \"\"\"\n",
        "    # Check the cache first\n",
        "    if name in coordinate_cache:\n",
        "        return coordinate_cache[name]\n",
        "\n",
        "    url = f\"http://nominatim.openstreetmap.org/search\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"ColabGeocoder/1.0 (leksea@gmail.com)\"\n",
        "    }\n",
        "    params = {\n",
        "        input_type: name,\n",
        "        \"format\": \"json\",\n",
        "        \"polygon\": 0\n",
        "    }\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if data:\n",
        "                if output_as == 'center':\n",
        "                    result = [float(data[0]['lat']), float(data[0]['lon'])]\n",
        "                elif output_as == 'boundingbox':\n",
        "                    result = [float(coord) for coord in data[0]['boundingbox']]\n",
        "                else:\n",
        "                    result = [0, 0]\n",
        "                # Cache the result\n",
        "                coordinate_cache[name] = result\n",
        "                return result\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching coordinates for {name}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Retrying in {delay} seconds... ({attempt + 1}/{retries})\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Failed to fetch coordinates for {name} after {retries} attempts.\")\n",
        "                return [0, 0]\n",
        "\n",
        "    # Cache failed attempt as [0, 0] to avoid repeated retries\n",
        "    coordinate_cache[name] = [0, 0]\n",
        "    return [0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7I6beD0fqZyw",
      "metadata": {
        "id": "7I6beD0fqZyw",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# add_coordinates_with_progress function\n",
        "def add_coordinates_with_progress(df, city_col='city', state_col='state', country_col='country'):\n",
        "    \"\"\"\n",
        "    Add latitude and longitude coordinates to a DataFrame based on unique combinations\n",
        "    of City, State, and Country, only for rows where these are not 'Unknown'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        city_col (str): Column name for city.\n",
        "        state_col (str): Column name for state.\n",
        "        country_col (str): Column name for country.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated DataFrame with 'Latitude' and 'Longitude' columns.\n",
        "    \"\"\"\n",
        "    # Create a unique DataFrame of City, State, Country combinations\n",
        "    unique_locations = df[(df[city_col] != 'Unknown') &\n",
        "                          (df[state_col] != 'Unknown') &\n",
        "                          (df[country_col] != 'Unknown')][[city_col, state_col, country_col]].drop_duplicates()\n",
        "\n",
        "    # Define a helper function to fetch coordinates\n",
        "    def fetch_coords(row):\n",
        "        location_name = f\"{row[city_col]}, {row[state_col]}, {row[country_col]}\"\n",
        "        return get_coordinates('city', location_name)\n",
        "\n",
        "    # Add Latitude and Longitude columns to the unique locations\n",
        "    unique_locations[['latitude', 'longitude']] = unique_locations.progress_apply(fetch_coords, axis=1, result_type='expand')\n",
        "\n",
        "    # Create a mapping dictionary for efficient lookup\n",
        "    location_to_coords = unique_locations.set_index([city_col, state_col, country_col])[['latitude', 'longitude']].to_dict('index')\n",
        "\n",
        "    # Initialize Latitude and Longitude in the main DataFrame\n",
        "    df['latitude'], df['longitude'] = 0.0, 0.0\n",
        "\n",
        "    # Map coordinates back to the original DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        key = (row[city_col], row[state_col], row[country_col])\n",
        "        if key in location_to_coords:\n",
        "            df.at[index, 'latitude'] = location_to_coords[key]['latitude']\n",
        "            df.at[index, 'longitude'] = location_to_coords[key]['longitude']\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "KS8rd-tnyU53",
      "metadata": {
        "id": "KS8rd-tnyU53",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Function to determine marker color based on tweet count\n",
        "def color(magnitude):\n",
        "    \"\"\"\n",
        "    Returns a color based on the magnitude using a hot-to-cool color map.\n",
        "    \"\"\"\n",
        "    if magnitude >= 2000:\n",
        "        return 'red'  # Hot color for high magnitude\n",
        "    elif 500 <= magnitude < 2000:\n",
        "        return 'orange'  # Medium-hot color\n",
        "    elif 100 <= magnitude < 500:\n",
        "        return 'yellow'  # Neutral color\n",
        "    elif 50 <= magnitude < 100:\n",
        "        return 'lightblue'  # Medium-cool color\n",
        "    else:\n",
        "        return 'blue'  # Cool color for low magnitude\n",
        "\n",
        "# Function to generate the heatmap\n",
        "def generateBaseMap(input_type, df, default_location=[37.774929, -122.419416], default_zoom_start=2):\n",
        "    \"\"\"\n",
        "    Function to generate a heatmap with markers for tweet distribution.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): 'country' or 'city' to specify the type of heatmap.\n",
        "        df (pd.DataFrame): DataFrame containing latitude, longitude, tweet count, and name.\n",
        "        default_location (list): Default map center location as [latitude, longitude].\n",
        "        default_zoom_start (int): Default zoom level for the map.\n",
        "\n",
        "    Returns:\n",
        "        folium.Map: A folium map object with heatmap and markers.\n",
        "    \"\"\"\n",
        "    # Initialize the base map\n",
        "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
        "    marker_cluster = plugins.MarkerCluster().add_to(base_map)\n",
        "\n",
        "    # Add the heatmap\n",
        "    HeatMap(data=df[['latitude', 'longitude']].values.tolist(), radius=20, max_zoom=13).add_to(base_map)\n",
        "\n",
        "    # Add markers with popups\n",
        "    for lat, lon, tweet_count, name in zip(df['latitude'], df['longitude'], df['tweet_count'], df.iloc[:, 0]):\n",
        "        popup_content = folium.Popup(f\"{name}<br>{tweet_count} tweets\", max_width=300)\n",
        "        folium.Marker(\n",
        "            location=[lat, lon],\n",
        "            popup=popup_content,\n",
        "            icon=folium.Icon(color=color(tweet_count), icon='twitter', prefix='fa')\n",
        "        ).add_to(marker_cluster)\n",
        "\n",
        "    # Add a colormap legend\n",
        "    min_val, max_val = df['tweet_count'].min(), df['tweet_count'].max()\n",
        "    colormap = cm.LinearColormap(colors=['blue', 'yellow', 'red'], vmin=min_val, vmax=max_val)\n",
        "    colormap.caption = f\"{input_type.title()} Distribution of COVID-19 Tweets\"\n",
        "    colormap.add_to(base_map)\n",
        "\n",
        "    return base_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "wGVwTKwrwBN8",
      "metadata": {
        "id": "wGVwTKwrwBN8",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# function to extract readable source from HTML content\n",
        "def extract_html_source(source_text):\n",
        "    \"\"\"\n",
        "    Extracts the readable text (e.g., 'Twitter for Android') from the source HTML string.\n",
        "\n",
        "    Args:\n",
        "        source_text (str): The raw HTML string in the source column.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned, readable source text.\n",
        "    \"\"\"\n",
        "    return re.sub(r'<.*?>', '', str(source_text)).strip()  # remove HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "clWtFweFUJir",
      "metadata": {
        "id": "clWtFweFUJir",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#define a dictionary mapping emoticons to emojis\n",
        "emoticon_to_emoji = {\n",
        "    \":)\": \"😊\",\n",
        "    \":D\": \"😃\",\n",
        "    \":(\": \"☹️\",\n",
        "    \":/\": \"😕\",\n",
        "    \":P\": \"😛\",\n",
        "    \";)\": \"😉\",\n",
        "    \":'(\": \"😢\",\n",
        "    \":o\": \"😮\",\n",
        "    \":|\": \"😐\",\n",
        "    \":))\": \"😂\",\n",
        "    \":*\": \"😘\",\n",
        "    \"xD\": \"😆\"\n",
        "}\n",
        "\n",
        "def replace_emoticons_with_emojis(text):\n",
        "    \"\"\"\n",
        "    Replaces emoticons in the text with corresponding emojis.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with emoticons replaced by emojis.\n",
        "    \"\"\"\n",
        "    # Use regex to find and replace emoticons\n",
        "    for emoticon, emoji in emoticon_to_emoji.items():\n",
        "        text = re.sub(re.escape(emoticon), emoji, text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "RvxDCHurw1Lg",
      "metadata": {
        "id": "RvxDCHurw1Lg"
      },
      "outputs": [],
      "source": [
        "# Preserve hashtags and contractions\n",
        "def process_sentence(text):\n",
        "    \"\"\"\n",
        "    Splits the sentence, preserves words with apostrophes, retains other words,\n",
        "    and processes hashtags.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input sentence.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of processed words.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    processed_words = []\n",
        "    for word in words:\n",
        "        if \"'\" in word:  # Preserve words with apostrophes\n",
        "            processed_words.append(word)\n",
        "        elif word.startswith('#'):  # Process hashtags\n",
        "            processed_words.append(word[1:])  # Remove the '#'\n",
        "        else:  # Retain other words\n",
        "            # Remove punctuation except for apostrophes\n",
        "            cleaned_word = re.sub(r\"[^\\w\\s']\", '', word)\n",
        "            processed_words.append(cleaned_word)\n",
        "    processed_sentence = \" \".join(processed_words)\n",
        "    return processed_sentence\n",
        "\n",
        "# big cleaning function\n",
        "def process_tweet_data(tweet, emoji_list=None):\n",
        "    \"\"\"\n",
        "    Processes a tweet to extract mentions, hashtags, retweets, emojis, hyperlinks, and cleaned text.\n",
        "\n",
        "    Args:\n",
        "        tweet (str): The raw tweet text.\n",
        "        emoji_list (list): List of emojis to extract. Defaults to keys of UNICODE_EMOJI.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with extracted components and cleaned text.\n",
        "    \"\"\"\n",
        "    # ensure input is a string\n",
        "    tweet = str(tweet)\n",
        "    # default emoji list if not provided\n",
        "    if emoji_list is None:\n",
        "        emoji_list = list(UNICODE_EMOJI.keys())\n",
        "\n",
        "    # extract mentions\n",
        "    mentions = re.findall(r'@\\w+', tweet)\n",
        "\n",
        "    # extract hashtags\n",
        "    hashtags = re.findall(r'#\\w+', tweet)\n",
        "\n",
        "    # check for retweets and extract username after RT\n",
        "    retweets = re.findall(r'^RT @(\\w+)', tweet)\n",
        "    retweet_user = retweets[0] if retweets else None\n",
        "\n",
        "    # extract hyperlinks before emoji\n",
        "    hyperlinks = re.findall(r'https?://[^\\s]+|www\\.[^\\s]+', tweet)\n",
        "    tweet = re.sub(r'https?://[^\\s]+|www\\.[^\\s]+', '', tweet) #Remove URL\n",
        "\n",
        "    # extract emojis\n",
        "    tweet = replace_emoticons_with_emojis(tweet)\n",
        "    emojis = ''.join([char for char in tweet if char in emoji_list])\n",
        "\n",
        "    # replace emojis with text\n",
        "    tweet = emoji.demojize(tweet).replace('_', ' ')\n",
        "\n",
        "    # remove mentions, retweets, emojis, and hyperlinks from the tweet, retain hashtags\n",
        "    cleaned_text = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
        "    cleaned_text = re.sub(r'^RT', '', cleaned_text)   # Remove retweets\n",
        "    cleaned_text = ''.join([char for char in cleaned_text if char not in emoji_list])  # Remove emojis\n",
        "\n",
        "    cleaned_text = process_sentence(cleaned_text)\n",
        "\n",
        "    # remove special characters, extra spaces, numbers\n",
        "    cleaned_text = re.sub(r'[{}]'.format(re.escape(string.punctuation).replace(\"'\", '')), '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
        "\n",
        "    #lowercase the text, remove numbers\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return {\n",
        "        'mentions': mentions,\n",
        "        'hashtags': hashtags,\n",
        "        'retweets': retweets,\n",
        "        'emojis': emojis,\n",
        "        'hyperlinks': hyperlinks,\n",
        "        'cleaned_text': cleaned_text\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "novHso6vyxEy",
      "metadata": {
        "id": "novHso6vyxEy"
      },
      "outputs": [],
      "source": [
        "def extract_cleaned_text(tweet):\n",
        "    \"\"\"Helper function to extract 'cleaned_text' from process_tweet_data.\"\"\"\n",
        "    return process_tweet_data(tweet)['cleaned_text']\n",
        "\n",
        "def clean_tweets_with_progress_parallel(df, text_col='original_text', num_processes=6):\n",
        "    \"\"\"\n",
        "    Cleans tweet data in parallel using ProcessPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing tweets.\n",
        "        text_col (str): Column name containing the tweet text.\n",
        "        num_processes (int): Number of processes to use for parallel execution.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with new columns for cleaned text and extracted components.\n",
        "    \"\"\"\n",
        "    with ThreadPoolExecutor(max_workers=num_processes) as executor:\n",
        "         # Extract only the 'cleaned_text' from process_tweet_data using the helper function\n",
        "        cleaned_texts = list(tqdm(executor.map(extract_cleaned_text, df[text_col]),\n",
        "                                   total=len(df), desc=\"Cleaning Tweets\"))\n",
        "\n",
        "    # Return as a pandas Series\n",
        "    return pd.Series(cleaned_texts, index=df.index, name='cleaned_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "E0GoMgylvvTK",
      "metadata": {
        "id": "E0GoMgylvvTK"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Customize stop words\n",
        "important_words = {\"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"being\", \"been\"}\n",
        "filtered_stop_words = stop_words - important_words  # Remove important words from stop words\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Tokenize the input text using TweetTokenizer.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to tokenize.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tokens.\n",
        "    \"\"\"\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "def map_pos_to_wordnet(pos_tag):\n",
        "    \"\"\"\n",
        "    Map POS tag to WordNet format.\n",
        "\n",
        "    Args:\n",
        "        pos_tag (str): POS tag (e.g., \"NN\", \"VB\").\n",
        "\n",
        "    Returns:\n",
        "        str: WordNet-compatible POS tag or wordnet.NOUN by default.\n",
        "    \"\"\"\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(pos_tag[0].upper(), wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemmatize_with_context(sentence):\n",
        "    \"\"\"\n",
        "    Lemmatize tokens in context using POS tagging.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence to lemmatize.\n",
        "\n",
        "    Returns:\n",
        "        list: Lemmatized tokens.\n",
        "    \"\"\"\n",
        "    tokens = tokenize_text(sentence)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return [\n",
        "        lemmatizer.lemmatize(word, map_pos_to_wordnet(pos))\n",
        "        for word, pos in pos_tags\n",
        "    ]\n",
        "# Rejoin contractions\n",
        "def rejoin_contractions(tokens):\n",
        "    \"\"\"\n",
        "    Rejoin tokens that are parts of contractions (e.g., \"i\", \"'\", \"m\" -> \"i'm\",\n",
        "    \"we\", \"'\", \"re\" -> \"we're\").\n",
        "    \"\"\"\n",
        "    new_tokens = []\n",
        "    skip_next = False\n",
        "    for i, token in enumerate(tokens):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "        # Handle contractions with apostrophe and next part\n",
        "        if i < len(tokens) - 2 and tokens[i + 1] in [\"'\", \"’\"] and tokens[i + 2] in [\"m\", \"re\", \"s\", \"ve\", \"d\", \"ll\"]:\n",
        "            # Combine contraction\n",
        "            new_tokens.append(token + \"'\" + tokens[i + 2])\n",
        "            skip_next = True  # Skip the next two tokens\n",
        "        else:\n",
        "            new_tokens.append(token)\n",
        "    return new_tokens\n",
        "#cleans tokens\n",
        "\n",
        "def clean_tokens(tokens, valid_words=None, min_frequency=1):\n",
        "    \"\"\"\n",
        "    Clean lemmatized tokens by applying various filters.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): List of lemmatized tokens.\n",
        "        valid_words (set): Set of valid words (optional).\n",
        "        min_frequency (int): Minimum frequency for a word to be retained.\n",
        "\n",
        "    Returns:\n",
        "        list: Cleaned tokens.\n",
        "    \"\"\"\n",
        "    # Word frequency (if required)\n",
        "    if min_frequency > 1:\n",
        "        word_counts = Counter(word for word in tokens)\n",
        "    else:\n",
        "        word_counts = None\n",
        "\n",
        "    return [\n",
        "        word.lower() for word in tokens\n",
        "        if len(word) > 1  # Remove one-character words\n",
        "        and word.isalpha()  # Remove non-alphabetic tokens\n",
        "        and word.lower() not in filtered_stop_words  # Remove stop words\n",
        "        and (valid_words is None or word.lower() in valid_words)  # Keep valid words only\n",
        "        and (word_counts is None or word_counts[word] >= min_frequency)  # Remove rare words\n",
        "    ]\n",
        "\n",
        "# Preprocess text column\n",
        "def preprocess_text(df, text_column):\n",
        "    \"\"\"\n",
        "    Preprocess text data by tokenizing, handling contractions, removing stop words, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the text column.\n",
        "        text_column (str): Name of the column with raw text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with additional processed columns.\n",
        "    \"\"\"\n",
        "    tqdm.pandas()  # Enable progress bar for pandas\n",
        "\n",
        "    # Ensure the text column is of string type\n",
        "    df[text_column] = df[text_column].astype(str)\n",
        "\n",
        "    # Tokenization and contraction handling\n",
        "    df['tokenized_text'] = (\n",
        "        df[text_column]\n",
        "        .progress_apply(tokenizer.tokenize)\n",
        "        .progress_apply(rejoin_contractions)\n",
        "    )\n",
        "\n",
        "    # Stop word removal\n",
        "    df['filtered_tokens'] = df['tokenized_text'].progress_apply(\n",
        "        lambda tokens: [word for word in tokens if word.lower() not in stop_words]\n",
        "    )\n",
        "    # Context-aware lemmatization\n",
        "    df['lemmatized_tokens'] = df['filtered_tokens'].progress_apply(\n",
        "        lambda tokens: lemmatize_with_context(' '.join(tokens))  # Join tokens into a sentence\n",
        "    )\n",
        "    # Apply cleaning to the DataFrame\n",
        "    cleaned_tokens = df['lemmatized_tokens'].apply(clean_tokens)\n",
        "    df['lemmatized_text'] = cleaned_tokens.apply(lambda tokens: ' '.join(tokens) \\\n",
        "                                                 if isinstance(tokens, list) else tokens)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "vVqQglx_uZJA",
      "metadata": {
        "id": "vVqQglx_uZJA"
      },
      "outputs": [],
      "source": [
        "def compute_ngrams(df, text_column, ngram_range=(2, 3), max_features=5000):\n",
        "    \"\"\"\n",
        "    Computes n-grams from preprocessed text data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the processed text column.\n",
        "        text_column (str): Name of the column with preprocessed text.\n",
        "        ngram_range (tuple): Range of n-grams to compute (e.g., (2, 3) for bi-grams and tri-grams).\n",
        "        max_features (int): Maximum number of n-gram features.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Sparse matrix of n-grams and fitted CountVectorizer.\n",
        "    \"\"\"\n",
        "    # Compute n-grams using CountVectorizer\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    ngram_matrix = vectorizer.fit_transform(df[text_column])\n",
        "\n",
        "    return ngram_matrix, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7gl069T-ASgi",
      "metadata": {
        "id": "7gl069T-ASgi"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings into a dictionary.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the GloVe file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping words to their vector embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "J3ddMTmSAlUE",
      "metadata": {
        "id": "J3ddMTmSAlUE"
      },
      "outputs": [],
      "source": [
        "def get_embedding(word, embeddings_dict, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Retrieve the embedding for a given word or a zero vector if the word is not in the embeddings.\n",
        "\n",
        "    Args:\n",
        "        word (str): Input word.\n",
        "        embeddings_dict (dict): Dictionary of pre-trained embeddings.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Embedding vector for the word.\n",
        "    \"\"\"\n",
        "    return embeddings_dict.get(word, np.zeros(embedding_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5D10sbgTA1AJ",
      "metadata": {
        "id": "5D10sbgTA1AJ"
      },
      "outputs": [],
      "source": [
        "def compute_sentence_embedding(sentence, embeddings_dict, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Compute sentence embedding by averaging word embeddings.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): Input sentence.\n",
        "        embeddings_dict (dict): Dictionary of pre-trained embeddings.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Sentence embedding.\n",
        "    \"\"\"\n",
        "    tokens = sentence.split()  # Tokenize the sentence\n",
        "    token_embeddings = [get_embedding(token, embeddings_dict, embedding_dim) for token in tokens]\n",
        "    return np.mean(token_embeddings, axis=0) if token_embeddings else np.zeros(embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "Cwk8177mDDDw",
      "metadata": {
        "id": "Cwk8177mDDDw"
      },
      "outputs": [],
      "source": [
        "# supplemental GWO-ABC feature selection\n",
        "def fitness_function(selected_features, X, y, model, cv=3, penalty=0.3):\n",
        "    X_selected = X[:, selected_features == 1]\n",
        "\n",
        "    # Avoid empty feature set\n",
        "    if X_selected.shape[1] == 0:\n",
        "        return 1.0  # High error for invalid subsets\n",
        "\n",
        "    # Cross-validation accuracy\n",
        "    score = cross_val_score(model, X_selected, y, cv=cv, scoring='accuracy').mean()\n",
        "\n",
        "    # Penalize small subsets\n",
        "    subset_penalty = penalty * (1 - X_selected.shape[1] / X.shape[1])\n",
        "    return -(score - subset_penalty)  # Negate because optimization minimizes\n",
        "\n",
        "# Initialize GWO population\n",
        "def initialize_population(n_wolves, n_features):\n",
        "    return np.random.randint(2, size=(n_wolves, n_features))\n",
        "\n",
        "def update_gwo(population, alpha, beta, delta, a, threshold=0.5):\n",
        "    n_wolves, n_features = population.shape\n",
        "    new_population = np.copy(population)\n",
        "\n",
        "    for i in range(n_wolves):\n",
        "        for j in range(n_features):\n",
        "            r1, r2 = np.random.rand(), np.random.rand()\n",
        "            A = 2 * a * r1 - a\n",
        "            C = 2 * r2\n",
        "            D_alpha = abs(C * alpha[j] - population[i, j])\n",
        "            X1 = alpha[j] - A * D_alpha\n",
        "\n",
        "            D_beta = abs(C * beta[j] - population[i, j])\n",
        "            X2 = beta[j] - A * D_beta\n",
        "\n",
        "            D_delta = abs(C * delta[j] - population[i, j])\n",
        "            X3 = delta[j] - A * D_delta\n",
        "\n",
        "            # Update position\n",
        "            new_population[i, j] = (X1 + X2 + X3) / 3\n",
        "\n",
        "    return (new_population > threshold).astype(int)\n",
        "\n",
        "# Hybrid optimization (GWO + ABC)\n",
        "def hybrid_gwo_abc(X, y, model, n_wolves=10, n_iter=20):\n",
        "    n_features = X.shape[1]\n",
        "    population = initialize_population(n_wolves, n_features)\n",
        "\n",
        "    alpha, beta, delta = None, None, None\n",
        "    a = 2  # Linear reduction coefficient\n",
        "\n",
        "    for t in range(n_iter):\n",
        "        fitness = np.array([fitness_function(wolf, X, y, model) for wolf in population])\n",
        "\n",
        "        # Sort wolves by fitness\n",
        "        sorted_indices = np.argsort(fitness)\n",
        "        population = population[sorted_indices]\n",
        "\n",
        "        # Update alpha, beta, delta\n",
        "        alpha, beta, delta = population[0], population[1], population[2]\n",
        "\n",
        "        # Update positions using GWO\n",
        "        population = update_gwo(population, alpha, beta, delta, a)\n",
        "\n",
        "        # Apply local ABC exploitation to the top wolves (e.g., top 3)\n",
        "        for i in range(3):\n",
        "            local_search_wolf = np.copy(population[i])\n",
        "            for _ in range(randint(1, 5)):  # Random local updates\n",
        "                feature_idx = randint(0, n_features)\n",
        "                local_search_wolf[feature_idx] = 1 - local_search_wolf[feature_idx]  # Flip feature\n",
        "            # Accept if fitness improves\n",
        "            if fitness_function(local_search_wolf, X, y, model) < fitness[i]:\n",
        "                population[i] = local_search_wolf\n",
        "\n",
        "        # Reduce exploration coefficient\n",
        "        a -= 2 / n_iter\n",
        "\n",
        "    # Return the best solution\n",
        "    best_wolf = population[0]\n",
        "    return best_wolf\n",
        "\n",
        "# ---- example use\n",
        "\n",
        "# Ensure X_bow is dense\n",
        "# X_dense = csr_matrix(X_bow).toarray()\n",
        "\n",
        "# Use fast-converging model\n",
        "# model = LogisticRegression(\n",
        "#    penalty='l1',  # L1 regularization (lasso regression penalty)\n",
        "#    C=0.1,  # Regularization strength (smaller = stronger regularization)\n",
        "#    max_iter=1000,  # Ensure convergence\n",
        "#    solver='liblinear',\n",
        "#    class_weight='balanced',\n",
        "#    random_state=42)\n",
        "\n",
        "# Use the hybrid GWO + ABC optimizer\n",
        "# best_features = hybrid_gwo_abc(X_dense, y_encoded, model)\n",
        "# Apply the best feature mask\n",
        "#selected_indices = np.where(best_features == 1)[0]\n",
        "#X_bow_selected = X_bow[:, selected_indices]\n",
        "\n",
        "#print(f\"Reduced feature set from {X_bow.shape[1]} to {X_bow_selected.shape[1]}\")\n",
        "# Train-test-val split\n",
        "#X_train_opt, X_temp_opt, y_train_opt, y_temp_opt = train_test_split(X_bow_selected, y_encoded, test_size=0.3, random_state=19)\n",
        "#X_val_opt, X_test_opt, y_val_opt, y_test_opt = train_test_split(X_temp_opt, y_temp_opt, test_size=0.5, random_state=19)\n",
        "# Use best model here\n",
        "#xgb_model.fit(X_train_opt, y_train_opt)\n",
        "\n",
        "# Evaluate\n",
        "#y_pred_opt = best_model.predict(X_test_opt)\n",
        "#accuracy = accuracy_score(y_test, y_pred_opt)\n",
        "#print(f\"Test Accuracy for GWO-ABC Feature Reduction: {accuracy:.2f}\")\n",
        "#print(classification_report(y_test_opt, y_pred_opt, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnoqoMM-gr0F",
      "metadata": {
        "id": "hnoqoMM-gr0F"
      },
      "source": [
        "## 2.5 Data Loading\n",
        "\n",
        "This section focuses on loading the COVID-19 Twitter dataset into a Pandas DataFrame for analysis. We will perform the following steps:\n",
        "\n",
        "1. **Data Directory Determination:** Identify the appropriate directory where the data files are stored, considering both local and cloud (Colab) environments.\n",
        "\n",
        "2. **File Identification:** Locate all CSV files within the determined data directory using the `glob` library.\n",
        "\n",
        "3. **Data Loading and Concatenation:**\n",
        "    - Read each CSV file into a separate Pandas DataFrame using `pd.read_csv`.\n",
        "    - Concatenate all the individual DataFrames into a single DataFrame named `data` using `pd.concat`.\n",
        "    - Print information about the loaded data, including its dimensions and a preview of the first few rows.\n",
        "\n",
        "4. **Error Handling:** Implement error handling mechanisms to address potential issues during file loading, such as missing files or incorrect file formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "Gn7kjSy5DgOe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn7kjSy5DgOe",
        "outputId": "b5a0a0ec-6792-4ae8-93d3-8fabd5516a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/Data\n",
            "File: /content/Data/Covid-19 Twitter Dataset (Apr-Jun 2021).csv | Rows: 147475, Columns: 17\n",
            "File: /content/Data/Covid-19 Twitter Dataset (Aug-Sep 2020).csv | Rows: 120509, Columns: 17\n",
            "File: /content/Data/Covid-19 Twitter Dataset (Apr-Jun 2020).csv | Rows: 143903, Columns: 17\n",
            "Data loaded successfully with 411887 rows and 17 columns.\n",
            "             id  created_at  \\\n",
            "0  1.386694e+18  2021-04-26   \n",
            "1  1.386694e+18  2021-04-26   \n",
            "2  1.386694e+18  2021-04-26   \n",
            "3  1.386694e+18  2021-04-26   \n",
            "4  1.386694e+18  2021-04-26   \n",
            "\n",
            "                                              source  \\\n",
            "0  <a href=\"http://twitter.com/download/android\" ...   \n",
            "1  <a href=\"http://twitter.com/download/iphone\" r...   \n",
            "2  <a href=\"http://twitter.com/download/iphone\" r...   \n",
            "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
            "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
            "\n",
            "                                       original_text lang  favorite_count  \\\n",
            "0  RT @VP: The U.S. is working closely with the I...   en             0.0   \n",
            "1  RT @JackPosobiec: Flip-Flop Fauci admits outdo...   en             0.0   \n",
            "2  RT @timmy315: Hi Twitter, I’m Tim Manning, the...   en             0.0   \n",
            "3  RT @itsaadee: Praying for #India as the countr...   en             0.0   \n",
            "4  Rapid Investment in Nursing to Strengthen the ...   en             0.0   \n",
            "\n",
            "   retweet_count  original_author      hashtags user_mentions    place  \\\n",
            "0            0.0  jfd4humanrights           NaN            VP  Alabama   \n",
            "1            0.0      andgrateful           NaN  JackPosobiec      NaN   \n",
            "2          252.0       jlreader8B           NaN      timmy315      NaN   \n",
            "3            2.0    ijennychauhan  India, Covid      itsaadee      NaN   \n",
            "4         7937.0      IJNSJournal           NaN           NaN       UK   \n",
            "\n",
            "                                         clean_tweet  compound    neg    neu  \\\n",
            "0   work close indian govern rapidli deploy addit...    0.0772  0.170  0.638   \n",
            "1  flip flop fauci admit outdoor covid19 transmis...   -0.4019  0.398  0.442   \n",
            "2  hi twitter tim man white hous covid19 suppli c...    0.0000  0.000  1.000   \n",
            "3  pray countri battl worst surg world wit let ir...   -0.4215  0.306  0.522   \n",
            "4  rapid invest nurs strengthen global covid19 re...    0.3182  0.000  0.723   \n",
            "\n",
            "     pos sentiment  \n",
            "0  0.191       pos  \n",
            "1  0.159       neg  \n",
            "2  0.000       neu  \n",
            "3  0.172       neg  \n",
            "4  0.277       pos  \n"
          ]
        }
      ],
      "source": [
        "## Loading the files\n",
        "# determine the data directory\n",
        "# ~~~ Global Variables~~~~\n",
        "data_dir = determine_data_dir()\n",
        "\n",
        "files_pattern = os.path.join(data_dir, \"Covid-19 Twitter Dataset*.csv\")\n",
        "files = glob.glob(files_pattern)\n",
        "\n",
        "# check if files are found\n",
        "if not files:\n",
        "    print(f\"No CSV files found in directory: {data_dir}\")\n",
        "else:\n",
        "     # load and inspect each file\n",
        "    dfs = []  # to store valid DataFrames\n",
        "    for file in files:\n",
        "        try:\n",
        "            # load the DataFrame\n",
        "            df = pd.read_csv(file)\n",
        "            rows, cols = df.shape\n",
        "            print(f\"File: {file} | Rows: {rows}, Columns: {cols}\")\n",
        "\n",
        "            # skip empty files or files with no columns\n",
        "            if rows == 0 or cols == 0:\n",
        "                print(f\"Skipping empty or invalid file: {file}\")\n",
        "                continue\n",
        "\n",
        "            # append to list if valid\n",
        "            dfs.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file}: {e}\")\n",
        "\n",
        "    # concatenate all valid DataFrames\n",
        "    if dfs:\n",
        "        data = pd.concat(dfs, ignore_index=True)\n",
        "        print(f\"Data loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
        "        print(data.head())\n",
        "    else:\n",
        "        print(\"No valid DataFrames to concatenate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75eceDA73wLf",
      "metadata": {
        "id": "75eceDA73wLf"
      },
      "source": [
        "## 2.6 Basic Data Understanding\n",
        "\n",
        "This section focuses on gaining an initial understanding of the dataset's structure and contents.\n",
        "We will perform the following steps:\n",
        "1. **Data Overview:** Examine the basic information about the dataset, including the number of rows, columns, and data types.\n",
        "2. **Column Selection:** Identify and select the relevant columns for the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "tsrDMCYiEfUL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsrDMCYiEfUL",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "c5c7f89d-efcb-4a88-f4d5-9c6e830aa8f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 411887 entries, 0 to 411886\n",
            "Data columns (total 17 columns):\n",
            " #   Column           Non-Null Count   Dtype  \n",
            "---  ------           --------------   -----  \n",
            " 0   id               411883 non-null  float64\n",
            " 1   created_at       411885 non-null  object \n",
            " 2   source           411587 non-null  object \n",
            " 3   original_text    411885 non-null  object \n",
            " 4   lang             411884 non-null  object \n",
            " 5   favorite_count   411884 non-null  float64\n",
            " 6   retweet_count    411884 non-null  float64\n",
            " 7   original_author  411884 non-null  object \n",
            " 8   hashtags         97775 non-null   object \n",
            " 9   user_mentions    295207 non-null  object \n",
            " 10  place            293775 non-null  object \n",
            " 11  clean_tweet      409915 non-null  object \n",
            " 12  compound         411887 non-null  float64\n",
            " 13  neg              411887 non-null  float64\n",
            " 14  neu              411887 non-null  float64\n",
            " 15  pos              411887 non-null  float64\n",
            " 16  sentiment        411887 non-null  object \n",
            "dtypes: float64(7), object(10)\n",
            "memory usage: 53.4+ MB\n"
          ]
        }
      ],
      "source": [
        "# get general info about the dataset\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "l8cIT_Ib3_UI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "l8cIT_Ib3_UI",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "7a610e45-4952-4cf2-97d3-014ca062f931"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id  favorite_count  retweet_count       compound  \\\n",
              "count  4.118830e+05   411884.000000  411884.000000  411887.000000   \n",
              "mean   1.324197e+18        0.216726    1585.174163       0.008415   \n",
              "std    5.902218e+16        6.332250    9423.896052       0.370853   \n",
              "min    1.250000e+18        0.000000       0.000000      -0.992500   \n",
              "25%    1.260000e+18        0.000000       1.000000      -0.102700   \n",
              "50%    1.310000e+18        0.000000      15.000000       0.000000   \n",
              "75%    1.395011e+18        0.000000     243.000000       0.226300   \n",
              "max    1.409140e+18     2923.000000  416923.000000       0.980500   \n",
              "\n",
              "                 neg            neu            pos  \n",
              "count  411887.000000  411887.000000  411887.000000  \n",
              "mean        0.090920       0.807021       0.102052  \n",
              "std         0.152717       0.200474       0.157080  \n",
              "min         0.000000       0.000000       0.000000  \n",
              "25%         0.000000       0.667000       0.000000  \n",
              "50%         0.000000       0.819000       0.000000  \n",
              "75%         0.180000       1.000000       0.200000  \n",
              "max         1.000000       1.000000       1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a28d5c2-85f8-409a-82d8-45765efd99ae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>compound</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4.118830e+05</td>\n",
              "      <td>411884.000000</td>\n",
              "      <td>411884.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "      <td>411887.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.324197e+18</td>\n",
              "      <td>0.216726</td>\n",
              "      <td>1585.174163</td>\n",
              "      <td>0.008415</td>\n",
              "      <td>0.090920</td>\n",
              "      <td>0.807021</td>\n",
              "      <td>0.102052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.902218e+16</td>\n",
              "      <td>6.332250</td>\n",
              "      <td>9423.896052</td>\n",
              "      <td>0.370853</td>\n",
              "      <td>0.152717</td>\n",
              "      <td>0.200474</td>\n",
              "      <td>0.157080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.250000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.992500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.260000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.102700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.667000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.310000e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.819000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.395011e+18</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>0.226300</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.409140e+18</td>\n",
              "      <td>2923.000000</td>\n",
              "      <td>416923.000000</td>\n",
              "      <td>0.980500</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a28d5c2-85f8-409a-82d8-45765efd99ae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a28d5c2-85f8-409a-82d8-45765efd99ae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a28d5c2-85f8-409a-82d8-45765efd99ae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3b9d9aee-f0ee-4830-adbd-60ec4846626b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b9d9aee-f0ee-4830-adbd-60ec4846626b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3b9d9aee-f0ee-4830-adbd-60ec4846626b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.023959288097553e+17,\n        \"min\": 411883.0,\n        \"max\": 1.40914e+18,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.3241974047471785e+18,\n          1.31e+18,\n          411883.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"favorite_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145478.61324083302,\n        \"min\": 0.0,\n        \"max\": 411884.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.21672606850472464,\n          2923.0,\n          6.332250463856097\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 190993.2062383029,\n        \"min\": 0.0,\n        \"max\": 416923.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1585.1741631138864,\n          15.0,\n          411884.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"compound\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145624.02059975133,\n        \"min\": -0.9925,\n        \"max\": 411887.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.008414607647243054,\n          0.0,\n          411887.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145623.97348716456,\n        \"min\": 0.0,\n        \"max\": 411887.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          411887.0,\n          0.0909200436041924,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145623.8184359867,\n        \"min\": 0.0,\n        \"max\": 411887.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          411887.0,\n          0.8070205736039254,\n          0.819\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 145623.9716943737,\n        \"min\": 0.0,\n        \"max\": 411887.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          411887.0,\n          0.10205184431652371,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# info about the numeric columns\n",
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "p7qKCkF41B4j",
      "metadata": {
        "id": "p7qKCkF41B4j",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# select subset of tweets we'll be working with:\n",
        "cols_to_keep = ['id', 'source', 'created_at', 'original_text', \\\n",
        "                'lang', 'favorite_count', 'retweet_count', 'original_author', \\\n",
        "                'hashtags', 'user_mentions', 'place', 'sentiment', 'compound', 'pos', 'neu', 'neg']\n",
        "tweets_df = data[cols_to_keep].copy()\n",
        "\n",
        "#del df\n",
        "\n",
        "del data\n",
        "\n",
        "# drop NaN ids\n",
        "tweets_df.dropna(subset=['id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "Lgl3O8AV0loS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgl3O8AV0loS",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "d2e21c61-56fc-404d-8a41-1c099eebadef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'source' has unique values: ['<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>'\n",
            " '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>'\n",
            " '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>'\n",
            " ... '<a href=\"https://t.co/X9G6ShmQ03\" rel=\"nofollow\">Locksmith App</a>'\n",
            " '<a href=\"https://twitter.com/TechnoJeder\" rel=\"nofollow\">TechnoJeder</a>'\n",
            " '<a href=\"http://jamaica-gleaner.com\" rel=\"nofollow\">GleanerNew</a>']\n",
            "Column 'created_at' has unique values: ['2021-04-26' '2021-04-27' '2021-04-28' '2021-04-29' '2021-04-30'\n",
            " '2021-05-01' '2021-05-03' '2021-05-04' '2021-05-05' '2021-05-07'\n",
            " '2021-05-08' '2021-05-09' '2021-05-10' '2021-05-11' '2021-05-13'\n",
            " '2021-05-14' '2021-05-15' '2021-05-16' '2021-05-18' '2021-05-19'\n",
            " '2021-05-20' '2021-05-21' '2021-05-23' '2021-05-25' '2021-05-26'\n",
            " '2021-05-27' '2021-05-28' '2021-05-29' '2021-05-30' '2021-06-01'\n",
            " '2021-06-02' '2021-06-03' '2021-06-05' '2021-06-07' '2021-06-08'\n",
            " '2021-06-09' '2021-06-11' '2021-06-12' '2021-06-13' '2021-06-14'\n",
            " '2021-06-15' '2021-06-16' '2021-06-17' '2021-06-18' '2021-06-19'\n",
            " '2021-06-24' '2021-06-26' '2021-06-27' '2020-08-20' '2020-08-21'\n",
            " '2020-08-22' '2020-08-23' '2020-08-24' '2020-08-26' '2020-08-28'\n",
            " '2020-08-29' '2020-08-30' '2020-08-31' '2020-09-01' '2020-09-02'\n",
            " '2020-09-03' '2020-09-06' '2020-09-12' '2020-09-13' '2020-09-15'\n",
            " '2020-09-17' '2020-09-19' '2020-09-20' '2020-09-22' '2020-09-23'\n",
            " '2020-09-24' '2020-09-25' '2020-09-26' '2020-09-27' '2020-09-28'\n",
            " '2020-09-29' '2020-09-30' '2020-10-01' '2020-10-02' '2020-10-03'\n",
            " '2020-10-04' nan '2020-10-05' '2020-10-06' '2020-10-07' '2020-10-08'\n",
            " '2020-10-09' '2020-10-10' '2020-10-11' '2020-10-12' '2020-10-13'\n",
            " '2020-10-14' '2020-10-15' '2020-10-16' '2020-10-17' '2020-10-18'\n",
            " '2020-10-19' '2020-10-20' '2020-04-19' '2020-04-22' '2020-04-23'\n",
            " '2020-04-24' '2020-04-25' '2020-04-26' '2020-04-27' '2020-04-28'\n",
            " '2020-04-29' '2020-04-30' '2020-05-01' '2020-05-02' '2020-05-03'\n",
            " '2020-05-04' '2020-05-05' '2020-05-06' '2020-05-07' '2020-05-08'\n",
            " '2020-05-09' '2020-05-10' '2020-05-11' '2020-05-12' '2020-05-13'\n",
            " '2020-05-14' '2020-05-15' '2020-05-16' '2020-05-17' '2020-05-18'\n",
            " '2020-05-19' '2020-05-20' '2020-05-22' '2020-05-23' '2020-05-24'\n",
            " '2020-05-25' '2020-05-26' '2020-05-27' '2020-05-28' '2020-05-29'\n",
            " '2020-05-30' '2020-05-31' '2020-06-01' '2020-06-03' '2020-06-04'\n",
            " '2020-06-05' '2020-06-06' '2020-06-07' '2020-06-08' '2020-06-09'\n",
            " '2020-06-10' '2020-06-11' '2020-06-12' '2020-06-13' '2020-06-14'\n",
            " '2020-06-15' '2020-06-17' '2020-06-19' '2020-06-20']\n",
            "Column 'original_text' has unique values: ['RT @VP: The U.S. is working closely with the Indian government to rapidly deploy additional support and supplies during an alarming COVID-1…'\n",
            " 'RT @JackPosobiec: Flip-Flop Fauci admits outdoor COVID transmission is very low https://t.co/6ztO7pOH1i'\n",
            " 'RT @timmy315: Hi Twitter, I’m Tim Manning, the White House COVID-19 Supply Coordinator. There’s been a lot of confusion around the use of t…'\n",
            " ...\n",
            " 'Another meat processing outbreak - this time in the US.\\n\\nhttps://t.co/zibo0wLUfs'\n",
            " \"RT @HonorDecency: We knew Covid-19 would be spread at the Trump 'Praise me or else' rally today, but\\nwe didn't know the first super-spreade…\"\n",
            " 'RT @DocJeffD: Friends, this story misses the main point: To move forward with opening safely we MUST continue to take COVID-19 precautions.…']\n",
            "Column 'lang' has unique values: ['en' nan]\n",
            "Column 'original_author' has unique values: ['jfd4humanrights' 'andgrateful' 'jlreader8B' ... 'abdalesaid'\n",
            " 'cinemaofdreams' 'BrodskyIgor']\n",
            "Column 'hashtags' has unique values: [nan 'India, Covid' 'Vizag, GITAM, GAYTRI, GITAM, Gayatri' ...\n",
            " 'RememberinNovember' 'nationallottery' 'Somaliland, ChosenForChevening']\n",
            "Column 'user_mentions' has unique values: ['VP' 'JackPosobiec' 'timmy315' ... 'DDMADelhi1' 'AliRazaAhsanBa1'\n",
            " 'StuartBrownFCO']\n",
            "Column 'place' has unique values: ['Alabama' nan 'UK' ... 'Epiphany World' 'burao, somaliland'\n",
            " 'AT THE MOVIES']\n",
            "Column 'sentiment' has unique values: ['pos' 'neg' 'neu']\n"
          ]
        }
      ],
      "source": [
        "display_categorical_vals(tweets_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "psy8TJ7r6Lit",
      "metadata": {
        "id": "psy8TJ7r6Lit"
      },
      "source": [
        "## 2.7 Data Cleaning: Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "bSzfHyP60pCc",
      "metadata": {
        "id": "bSzfHyP60pCc",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#rename date column for clarity and convert to date\n",
        "tweets_df.rename(columns={'created_at': 'date'}, inplace=True)\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5wNyc5aZ8KYM",
      "metadata": {
        "id": "5wNyc5aZ8KYM"
      },
      "source": [
        "Follow by exploratory data analysis:\n",
        "* What were daily tweet patters?\n",
        "* What were the top 20 days with most tweets?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "KWByJUQb8HnZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "KWByJUQb8HnZ",
        "outputId": "b9e732e9-6e30-46fd-c78a-bff2294c0186"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# exploratory analysis: plot number of tweets per day\n",
        "# group by date and count tweets\n",
        "tweets_per_day = tweets_df.groupby(tweets_df['date'].dt.date)['id'].count()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(tweets_per_day.index, tweets_per_day.values)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.title(\"Number of Tweets Per Day\")\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.savefig(os.path.join(images_dir,\"Dates_Dist_2.7.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "tKhB8-_k6oVd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKhB8-_k6oVd",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "9bd113e4-9037-46da-c91f-560127c62257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date\n",
            "2021-06-03    7229\n",
            "2020-05-04    7165\n",
            "2021-05-25    7114\n",
            "2020-09-27    7040\n",
            "2020-09-30    7028\n",
            "2020-04-25    6178\n",
            "2020-10-01    6166\n",
            "2020-05-09    5857\n",
            "2020-05-23    5600\n",
            "2020-04-24    5323\n",
            "2021-06-01    5175\n",
            "2021-06-09    5158\n",
            "2020-05-22    5102\n",
            "2021-05-08    5032\n",
            "2020-05-05    4949\n",
            "2021-05-05    4886\n",
            "2020-05-25    4804\n",
            "2021-05-26    4750\n",
            "2021-05-07    4586\n",
            "2020-05-06    4575\n",
            "Name: id, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# exploratory analysis: list the top 20 days with the most tweets\n",
        "# rename columns for clarity\n",
        "tweets_per_day.columns = ['date', 'tweet_count']\n",
        "\n",
        "# sort by tweet count and get the top 20\n",
        "top_20_days = tweets_per_day.sort_values(ascending=False).head(20)\n",
        "\n",
        "# display the result\n",
        "print(top_20_days)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Br52XIjx-PYG",
      "metadata": {
        "id": "Br52XIjx-PYG"
      },
      "source": [
        "## 2.8 Data Cleaning: Language\n",
        "Before coming up with the strategy for each column, we'll check the contents of categorical data and the distributiuon of NaNs.\n",
        "\n",
        "* It would make sence that fields like ```hashtags``` and ```user_mentions``` would have missing values and we'll leave it as it is.\n",
        "* We'll check the ```lang``` and ```place``` columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "ZPUde-jG8BO2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPUde-jG8BO2",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "fe48c18d-3191-4e2a-ac14-f9dc8ff2ff9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# number of NaNs in lang\n",
        "sum(tweets_df.lang.isna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "NjCQDN2y-0Ww",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "NjCQDN2y-0Ww",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "21ec52c7-e2b1-4cb3-e552-643023d2c69b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id                                             source  \\\n",
              "92876   1.400490e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "224632  1.310000e+18                                                NaN   \n",
              "\n",
              "             date                                      original_text lang  \\\n",
              "92876  2021-06-03  @santoshmt7666 @globaltimesnews The COVID-19 d...  NaN   \n",
              "224632        NaT                                                NaN  NaN   \n",
              "\n",
              "        favorite_count  retweet_count original_author hashtags user_mentions  \\\n",
              "92876              NaN            NaN             NaN      NaN           NaN   \n",
              "224632             NaN            NaN             NaN      NaN           NaN   \n",
              "\n",
              "       place sentiment  compound  pos    neu    neg  \n",
              "92876    NaN       neg   -0.5994  0.0  0.606  0.394  \n",
              "224632   NaN       neu    0.0000  0.0  1.000  0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e085df0b-bfa5-488b-a9ee-531b8e293d49\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>date</th>\n",
              "      <th>original_text</th>\n",
              "      <th>lang</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>place</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>compound</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>92876</th>\n",
              "      <td>1.400490e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2021-06-03</td>\n",
              "      <td>@santoshmt7666 @globaltimesnews The COVID-19 d...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.5994</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224632</th>\n",
              "      <td>1.310000e+18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e085df0b-bfa5-488b-a9ee-531b8e293d49')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e085df0b-bfa5-488b-a9ee-531b8e293d49 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e085df0b-bfa5-488b-a9ee-531b8e293d49');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2efb3ff-9e93-4aaa-baa9-770c7911c1ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2efb3ff-9e93-4aaa-baa9-770c7911c1ce')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2efb3ff-9e93-4aaa-baa9-770c7911c1ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "0"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# look at the tweet text\n",
        "tweets_df[tweets_df.lang.isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "peqUklAw_XG3",
      "metadata": {
        "id": "peqUklAw_XG3"
      },
      "source": [
        "The only 3 rows where language is missing are missing the original text, so we'll discard them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "UVXP7Mdl-_JG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVXP7Mdl-_JG",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "51192623-5894-474d-a94d-e61e05dc78df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs in 'lang' after dropping: 0\n"
          ]
        }
      ],
      "source": [
        "# drop rows where 'lang' is NaN\n",
        "tweets_df = tweets_df.dropna(subset=['lang'])\n",
        "\n",
        "# verify the changes\n",
        "print(f\"Number of NaNs in 'lang' after dropping: {sum(tweets_df.lang.isna())}\")\n",
        "\n",
        "# drop the lang column from the df\n",
        "tweets_df = tweets_df.drop(columns=['lang'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "BSX8Xlp4QYRL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "BSX8Xlp4QYRL",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "21a1da45-e738-4612-9487-8c1aa12f8e1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  id                                             source  \\\n",
              "0       1.386694e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "1       1.386694e+18  <a href=\"http://twitter.com/download/iphone\" r...   \n",
              "2       1.386694e+18  <a href=\"http://twitter.com/download/iphone\" r...   \n",
              "3       1.386694e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "4       1.386694e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "...              ...                                                ...   \n",
              "411882  1.270000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "411883  1.270000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "411884  1.270000e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "411885  1.270000e+18  <a href=\"http://twitter.com/download/android\" ...   \n",
              "411886  1.270000e+18  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
              "\n",
              "             date                                      original_text  \\\n",
              "0      2021-04-26  RT @VP: The U.S. is working closely with the I...   \n",
              "1      2021-04-26  RT @JackPosobiec: Flip-Flop Fauci admits outdo...   \n",
              "2      2021-04-26  RT @timmy315: Hi Twitter, I’m Tim Manning, the...   \n",
              "3      2021-04-26  RT @itsaadee: Praying for #India as the countr...   \n",
              "4      2021-04-26  Rapid Investment in Nursing to Strengthen the ...   \n",
              "...           ...                                                ...   \n",
              "411882 2020-06-20  RT @StuartBrownFCO: Congratulations to all tho...   \n",
              "411883 2020-06-20  RT @gemmaod1: Apologies for any fear or anxiet...   \n",
              "411884 2020-06-20  Another meat processing outbreak - this time i...   \n",
              "411885 2020-06-20  RT @HonorDecency: We knew Covid-19 would be sp...   \n",
              "411886 2020-06-20  RT @DocJeffD: Friends, this story misses the m...   \n",
              "\n",
              "        favorite_count  retweet_count  original_author  \\\n",
              "0                  0.0            0.0  jfd4humanrights   \n",
              "1                  0.0            0.0      andgrateful   \n",
              "2                  0.0          252.0       jlreader8B   \n",
              "3                  0.0            2.0    ijennychauhan   \n",
              "4                  0.0         7937.0      IJNSJournal   \n",
              "...                ...            ...              ...   \n",
              "411882             0.0          207.0       abdalesaid   \n",
              "411883             0.0            1.0        twilouhom   \n",
              "411884             0.0          271.0         gmseed_T   \n",
              "411885             0.0            1.0   cinemaofdreams   \n",
              "411886             0.0         7476.0      BrodskyIgor   \n",
              "\n",
              "                              hashtags   user_mentions  \\\n",
              "0                                  NaN              VP   \n",
              "1                                  NaN    JackPosobiec   \n",
              "2                                  NaN        timmy315   \n",
              "3                         India, Covid        itsaadee   \n",
              "4                                  NaN             NaN   \n",
              "...                                ...             ...   \n",
              "411882  Somaliland, ChosenForChevening  StuartBrownFCO   \n",
              "411883                        Covid_19        gemmaod1   \n",
              "411884                             NaN             NaN   \n",
              "411885                             NaN    HonorDecency   \n",
              "411886                             NaN        DocJeffD   \n",
              "\n",
              "                            place sentiment  compound    pos    neu    neg  \n",
              "0                         Alabama       pos    0.0772  0.191  0.638  0.170  \n",
              "1                             NaN       neg   -0.4019  0.159  0.442  0.398  \n",
              "2                             NaN       neu    0.0000  0.000  1.000  0.000  \n",
              "3                             NaN       neg   -0.4215  0.172  0.522  0.306  \n",
              "4                              UK       pos    0.3182  0.277  0.723  0.000  \n",
              "...                           ...       ...       ...    ...    ...    ...  \n",
              "411882          burao, somaliland       neu    0.0000  0.000  1.000  0.000  \n",
              "411883            London, England       neg   -0.4939  0.000  0.714  0.286  \n",
              "411884  United Kingdom of Torture       neu    0.0000  0.000  1.000  0.000  \n",
              "411885              AT THE MOVIES       pos    0.5994  0.245  0.755  0.000  \n",
              "411886           Philadelphia, PA       pos    0.6705  0.326  0.588  0.086  \n",
              "\n",
              "[411881 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd3b5722-1948-428e-8295-5df47a67b83d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>source</th>\n",
              "      <th>date</th>\n",
              "      <th>original_text</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>original_author</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>place</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>compound</th>\n",
              "      <th>pos</th>\n",
              "      <th>neu</th>\n",
              "      <th>neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.386694e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2021-04-26</td>\n",
              "      <td>RT @VP: The U.S. is working closely with the I...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>jfd4humanrights</td>\n",
              "      <td>NaN</td>\n",
              "      <td>VP</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.0772</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.638</td>\n",
              "      <td>0.170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.386694e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>2021-04-26</td>\n",
              "      <td>RT @JackPosobiec: Flip-Flop Fauci admits outdo...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>andgrateful</td>\n",
              "      <td>NaN</td>\n",
              "      <td>JackPosobiec</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.4019</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.386694e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
              "      <td>2021-04-26</td>\n",
              "      <td>RT @timmy315: Hi Twitter, I’m Tim Manning, the...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>jlreader8B</td>\n",
              "      <td>NaN</td>\n",
              "      <td>timmy315</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.386694e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2021-04-26</td>\n",
              "      <td>RT @itsaadee: Praying for #India as the countr...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>ijennychauhan</td>\n",
              "      <td>India, Covid</td>\n",
              "      <td>itsaadee</td>\n",
              "      <td>NaN</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.4215</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.522</td>\n",
              "      <td>0.306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.386694e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2021-04-26</td>\n",
              "      <td>Rapid Investment in Nursing to Strengthen the ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7937.0</td>\n",
              "      <td>IJNSJournal</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UK</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.3182</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.723</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411882</th>\n",
              "      <td>1.270000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-06-20</td>\n",
              "      <td>RT @StuartBrownFCO: Congratulations to all tho...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>abdalesaid</td>\n",
              "      <td>Somaliland, ChosenForChevening</td>\n",
              "      <td>StuartBrownFCO</td>\n",
              "      <td>burao, somaliland</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411883</th>\n",
              "      <td>1.270000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-06-20</td>\n",
              "      <td>RT @gemmaod1: Apologies for any fear or anxiet...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>twilouhom</td>\n",
              "      <td>Covid_19</td>\n",
              "      <td>gemmaod1</td>\n",
              "      <td>London, England</td>\n",
              "      <td>neg</td>\n",
              "      <td>-0.4939</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.714</td>\n",
              "      <td>0.286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411884</th>\n",
              "      <td>1.270000e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2020-06-20</td>\n",
              "      <td>Another meat processing outbreak - this time i...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>271.0</td>\n",
              "      <td>gmseed_T</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>United Kingdom of Torture</td>\n",
              "      <td>neu</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411885</th>\n",
              "      <td>1.270000e+18</td>\n",
              "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
              "      <td>2020-06-20</td>\n",
              "      <td>RT @HonorDecency: We knew Covid-19 would be sp...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>cinemaofdreams</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HonorDecency</td>\n",
              "      <td>AT THE MOVIES</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.5994</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411886</th>\n",
              "      <td>1.270000e+18</td>\n",
              "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
              "      <td>2020-06-20</td>\n",
              "      <td>RT @DocJeffD: Friends, this story misses the m...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7476.0</td>\n",
              "      <td>BrodskyIgor</td>\n",
              "      <td>NaN</td>\n",
              "      <td>DocJeffD</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>pos</td>\n",
              "      <td>0.6705</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>411881 rows × 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd3b5722-1948-428e-8295-5df47a67b83d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd3b5722-1948-428e-8295-5df47a67b83d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd3b5722-1948-428e-8295-5df47a67b83d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-287f9207-48c4-47e7-ad57-53ab92c27f88\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-287f9207-48c4-47e7-ad57-53ab92c27f88')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-287f9207-48c4-47e7-ad57-53ab92c27f88 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8e9a01a1-58fd-45e1-b7f1-99fd2be33855\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('tweets_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8e9a01a1-58fd-45e1-b7f1-99fd2be33855 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('tweets_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tweets_df"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "tweets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "VDUco94l_msI",
      "metadata": {
        "id": "VDUco94l_msI",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#rename place column to location for clarity\n",
        "tweets_df.rename(columns={'place': 'location'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "IRZGktsdBCU8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRZGktsdBCU8",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "86505135-7bc0-4228-8a23-ac2e8d10855e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118109"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# count of NaNs\n",
        "sum(tweets_df.location.isna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "DElV9lreBbuR",
      "metadata": {
        "id": "DElV9lreBbuR",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# fill location NaNs with Unknown\n",
        "tweets_df.fillna(value={'location':'Unknown'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QltD9yKyKM2i",
      "metadata": {
        "id": "QltD9yKyKM2i"
      },
      "source": [
        "## 2.9 Data Cleaning: Location\n",
        "\n",
        "Here we'll see if location can be cleaned up for further visualization. We'll start with checking the locations with at least 10 tweets.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "v7mKo2s6BNWZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7mKo2s6BNWZ",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "e313d4a7-8cdb-4f5f-fada-2aa4621193c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "location\n",
            "Unknown               118146\n",
            "United States           4539\n",
            "India                   4283\n",
            "London                  2646\n",
            "London, England         2638\n",
            "                       ...  \n",
            "Northumberland, UK        10\n",
            "Toronto Ontario           10\n",
            "Rochdale, England         10\n",
            "MT                        10\n",
            "Vancouver Island          10\n",
            "Name: count, Length: 2748, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# calculate value counts for the 'location' column\n",
        "location_counts = tweets_df.location.value_counts()\n",
        "\n",
        "# filter to keep locations with at least 20 occurrences\n",
        "filtered_locations = location_counts[location_counts >= 10]\n",
        "\n",
        "# fisplay the filtered results\n",
        "print(filtered_locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yJbEJxdeTk6I",
      "metadata": {
        "id": "yJbEJxdeTk6I"
      },
      "source": [
        "Next, we'll check the locations with the special characters (non-alphanumeric that are not the ```-,.```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "q7Kf4O_JSdrL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7Kf4O_JSdrL",
        "jupyter": {
          "source_hidden": true
        },
        "outputId": "65955492-969c-405b-cdbd-65d7d81bf57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "location\n",
            "भारत                             211\n",
            "കേരളം                            155\n",
            "नई दिल्ली, भारत                   87\n",
            "she/her                           81\n",
            "मुंबई, भारत                       80\n",
            "                                ... \n",
            "🌏 The Lost Atlantis 🏝              1\n",
            "Mohale's Hoek | Bloemfontein       1\n",
            "🇪🇸 ¡En guardia! 🗡️                 1\n",
            "Whk, Namibia | Hre, Zimbabwe       1\n",
            "Denver, CO | Washington, DC        1\n",
            "Name: count, Length: 16221, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# apply the function to create a boolean mask\n",
        "special_chars_mask = tweets_df['location'].apply(has_special_chars)\n",
        "\n",
        "# filter the DataFrame and get value counts\n",
        "locations_with_special_chars = tweets_df.loc[special_chars_mask, \\\n",
        "                                             'location'].value_counts()\n",
        "\n",
        "# display the result\n",
        "print(locations_with_special_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nXn706c4VKH0",
      "metadata": {
        "id": "nXn706c4VKH0"
      },
      "source": [
        " We'll try to salvage the locations by using the first or the last word in a multi-word location. Otherwise, set it to the 'Unknown'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NzY9yF_YUIKv",
      "metadata": {
        "id": "NzY9yF_YUIKv",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# create a boolean mask for one-word locations, those we won't be able to automatically ID\n",
        "one_word_mask = locations_with_special_chars.index.str.split().str.len() == 1\n",
        "one_word_locations = locations_with_special_chars[one_word_mask].index\n",
        "\n",
        "# replace those locations in the original DataFrame with 'Unknown'\n",
        "tweets_df['location'] = tweets_df['location'].replace(one_word_locations, 'Unknown')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xd9iepNmWjnz",
      "metadata": {
        "id": "Xd9iepNmWjnz"
      },
      "source": [
        "The brief explanation for ```geocode_location()``` function:\n",
        "* It utilizes the geocoding API. We give it a location (string), and it returns a best(first) guess for the location. For example, if I give it ```Paris```, it will return ```Paris, France``` and not ```Paris, Texas```.\n",
        "* It executes slowly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J7zg7xziM9yi",
      "metadata": {
        "id": "J7zg7xziM9yi",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# test it on few entries\n",
        "print(geocode_location('New York, NY'))             # Expected: \"New York, New York, United States\"\n",
        "print(geocode_location('Toronto, Ontario'))         # Expected: \"Toronto, Ontario, Canada\"\n",
        "print(geocode_location('India'))                    # Expected: \"Unknown, Unknown, India\"\n",
        "print(geocode_location('USA'))                      # Expected: \"Unknown, Unknown, United States\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygHbOge8X0EU",
      "metadata": {
        "id": "ygHbOge8X0EU"
      },
      "source": [
        "Additionally, we implemented parallel optimization in ```batch_geocode()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TIdeQFu-PxRd",
      "metadata": {
        "id": "TIdeQFu-PxRd",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# before applying, do minor cleaning: replace 'unknown' with 'Unknown'\n",
        "tweets_df.location.replace('unknown', 'Unknown')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNgR_KIOWndU",
      "metadata": {
        "id": "bNgR_KIOWndU",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# calculate value counts for the 'location' column\n",
        "location_counts = tweets_df.location.value_counts()\n",
        "\n",
        "# filter to keep locations with at least 10 occurrences\n",
        "filtered_locations = location_counts[location_counts >= 10]\n",
        "\n",
        "# remove 'Unknown' from filtered_locations\n",
        "filtered_locations_known = filtered_locations[filtered_locations.index != 'Unknown']\n",
        "\n",
        "# create a boolean mask for locations in filtered_locations_known\n",
        "mask = tweets_df['location'].isin(filtered_locations_known.index)\n",
        "\n",
        "# get unique locations from the filtered DataFrame\n",
        "unique_locations = tweets_df.loc[mask, 'location'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j6l1lw7GfENf",
      "metadata": {
        "id": "j6l1lw7GfENf"
      },
      "source": [
        "The following code will apply batch geocoding requests. To respect the Nominatim API, we put a sleep(0.5) before each request and longer timeout (8s).\n",
        "The code would run ~40 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aKl7CyUDNBMf",
      "metadata": {
        "id": "aKl7CyUDNBMf",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# apply geocode_location only to the selected locations in parallel\n",
        "geocoded_results = dict(zip(unique_locations, batch_geocode(unique_locations)))\n",
        "\n",
        "# map the results back to the DataFrame\n",
        "tweets_df.loc[mask, 'geocoded_location'] = tweets_df.loc[mask, 'location'].map(geocoded_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6pTfHQSQiun3",
      "metadata": {
        "id": "6pTfHQSQiun3",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# save location cache after processing\n",
        "save_cache_to_json(location_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4HJk5wdYNxyJ",
      "metadata": {
        "id": "4HJk5wdYNxyJ",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# check value counts\n",
        "tweets_df['geocoded_location'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rmtVoVogWUIG",
      "metadata": {
        "id": "rmtVoVogWUIG"
      },
      "source": [
        "Continue location data cleaning. For all undecoded ```geocoded_location```, try to apply first then last words of ```location```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2GOQnD6R2uc",
      "metadata": {
        "id": "m2GOQnD6R2uc",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# create a mask for rows where 'geocoded_location' is 'Unknown' and where its known\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "known_mask = tweets_df['geocoded_location'] != 'Unknown'\n",
        "# extract FIRST word from 'location' for 'Unknown' geocoded rows and batch process\n",
        "if unknown_mask.any():\n",
        "    first_words = tweets_df.loc[unknown_mask, 'location'].apply(lambda loc: extract_word(loc, position=\"first\"))\n",
        "\n",
        "    unique_first_words = first_words.unique()\n",
        "    unique_first_words_set = set(unique_first_words)\n",
        "    # extract known locations where geocoded_location is not 'Unknown'\n",
        "    known_locations = tweets_df.loc[known_mask, 'location']\n",
        "    # tokenize each location into words and find intersection with unique_first_words_set\n",
        "    known_words_in_locations = known_locations.str.split().apply(lambda words: unique_first_words_set.intersection(words))\n",
        "\n",
        "    # extract matches\n",
        "    matched_words = {word for words in known_words_in_locations for word in words}\n",
        "\n",
        "    # perform geocoding for matched first words\n",
        "    first_word_results = batch_geocode(matched_words)\n",
        "    # create a mapping of unique first words to geocoded results\n",
        "    first_word_mapping = dict(zip(matched_words, first_word_results))\n",
        "\n",
        "    # map the results back to the DataFrame\n",
        "    tweets_df.loc[unknown_mask, 'geocoded_location'] = first_words.map(first_word_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l5uLD4NlRufX",
      "metadata": {
        "id": "l5uLD4NlRufX",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# update the mask for remaining 'Unknown' rows\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "known_mask = tweets_df['geocoded_location'] != 'Unknown'\n",
        "\n",
        "# extract LAST word from 'location' for remaining 'Unknown' rows and batch process\n",
        "if unknown_mask.any():\n",
        "    last_words = tweets_df.loc[unknown_mask, 'location'].apply(lambda loc: extract_word(loc, position=\"last\"))\n",
        "    unique_last_words = last_words.unique()\n",
        "    unique_last_words_set = set(last_words)\n",
        "\n",
        "    known_locations = tweets_df.loc[known_mask, 'location']\n",
        "\n",
        "     # tokenize each location into words and find intersection with unique_first_words_set\n",
        "    known_words_in_locations = known_locations.str.split().apply(lambda words: unique_last_words_set.intersection(words))\n",
        "\n",
        "    # extract matches\n",
        "    matched_words = {word for words in known_words_in_locations for word in words}\n",
        "\n",
        "    # perform geocoding for unique last words\n",
        "    last_word_results = batch_geocode(matched_words)  # Ensure batch_geocode is defined\n",
        "    # create a mapping of unique last words to geocoded results\n",
        "    last_word_mapping = dict(zip(matched_words, last_word_results))\n",
        "\n",
        "    # map the results back to the DataFrame\n",
        "    tweets_df.loc[unknown_mask, 'geocoded_location'] = last_words.map(last_word_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRDg2uJDRxtq",
      "metadata": {
        "id": "gRDg2uJDRxtq",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# if still 'Unknown', give up\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "if unknown_mask.any():\n",
        "    print(f\"Giving up on {unknown_mask.sum()} locations. Could not geocode these entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k_SZ2FctjEb9",
      "metadata": {
        "id": "k_SZ2FctjEb9",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# update location cache after processing\n",
        "save_cache_to_json(location_cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2orl_xq7Yazz",
      "metadata": {
        "id": "2orl_xq7Yazz",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# fill location NaNs with Unknown\n",
        "tweets_df.fillna(value={'geocoded_location':'Unknown'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6qrp8WChaMCI",
      "metadata": {
        "id": "6qrp8WChaMCI"
      },
      "source": [
        "Next, we'll use ```split_geocoded_location()``` to split the geocoded_location into three columns: ```country```, ```state```, and ```city```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9virTNaVavK_",
      "metadata": {
        "id": "9virTNaVavK_",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# apply the function to create separate columns\n",
        "tweets_df[['city', 'state', 'country']] = tweets_df['geocoded_location'].apply(\n",
        "    lambda loc: pd.Series(split_geocoded_location(loc))\n",
        ")\n",
        "# display the DataFrame with new columns\n",
        "tweets_df[['geocoded_location', 'city', 'state', 'country']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3iWZ3sxbXCA",
      "metadata": {
        "id": "f3iWZ3sxbXCA",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# minor fix: capitalization\n",
        "# Capitalize all entries in 'City', 'State', and 'Country' columns\n",
        "tweets_df['city'] = tweets_df['city'].str.title()\n",
        "tweets_df['state'] = tweets_df['state'].str.title()\n",
        "tweets_df['country'] = tweets_df['country'].str.title()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G3d4li4aCFQo",
      "metadata": {
        "id": "G3d4li4aCFQo"
      },
      "source": [
        "Next we'll perform location EDA: tweet histograms by Country (Worldwide), by City (Worldwide), By  State(US), followed by additional cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KLMPIu6CBVMT",
      "metadata": {
        "id": "KLMPIu6CBVMT",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# 1. Country EDA: Remove 'Unknown' countries and plot histogram for top 20 (ordered)\n",
        "known_countries = tweets_df[tweets_df['country'] != 'Unknown']\n",
        "top_20_countries = known_countries['country'].value_counts().nlargest(20)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=top_20_countries.index,\n",
        "    x=top_20_countries.values,\n",
        "    hue=top_20_countries.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 Countries (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Country')\n",
        "for i, v in enumerate(top_20_countries.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4RIdk7xSCbBV",
      "metadata": {
        "id": "4RIdk7xSCbBV",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# 2. Remove 'Unknown' cities and plot histogram for top 50 (ordered)\n",
        "known_cities = tweets_df[tweets_df['city'] != 'Unknown']\n",
        "top_50_cities = known_cities['city'].value_counts().nlargest(50)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    y=top_50_cities.index,\n",
        "    x=top_50_cities.values,\n",
        "    hue=top_50_cities.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 50 Cities (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('City')\n",
        "for i, v in enumerate(top_50_cities.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKlTA2rvD9FJ",
      "metadata": {
        "id": "UKlTA2rvD9FJ",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Filter out 'Unknown' states and keep only US entries\n",
        "us_states = tweets_df[(tweets_df['country'] == 'United States') & (tweets_df['state'] != 'Unknown')]\n",
        "\n",
        "# Count occurrences of each state and get the top 20\n",
        "state_counts = us_states['state'].value_counts().nlargest(20)\n",
        "\n",
        "# Plot the distribution of tweets by the top 20 US states\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=state_counts.index,\n",
        "    x=state_counts.values,\n",
        "    hue=state_counts.index,  # Use state as hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Prevent splitting bars\n",
        "    legend=False  # Suppress the legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 US States (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('US State')\n",
        "\n",
        "# Add labels to the bars\n",
        "for i, v in enumerate(state_counts.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CpKOlC0mEcFv",
      "metadata": {
        "id": "CpKOlC0mEcFv",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# minor fixes: Move Florida, California, Texas, Michigan  from City to State and rename None in state to Unknown\n",
        "states_as_cities = ['Florida', 'California', 'Texas', 'Michigan']\n",
        "mask_states_as_cities = (tweets_df['city'].isin(states_as_cities)) & (tweets_df['country'] == 'United States')\n",
        "\n",
        "# update State and City columns for these entries\n",
        "tweets_df.loc[mask_states_as_cities, 'state'] = tweets_df.loc[mask_states_as_cities, 'city']\n",
        "tweets_df.loc[mask_states_as_cities, 'city'] = 'Unknown'\n",
        "\n",
        "# rename all 'None' in State to 'Unknown'\n",
        "tweets_df['state'] = tweets_df['state'].replace('None', 'Unknown')\n",
        "\n",
        "# fix \"Alba/Scotland\" entries\n",
        "mask_alba_scotland = tweets_df['city'].str.contains('Alba/Scotland', case=False, na=False)\n",
        "\n",
        "# update City and Country for Alba/Scotland\n",
        "tweets_df.loc[mask_alba_scotland, 'city'] = 'Unknown'\n",
        "tweets_df.loc[mask_alba_scotland, 'country'] = 'Scotland'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zpEycPkWcHrz",
      "metadata": {
        "id": "zpEycPkWcHrz",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# redo the city plot:\n",
        "# 2. Remove 'Unknown' cities and plot histogram for top 50 (ordered)\n",
        "known_cities = tweets_df[tweets_df['city'] != 'Unknown']\n",
        "top_50_cities = known_cities['city'].value_counts().nlargest(50)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    y=top_50_cities.index,\n",
        "    x=top_50_cities.values,\n",
        "    hue=top_50_cities.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 50 Cities (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('City')\n",
        "for i, v in enumerate(top_50_cities.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iN_jW3o1b_M4",
      "metadata": {
        "id": "iN_jW3o1b_M4",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# redo the states plot:\n",
        "# Filter out 'Unknown' states and keep only US entries\n",
        "us_states = tweets_df[(tweets_df['country'] == 'United States') & (tweets_df['state'] != 'Unknown')]\n",
        "\n",
        "# Count occurrences of each state and get the top 20\n",
        "state_counts = us_states['state'].value_counts().nlargest(20)\n",
        "\n",
        "# Plot the distribution of tweets by the top 20 US states\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=state_counts.index,\n",
        "    x=state_counts.values,\n",
        "    hue=state_counts.index,  # Use state as hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Prevent splitting bars\n",
        "    legend=False  # Suppress the legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 US States (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('US State')\n",
        "\n",
        "# Add labels to the bars\n",
        "for i, v in enumerate(state_counts.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZiT_qKnDdz5K",
      "metadata": {
        "id": "ZiT_qKnDdz5K",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# USA tweets per day\n",
        "usa_tweets = tweets_df[tweets_df['country'] == 'United States']\n",
        "\n",
        "# group by date and count tweets per day\n",
        "tweets_per_day = usa_tweets.groupby(usa_tweets['date'].dt.date).size().reset_index(name='Total Tweets')\n",
        "tweets_per_day.columns = ['Date', 'Total Tweets']\n",
        "\n",
        "# bar plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(\n",
        "    data=tweets_per_day,\n",
        "    x='Date',\n",
        "    y='Total Tweets',\n",
        "    hue='Date',  # Assign the x variable to hue\n",
        "    palette='viridis',\n",
        "    legend=False  # Disable the legend\n",
        ")\n",
        "\n",
        "# customize x-axis ticks to show every third date\n",
        "xticks = plt.gca().get_xticks()\n",
        "xtick_labels = tweets_per_day['Date'].astype(str).values\n",
        "plt.xticks(\n",
        "    ticks=xticks[::3],  # show every third tick\n",
        "    labels=xtick_labels[::3],  # use corresponding labels\n",
        "    rotation=90  # rotate 45 deg for better visibility\n",
        ")\n",
        "\n",
        "plt.xticks(rotation=90)  # rotate x-axis labels for better readability\n",
        "plt.title('Tweets Per Day in the USA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.tight_layout()  # Adjust layout to prevent label overlap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fo8-mFt8rMaz",
      "metadata": {
        "id": "fo8-mFt8rMaz"
      },
      "source": [
        "**[Optional] Adding lattitude, longitude (long runtime)**. This piece of code will run for about 1 hr, it's going to retrieve geo coordinates (latitude, longitude) for all cities in the dataset. Strongly recommend executing the following ```save_cache_to_json()``` to save the work and downloading the file if you're running the notebook from the cloud.\n",
        "\n",
        "For this particular dataset or scope of the business problem it might not worth it, but for advanced analysis or futire work it might be a useful EDA section to utilize.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F23LeybSxdp2",
      "metadata": {
        "id": "F23LeybSxdp2",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "tweets_df = add_coordinates_with_progress(tweets_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i4f-X-xWq1CE",
      "metadata": {
        "id": "i4f-X-xWq1CE",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# save coordinate cache to json\n",
        "save_cache_to_json(coordinate_cache, \"coordinate_cache.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vTzcb11aBfOn",
      "metadata": {
        "id": "vTzcb11aBfOn",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Generate the map if it's not found\n",
        "try:\n",
        "    # Try to open the file\n",
        "    heatmap_file = os.path.join(data_dir, \"heatmap_city.html\")\n",
        "    with open(heatmap_file, \"r\") as file:\n",
        "        print(\"heatmap_city.html already exists. No need to recompute.\")\n",
        "        heatmap_city = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"heatmap.html does not exist. Generating the heatmap...\")\n",
        "    # Call the function to compute the heatmap\n",
        "    # Filter rows where both latitude and longitude are not 0.0\n",
        "    filtered_tweets_df = tweets_df[(tweets_df['latitude'] != 0.0) & (tweets_df['longitude'] != 0.0)]\n",
        "    # Create tweet_count column\n",
        "    filtered_tweets_df.loc[:, 'tweet_count'] = filtered_tweets_df.groupby('city')['city'].transform('count')\n",
        "    heatmap_city = generateBaseMap(input_type=\"city\", df=filtered_tweets_df)\n",
        "    heatmap_city.save(heatmap_file)\n",
        "    del filtered_tweets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4odEWaXzHfPI",
      "metadata": {
        "id": "4odEWaXzHfPI",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Display the map directly in the notebook (might not work)\n",
        "#display(HTML(heatmap_city))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h3Am38oyq31u",
      "metadata": {
        "id": "h3Am38oyq31u"
      },
      "outputs": [],
      "source": [
        "# cleanup\n",
        "del us_states\n",
        "del usa_tweets\n",
        "del tweets_per_day\n",
        "del known_cities\n",
        "del known_countries\n",
        "del state_counts\n",
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w4PgWcupwYB6",
      "metadata": {
        "id": "w4PgWcupwYB6"
      },
      "source": [
        "## 2.10 Data Cleaning: Source\n",
        "Next, we'll replace the ```source``` values with readable meaningful values using ```extract_html_source()``` function that will extract the value between the HTML tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XWS6nPsxn2UX",
      "metadata": {
        "id": "XWS6nPsxn2UX"
      },
      "outputs": [],
      "source": [
        "# replace the source with the meaningful value\n",
        "tweets_df.source.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GvPFCp0voW5f",
      "metadata": {
        "id": "GvPFCp0voW5f",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# apply the function to the 'source' column\n",
        "tweets_df['source'] = tweets_df['source'].apply(extract_html_source)\n",
        "# identify sources with counts less than 100\n",
        "source_counts = tweets_df['source'].value_counts()\n",
        "low_count_sources = source_counts[source_counts < 100].index\n",
        "\n",
        "# replace low-count sources with 'Other'\n",
        "tweets_df['source'] = tweets_df['source'].replace(low_count_sources, 'Other')\n",
        "#replace NaNs with 'Other'\n",
        "tweets_df['source'] = tweets_df['source'].fillna('Other')\n",
        "tweets_df['source'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AJmCePQOoSb5",
      "metadata": {
        "id": "AJmCePQOoSb5",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# plot the top 20 sources\n",
        "top_20_sources = tweets_df['source'].value_counts().nlargest(20)\n",
        "\n",
        "# Convert to a DataFrame for plotting\n",
        "top_20_df = top_20_sources.reset_index()\n",
        "top_20_df.columns = ['Source', 'Count']\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=top_20_df,\n",
        "    y='Source',\n",
        "    x='Count',\n",
        "    hue='Source',  # Assign the x variable to hue\n",
        "    palette='viridis',\n",
        "    legend=False  # Disable the legend\n",
        ")\n",
        "plt.title('Top 20 Twitter Sources')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Source')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "asvDQcaVaZQp",
      "metadata": {
        "id": "asvDQcaVaZQp"
      },
      "source": [
        "## 2.11 Data Cleaning: Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95I77q_xoCPX",
      "metadata": {
        "id": "95I77q_xoCPX"
      },
      "outputs": [],
      "source": [
        "# Show distribution of tweet sentiments\n",
        "sentiment_counts = tweets_df.sentiment.value_counts()\n",
        "print(sentiment_counts)\n",
        "# is there any NaNs\n",
        "print(tweets_df.sentiment.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OkmtHZGsoE0N",
      "metadata": {
        "id": "OkmtHZGsoE0N"
      },
      "outputs": [],
      "source": [
        "# Define custom colors for the sentiments\n",
        "custom_colors = {\n",
        "    'pos': '#90EE90',  # Light Green\n",
        "    'neu': '#ADD8E6',   # Light Blue\n",
        "    'neg': '#FFB6C1'   # Light Red\n",
        "}\n",
        "\n",
        "# Ensure the colors map correctly to the sentiment categories\n",
        "print(\"Sentiment Categories:\", sentiment_counts.index)  # Debugging step\n",
        "colors = [custom_colors.get(sentiment, '#D3D3D3') for sentiment in sentiment_counts.index]\n",
        "\n",
        "# Check the colors being applied\n",
        "print(\"Applied Colors:\", colors)  # Debugging step\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sentiment_counts.values,\n",
        "    labels=sentiment_counts.index,\n",
        "    autopct=lambda p: f'{p:.1f}% ({int(p * sum(sentiment_counts.values) / 100)})',  # Percentage and count\n",
        "    colors=colors,  # Use custom colors\n",
        "    startangle=90\n",
        ")\n",
        "plt.title('Sentiment Distribution', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EQnrTxdO4TWj",
      "metadata": {
        "id": "EQnrTxdO4TWj",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Define sentiments and their corresponding colors\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
        "\n",
        "# Scatter plots for the relationship between sentiment scores and compound value\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (sentiment, color) in enumerate(zip(sentiments, colors), start=1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.scatterplot(\n",
        "        data=tweets_df,\n",
        "        x=sentiment,\n",
        "        y='compound',\n",
        "        alpha=0.6,\n",
        "        color=color\n",
        "    )\n",
        "    plt.title(f'Compound Score vs {sentiment.capitalize()}')\n",
        "    plt.xlabel(sentiment.capitalize())\n",
        "    plt.ylabel('Compound Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FHOVEGR4FPx6",
      "metadata": {
        "id": "FHOVEGR4FPx6"
      },
      "source": [
        "From Kaggle's data card:\n",
        "Algorithm Sentiment Classification of Tweets (compound, sentiment):\n",
        "\n",
        "if tweet[compound] < 0:\n",
        "tweet[sentiment] = 0.0 # assigned 0.0 for Negative Tweets\n",
        "elif tweet[compound] > 0:\n",
        "tweet[sentiment] = 1.0 # assigned 1.0 for Positive Tweets\n",
        "else:\n",
        "tweet[sentiment] = 0.5 # assigned 0.5 for Neutral Tweets\n",
        "end```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VsDAMyKDFj6Z",
      "metadata": {
        "id": "VsDAMyKDFj6Z",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "positive_tweets = tweets_df[tweets_df['sentiment'] == 'pos']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = positive_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N-Ozs-UCHEl9",
      "metadata": {
        "id": "N-Ozs-UCHEl9",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "neutral_tweets = tweets_df[tweets_df['sentiment'] == 'neu']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = neutral_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KfLiQNJuHRSC",
      "metadata": {
        "id": "KfLiQNJuHRSC",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "negative_tweets = tweets_df[tweets_df['sentiment'] == 'neg']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = negative_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kflp-YXVIGFk",
      "metadata": {
        "id": "Kflp-YXVIGFk"
      },
      "source": [
        "Scatterplot for value distribution in neutral sentiment column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vdy3Su78IFcX",
      "metadata": {
        "id": "vdy3Su78IFcX",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "tweets_df['neu'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "leAjLtZpzE2W",
      "metadata": {
        "id": "leAjLtZpzE2W"
      },
      "source": [
        "Understanding sentiment compound distribution between positive, neutral, negative score values.\n",
        "\n",
        "Here we have a good distribution for positive, negative tweets. We're unable to plot neutral tweets's distribution because the variance is almost zero (most variables are around 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VVtJyTzSzDqd",
      "metadata": {
        "id": "VVtJyTzSzDqd",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.kdeplot(\n",
        "    data=tweets_df,\n",
        "    x='compound',\n",
        "    hue='sentiment',\n",
        "    fill=True,\n",
        "    alpha=0.6,\n",
        "    palette={'neg': 'lightcoral', 'neu': 'lightblue', 'pos': 'lightgreen'}\n",
        ")\n",
        "plt.title('KDE of Compoundby by Sentiment')\n",
        "plt.xlabel('Compound Weighted')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78HibLzkLK0S",
      "metadata": {
        "id": "78HibLzkLK0S"
      },
      "source": [
        "**Re-computing the compound score using weighted formula: Compound Score=(pos−neg)×(1−neu)**\n",
        "\n",
        "We'll also exclude rows where all 3 values for `pos`, `neu`, `neg` are either 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6akAIPcRMnTV",
      "metadata": {
        "id": "6akAIPcRMnTV",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Filter out rows where all three values are either 0 or 1\n",
        "valid_rows = ~(\n",
        "    ((tweets_df['pos'] == 0) & (tweets_df['neu'] == 0) & (tweets_df['neg'] == 0)) |\n",
        "    ((tweets_df['pos'] == 1) & (tweets_df['neu'] == 1) & (tweets_df['neg'] == 1))\n",
        ")\n",
        "\n",
        "# Apply the filter explicitly with .loc[]\n",
        "tweets_df = tweets_df.loc[valid_rows]\n",
        "\n",
        "# Compute the weighted compound score using .loc[]\n",
        "tweets_df.loc[:, 'compound_weighted'] = (tweets_df['pos'] - tweets_df['neg']) * (1 - tweets_df['neu'])\n",
        "\n",
        "# Scale it to [0, 1]\n",
        "tweets_df.loc[:, 'compound_weighted'] = (tweets_df['compound_weighted'] - tweets_df['compound_weighted'].min()) / (\n",
        "    tweets_df['compound_weighted'].max() - tweets_df['compound_weighted'].min()\n",
        ")\n",
        "\n",
        "# Display a preview of the updated DataFrame\n",
        "print(tweets_df[['pos', 'neu', 'neg', 'compound', 'compound_weighted', 'sentiment']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-iy7Olu1NiZM",
      "metadata": {
        "id": "-iy7Olu1NiZM",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# replot scores sentiments\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
        "\n",
        "# Scatter plots for the relationship between sentiment scores and compound value\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (sentiment, color) in enumerate(zip(sentiments, colors), start=1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.scatterplot(\n",
        "        data=tweets_df,\n",
        "        x=sentiment,\n",
        "        y='compound_weighted',\n",
        "        alpha=0.6,\n",
        "        color=color\n",
        "    )\n",
        "    plt.title(f'Compound Score vs {sentiment.capitalize()}')\n",
        "    plt.xlabel(sentiment.capitalize())\n",
        "    plt.ylabel('Compound Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wYpFMoLrOZdc",
      "metadata": {
        "id": "wYpFMoLrOZdc",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Plotting new and old compound score distributions.\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Plot weighted compound distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(tweets_df['compound_weighted'], bins=30, kde=False, color='purple')\n",
        "plt.title('Distribution of Weighted Compound Scores')\n",
        "plt.xlabel('Weighted Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Plot compound distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(tweets_df['compound'], bins=30, kde=False, color='orange')\n",
        "plt.title('Distribution of Original Compound Scores')\n",
        "plt.xlabel('Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-3jWXTSASXj8",
      "metadata": {
        "id": "-3jWXTSASXj8",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# computing new sentiments:\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Define conditions and labels for new_simple_sentiment\n",
        "simple_conditions = [\n",
        "    (tweets_df['compound_weighted'] <= 0.49),  # Negative sentiment\n",
        "    (tweets_df['compound_weighted'] >0.49) & (tweets_df['compound_weighted'] <= 0.51),  # Neutral sentiment\n",
        "    (tweets_df['compound_weighted'] > 0.51)  # Positive sentiment\n",
        "]\n",
        "simple_labels = ['neg', 'neu', 'pos']\n",
        "\n",
        "# Define conditions and labels for new_advanced_sentiment\n",
        "advanced_conditions = [\n",
        "    (tweets_df['compound_weighted'] < 0.35),  # Strongly Negative\n",
        "    (tweets_df['compound_weighted'] >= 0.35) & (tweets_df['compound_weighted'] <= 0.49),  # Weakly Negative (neu-neg)\n",
        "    (tweets_df['compound_weighted'] >0.49) & (tweets_df['compound_weighted'] <= 0.51),  # Neutral sentiment\n",
        "    (tweets_df['compound_weighted'] > 0.51) & (tweets_df['compound_weighted'] <= 0.75),  # Weakly Positive (neu-pos)\n",
        "    (tweets_df['compound_weighted'] > 0.75)  # Strongly Positive\n",
        "]\n",
        "advanced_labels = ['neg', 'neu-neg', 'neu', 'neu-pos', 'pos']\n",
        "\n",
        "# Assign new_simple_sentiment\n",
        "tweets_df.loc[:, 'new_simple_sentiment'] = pd.cut(\n",
        "    tweets_df['compound_weighted'],\n",
        "    bins=[-float('inf'), 0.49, 0.51, float('inf')],  # Define boundaries\n",
        "    labels=['neg', 'neu', 'pos']\n",
        ")\n",
        "\n",
        "# Assign new_advanced_sentiment\n",
        "tweets_df.loc[:, 'new_advanced_sentiment'] = pd.cut(\n",
        "    tweets_df['compound_weighted'],\n",
        "    bins=[-float('inf'), 0.35, 0.49, 0.51, 0.75, float('inf')],  # Define boundaries\n",
        "    labels=['neg', 'neu-neg', 'neu', 'neu-pos', 'pos']\n",
        ")\n",
        "\n",
        "# Check the distribution of the new sentiment columns\n",
        "print(tweets_df['new_simple_sentiment'].value_counts())\n",
        "print(tweets_df['new_advanced_sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3nq9SmWDWk1Z",
      "metadata": {
        "id": "3nq9SmWDWk1Z"
      },
      "source": [
        "Intermediate conclusions:\n",
        "* We don't know whow assigned scores are computed (it's not regular or weighted computation).\n",
        "* We can guess the prescence of a weights placed on the negative or positive parts of the score, when one dominates the other. That would explain the difference in compound score distribution, skewing tweets with more positive or negative scores urther away from neutral.\n",
        "* We should also consider that back in 2020 twitter was a better moderated environment, and many tweets would originate from official accounts, contributing to their neutral sentiment.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sn9Hm_WY84o3",
      "metadata": {
        "id": "sn9Hm_WY84o3"
      },
      "source": [
        "## 2.12 Exploratory Data Analysis (EDA): Social Connections\n",
        "Vanity fair. Lookng at social engagement, connections, popularity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_rsWWdIhAniD",
      "metadata": {
        "id": "_rsWWdIhAniD"
      },
      "source": [
        "* Hashtags word cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zYYTMdrq9nNs",
      "metadata": {
        "id": "zYYTMdrq9nNs"
      },
      "outputs": [],
      "source": [
        "# Flatten the list of hashtags\n",
        "all_hashtags = [\n",
        "    ''.join(hashtags)  # Join characters into a string\n",
        "    for hashtags in tweets_df['hashtags']\n",
        "    if isinstance(hashtags, (list, tuple, str, np.ndarray))  # Check if iterable\n",
        "]\n",
        "# Convert the list of hashtags into a single string, separated by spaces\n",
        "hashtag_text = ' '.join(all_hashtags)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, \\\n",
        "                      background_color='white', colormap='viridis').generate(hashtag_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Hashtag Word Cloud\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C1Lt_TCCArYL",
      "metadata": {
        "id": "C1Lt_TCCArYL"
      },
      "source": [
        "* Histograms for favorite tweets, retweets.\n",
        "Convert both to log scale because of the value range (wide range, outliers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZfLetef9-DkN",
      "metadata": {
        "id": "ZfLetef9-DkN",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Log-transform favorite_count (add 1 to avoid log(0))\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "tweets_df.loc[:, 'log_favorite_count'] = np.log1p(tweets_df['favorite_count'])\n",
        "\n",
        "# Histogram for log-transformed favorite_count\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(tweets_df['log_favorite_count'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribution of Log-Transformed Favorite Counts')\n",
        "plt.xlabel('Log(Favorite Count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Log-transform retweet_count (add 1 to avoid log(0))\n",
        "tweets_df.loc[:, 'log_retweet_count'] = np.log1p(tweets_df['retweet_count'])\n",
        "\n",
        "# Histogram for log-transformed retweet_count\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(tweets_df['log_retweet_count'], bins=30, kde=True, color='green')\n",
        "plt.title('Distribution of Log-Transformed Retweet Counts')\n",
        "plt.xlabel('Log(Retweet Count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EnxKnDBe-c7q",
      "metadata": {
        "id": "EnxKnDBe-c7q",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Log-transform favorite_count and retweet_count (add 1 to avoid log(0))\n",
        "tweets_df.loc[:, 'log_favorite_count'] = np.log1p(tweets_df['favorite_count'])\n",
        "tweets_df.loc[:, 'log_retweet_count'] = np.log1p(tweets_df['retweet_count'])\n",
        "\n",
        "# Scatter plot with log-transformed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='log_favorite_count', y='log_retweet_count', data=tweets_df, alpha=0.6)\n",
        "plt.title('Log(Favorites) vs Log(Retweets)')\n",
        "plt.xlabel('Log(Favorite Count)')\n",
        "plt.ylabel('Log(Retweet Count)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NNUqkPi5-mXH",
      "metadata": {
        "id": "NNUqkPi5-mXH",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#  Show trends in favorites and retweets over time.\n",
        "aggregated = tweets_df.groupby('date')[['favorite_count', 'retweet_count']].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='date', y='favorite_count', data=aggregated, label='Favorites', color='blue')\n",
        "plt.yscale('log')\n",
        "sns.lineplot(x='date', y='retweet_count', data=aggregated, label='Retweets', color='green')\n",
        "plt.title('Log Trends in Favorites and Retweets Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YgsLPGyT-uc6",
      "metadata": {
        "id": "YgsLPGyT-uc6",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# top contributors by the number of tweets\n",
        "top_authors = tweets_df['original_author'].value_counts().head(10)\n",
        "# Bar Plot for Top Authors\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    x=top_authors.values,\n",
        "    y=top_authors.index,\n",
        "    hue=top_authors.index,  # Assign hue to the y variable\n",
        "    palette='viridis',\n",
        "    dodge=False,            # Ensure no splitting\n",
        "    legend=False            # Disable the legend\n",
        ")\n",
        "plt.title('Top 10 Authors by Number of Tweets')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Author')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YWQxhA-u_GOW",
      "metadata": {
        "id": "YWQxhA-u_GOW",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Plot authors based on their average favorite_count and retweet_count\n",
        "author_stats = tweets_df.groupby('original_author')[['favorite_count', \\\n",
        "                                                     'retweet_count']].mean().reset_index()\n",
        "\n",
        "author_stats.loc[:, 'log_favorite_count'] = np.log1p(author_stats['favorite_count'])\n",
        "author_stats.loc[:, 'log_retweet_count'] = np.log1p(author_stats['retweet_count'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='log_favorite_count', y='log_retweet_count', data=author_stats, alpha=0.6\n",
        ")\n",
        "plt.title('Author Popularity: Log-Transformed Average Favorites vs Retweets')\n",
        "plt.xlabel('Log(Average Favorite Count)')\n",
        "plt.ylabel('Log(Average Retweet Count)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ndi-I9-9__Hx",
      "metadata": {
        "id": "ndi-I9-9__Hx"
      },
      "source": [
        "* Visualize the most mentioned users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53_X2kEx_Rlf",
      "metadata": {
        "id": "53_X2kEx_Rlf",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Process user_mentions: split by comma and remove empty/malformed entries\n",
        "tweets_df.loc[:, 'user_mentions'] = tweets_df['user_mentions'].fillna('')  # Replace None with empty strings\n",
        "tweets_df.loc[:, 'user_mentions_clean'] = tweets_df['user_mentions'].apply(lambda x: [mention.strip() for mention in x.split(',') if mention.strip()])\n",
        "\n",
        "# Flatten mentions and count frequencies\n",
        "all_mentions = [mention for mentions in tweets_df['user_mentions_clean'] for mention in mentions]\n",
        "mention_counts = Counter(all_mentions).most_common(10)\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "mention_df = pd.DataFrame(mention_counts, columns=['user', 'count'])\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    x='count',\n",
        "    y='user',\n",
        "    data=mention_df,\n",
        "    hue='user',        # Assign hue to the y variable\n",
        "    palette='magma',\n",
        "    dodge=False,       # Prevent bar splitting\n",
        "    legend=False       # Disable the legend\n",
        ")\n",
        "plt.title('Top 10 Mentioned Users', fontsize=14)\n",
        "plt.xlabel('Number of Mentions', fontsize=12)\n",
        "plt.ylabel('User', fontsize=12)\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fZvsOhFo_6JZ",
      "metadata": {
        "id": "fZvsOhFo_6JZ"
      },
      "source": [
        "* Show the relationships between authors and mentioned users. With a subset that big, we won't have computational resources to compute a full graph.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ZXy4hax_cYf",
      "metadata": {
        "id": "5ZXy4hax_cYf",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Filter nodes with degree above a certain threshold (200 connections)\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "# Populate the graph with edges from tweets_df\n",
        "for _, row in tweets_df.iterrows():\n",
        "    original_author = row['original_author']\n",
        "    mentioned_users = row['user_mentions_clean']  # List of mentioned users\n",
        "\n",
        "    # Add edges: author -> mentioned user\n",
        "    for mention in mentioned_users:\n",
        "        G.add_edge(original_author, mention)\n",
        "\n",
        "degree_threshold = 200\n",
        "high_degree_nodes = [node for node, degree in G.degree() if degree > degree_threshold]\n",
        "subgraph = G.subgraph(high_degree_nodes)\n",
        "\n",
        "# Draw the subgraph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw_networkx(\n",
        "    subgraph,\n",
        "    node_size=50,\n",
        "    alpha=0.7,\n",
        "    font_size=8,\n",
        "    edge_color='gray'\n",
        ")\n",
        "plt.title(f'User Mentions Network (Nodes with Degree > {degree_threshold})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zNKuJ_Nh_jDi",
      "metadata": {
        "id": "zNKuJ_Nh_jDi"
      },
      "source": [
        "\n",
        "* Display correlation between different metrics (e.g., favorites, retweets, mentions).*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aUI5MgVT_lwr",
      "metadata": {
        "id": "aUI5MgVT_lwr",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(tweets_df[['favorite_count', 'retweet_count']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Between Metrics')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EUTwTTrb_w8t",
      "metadata": {
        "id": "EUTwTTrb_w8t"
      },
      "source": [
        "* Visualize authors with bubbles representing their popularity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "of54h0nm_0Ku",
      "metadata": {
        "id": "of54h0nm_0Ku",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Aggregate favorite_count and retweet_count for each author\n",
        "author_stats = tweets_df.groupby('original_author').agg({\n",
        "    'favorite_count': 'mean',\n",
        "    'retweet_count': 'mean',\n",
        "    'original_author': 'count'  # Count tweets for each author\n",
        "}).rename(columns={'original_author': 'tweet_count'}).reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='favorite_count',\n",
        "    y='retweet_count',\n",
        "    size='tweet_count',\n",
        "    data=author_stats,\n",
        "    sizes=(50, 500),\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Author Popularity: Favorites vs Retweets with Tweet Count')\n",
        "plt.xlabel('Average Favorite Count')\n",
        "plt.ylabel('Average Retweet Count')\n",
        "plt.legend(title='Tweet Count', loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jYZWKykKsi10",
      "metadata": {
        "id": "jYZWKykKsi10"
      },
      "outputs": [],
      "source": [
        "# cleanup\n",
        "del author_stats\n",
        "del mention_df\n",
        "del positive_tweets\n",
        "del neutral_tweets\n",
        "del negative_tweets\n",
        "del subset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MWCtcPLd3q8z",
      "metadata": {
        "id": "MWCtcPLd3q8z"
      },
      "source": [
        "#3. Text Preprocessing and Feature Engineering\n",
        "\n",
        "In this section we'll be transforming the contents of the `original_tweet` column to the text format suitable for classification. This will include the following steps:\n",
        "\n",
        "**Cleaning**\n",
        "\n",
        "This step removes noise and unnecessary elements from the raw tweet text, such as special characters, links, mentions (@usernames), and hashtags (#topic), ii also detects emoji and converts it to plain text, (😀 to smile face).\n",
        "\n",
        "**Preprocessing**\n",
        "This section prepares the cleaned text for sentiment analysis by breaking it down into individual words (tokenization), removing common words that don't carry much meaning (stop word removal), and reducing words to their base form (lemmatization).\n",
        "**Feature Extraction**\n",
        "\n",
        "This stage involves creating numerical representations of the text that can be used as input for machine learning models. We'll be extracting\n",
        "* Bag of Words (BoW): This technique represents text as a collection of individual words and their frequencies, ignoring grammar and word order. It creates a numerical vector for each document, where each element represents the count of a specific word in the document's vocabulary.\n",
        "* N-grams: Creating combinations of words (e.g., \"Covid cases\" as a bigram, or \"Die of Covid\") to capture more context.\n",
        "* TF-IDF: Calculating the importance of words in a document relative to a collection of documents.\n",
        "* Embeddings: Using pre-trained models to create vector representations of words that capture semantic meaning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vkjyw35VopKU",
      "metadata": {
        "id": "Vkjyw35VopKU"
      },
      "source": [
        "## 3.1 Text Cleaning\n",
        "We wrote ```process_tweet_data()```to do the following:\n",
        "* Remove and store emoji as a separate column(using default UNICODE_EMOJI).\n",
        "* Remove and store mentions @ as a separate column.\n",
        "* Remove and store retweets RT @ as a separate column.\n",
        "* Remove and store hashtags # as a separate column.\n",
        "* Remove and store URLs www. or t. or bit. as a separate column.\n",
        "* Remove special characters, whitespaces, numbers.\n",
        "* Lowercase and store text in ```cleaned_text``` column  \n",
        "\n",
        "\n",
        "For debugging purposes, we'll create a small dataframe with URLs, mentions, retweents, and emojis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hu094gmiwvFz",
      "metadata": {
        "id": "Hu094gmiwvFz"
      },
      "outputs": [],
      "source": [
        "# example df\n",
        "df_test = {\n",
        "    'tweet_text': [\n",
        "        \"WOW!!! Just found this link: https://example.com :) @user123 #hashtag #fun\",\n",
        "        \"RT @user456: Another day in paradise! 😃 #sunnyday\",\n",
        "        \"Why am I so serious? :(( Visit www.example.org for details! #serious\",\n",
        "        \"How're we supposed to know how it's been done before :( #bad\",\n",
        "        \"I'm thinking about #tomorrow\"\n",
        "    ]\n",
        "}\n",
        "df_test = pd.DataFrame(df_test)\n",
        "\n",
        "# Clean the text data\n",
        "df_test = pd.concat([\n",
        "    df_test,\n",
        "    df_test['tweet_text'].apply(lambda x: pd.Series(process_tweet_data(x)))\n",
        "], axis=1)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769e70be-bf01-48ff-bc43-96e6c82d727a",
      "metadata": {
        "id": "769e70be-bf01-48ff-bc43-96e6c82d727a"
      },
      "outputs": [],
      "source": [
        "with pd.option_context('display.max_colwidth', None):\n",
        "    print(df_test.loc[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zG2ge4CK27kf",
      "metadata": {
        "id": "zG2ge4CK27kf"
      },
      "source": [
        "We'll apply text cleaning with progress to the subset of entire twitter df (takes about 2 hours minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XvYD8vhPju9c",
      "metadata": {
        "id": "XvYD8vhPju9c"
      },
      "outputs": [],
      "source": [
        "tweets_slice_df = tweets_df[['id', 'date', 'source', 'original_text', 'sentiment', \\\n",
        "                             'pos', 'neu', 'neg', 'compound', 'compound_weighted', \\\n",
        "                             'new_simple_sentiment', 'new_advanced_sentiment']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j_q9NBBTeSbM",
      "metadata": {
        "id": "j_q9NBBTeSbM"
      },
      "outputs": [],
      "source": [
        "# check for NaNs before text pre-processing.\n",
        "tweets_slice_df.isna().sum()\n",
        "# drop NaNs from ID\n",
        "tweets_slice_df = tweets_slice_df.dropna(subset=['id'])\n",
        "#re-check for NaNs:\n",
        "tweets_slice_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JQp5P0qtgWjr",
      "metadata": {
        "id": "JQp5P0qtgWjr",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Remove all retweets\n",
        "# Detect retweets: Create a mask for rows that are retweets\n",
        "retweet_mask = tweets_slice_df['original_text'].str.startswith('RT @')\n",
        "\n",
        "# Filter out retweets\n",
        "tweets_df_no_retweets = tweets_slice_df[~retweet_mask].copy()\n",
        "\n",
        "# Print results\n",
        "print(f\"Original DataFrame size: {tweets_df.shape[0]} rows\")\n",
        "print(f\"DataFrame size after removing retweets: {tweets_df_no_retweets.shape[0]} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fGwYB9o5wo8j",
      "metadata": {
        "id": "fGwYB9o5wo8j"
      },
      "outputs": [],
      "source": [
        "# do cleanup here\n",
        "del tweets_slice_df\n",
        "del tweets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rtFLmGL6kb_r",
      "metadata": {
        "id": "rtFLmGL6kb_r"
      },
      "outputs": [],
      "source": [
        "# if there's saved DF, try to load it and join it with\n",
        "path_to_cleaned_df = os.path.join(data_dir, \"cleaned_text_df.csv\")  # Path to saved file\n",
        "if os.path.exists(path_to_cleaned_df):\n",
        "    # Load the cleaned_text_df\n",
        "    cleaned_text_df = pd.read_csv(path_to_cleaned_df, low_memory=False)\n",
        "    print(\"Cleaned text successfully loaded\")\n",
        "else:\n",
        "    # Apply the cleaning function\n",
        "    cleaned_text_column = clean_tweets_with_progress_parallel(tweets_df_no_retweets, text_col='original_text')\n",
        "\n",
        "    # Add the cleaned text column to the dataframe\n",
        "    tweets_df_no_retweets['cleaned_text'] = cleaned_text_column\n",
        "\n",
        "    # Drop rows with NaN values in 'cleaned_text'\n",
        "    tweets_df_no_retweets = tweets_df_no_retweets.dropna(subset=['cleaned_text'])\n",
        "\n",
        "    # Save the intermediate processing step\n",
        "    cleaned_text_df = tweets_df_no_retweets[['sentiment', 'cleaned_text']]\n",
        "    cleaned_text_df.to_csv(path_to_cleaned_df, index=False)\n",
        "    print(f\"tweets_df_no_retweets has been saved as '{path_to_cleaned_df}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JcPLYI2XPqb9",
      "metadata": {
        "id": "JcPLYI2XPqb9"
      },
      "outputs": [],
      "source": [
        "cleaned_text_df.isna().sum()\n",
        "#drop NaNs\n",
        "cleaned_text_df = cleaned_text_df.dropna(subset=['cleaned_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o_Kzz-ygoslI",
      "metadata": {
        "id": "o_Kzz-ygoslI"
      },
      "source": [
        "## 3.2 Text Preprocessing\n",
        "\n",
        "This section focuses on preparing the tweet text data for sentiment analysis by applying essential preprocessing techniques. The following steps are performed:\n",
        "\n",
        "* Tokenization: The text of each tweet is broken down into individual words or tokens using the word_tokenize function from the nltk library.\n",
        "\n",
        "* Stop Word Removal: Common words (like \"the,\" \"a,\" \"is\") that don't carry much meaning are removed from the tokenized text to reduce noise and improve analysis accuracy.\n",
        "\n",
        "* Lemmatization: Words are reduced to their base or root form (e.g., \"running\" becomes \"run\") using the WordNetLemmatizer to standardize the vocabulary and group similar words together.\n",
        "\n",
        "* Text Recombination: The preprocessed tokens are combined back into a single text string for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zw8XhSX1fzIc",
      "metadata": {
        "id": "zw8XhSX1fzIc"
      },
      "outputs": [],
      "source": [
        "tweets_df_no_retweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9654bbf-4643-47d9-a534-f6ba4e8f9111",
      "metadata": {
        "id": "f9654bbf-4643-47d9-a534-f6ba4e8f9111"
      },
      "outputs": [],
      "source": [
        "print(preprocessed_df_test['lemmatized_text'].apply(type).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uhWbdBrPrTvy",
      "metadata": {
        "id": "uhWbdBrPrTvy"
      },
      "outputs": [],
      "source": [
        "# Preprocess text in the 'cleaned_text' column\n",
        "preprocessed_df_test = preprocess_text(df_test, text_column='cleaned_text')\n",
        "# Display the preprocessed DataFrame\n",
        "preprocessed_df_test[['cleaned_text', 'lemmatized_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T9567GjY2sBe",
      "metadata": {
        "id": "T9567GjY2sBe"
      },
      "outputs": [],
      "source": [
        "# apply lemmatization to entire dataframe\n",
        "lemmatized_df = preprocess_text(cleaned_text_df, text_column='cleaned_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OEpUVykLWiP5",
      "metadata": {
        "id": "OEpUVykLWiP5"
      },
      "outputs": [],
      "source": [
        "lemmatized_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NmExG5RhtM73",
      "metadata": {
        "id": "NmExG5RhtM73"
      },
      "outputs": [],
      "source": [
        "lemmatized_df= lemmatized_df.dropna(subset=['lemmatized_text'])\n",
        "# checking NaNs\n",
        "lemmatized_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eQ7jT1e-rSV",
      "metadata": {
        "id": "4eQ7jT1e-rSV"
      },
      "outputs": [],
      "source": [
        "#Random 10 rows\n",
        "lemmatized_df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SuA0v2ka-wZI",
      "metadata": {
        "id": "SuA0v2ka-wZI"
      },
      "outputs": [],
      "source": [
        "#cleanup\n",
        "del tweets_df_no_retweets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1uQsQ8CI5rTG",
      "metadata": {
        "id": "1uQsQ8CI5rTG"
      },
      "source": [
        "## 3.3. Feature Extraction\n",
        "In this section, we'll transform the preprocessed text data into numerical representations suitable for machine learning models. We'll explore techniques like:\n",
        "\n",
        "N-grams: Extracting sequences of adjacent words (e.g., \"social distancing,\" \"stay home\") to capture contextual information.\n",
        "TF-IDF: Calculating word importance based on their frequency within a document and across the entire corpus.\n",
        "Embeddings (optional): Representing words as dense vectors to capture semantic relationships.\n",
        "These extracted features will serve as input for the sentiment analysis models in the next stage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BqK3b_82vt6b",
      "metadata": {
        "id": "BqK3b_82vt6b"
      },
      "source": [
        "First, we'll run test datafram through the N-grams generation, Bag of Words, TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HpqpRxgzwG6X",
      "metadata": {
        "id": "HpqpRxgzwG6X"
      },
      "outputs": [],
      "source": [
        "# Apply N-gram computation to test lemmatized twitter df\n",
        "ngram_matrix_0, vectorizer_0 = compute_ngrams(preprocessed_df_test, \\\n",
        "                                          text_column='lemmatized_text', ngram_range=(2, 3))\n",
        "\n",
        "# Feature names (n-grams)\n",
        "ngram_features_0 = vectorizer_0.get_feature_names_out()\n",
        "\n",
        "print(f\"Top 10 n-grams: {ngram_features_0[:10]}\")\n",
        "print(f\"Shape of n-gram matrix: {ngram_matrix_0.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yS_svUDn5O3R",
      "metadata": {
        "id": "yS_svUDn5O3R"
      },
      "outputs": [],
      "source": [
        "#Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # can be adjusted as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L9uvTwVdbnxI",
      "metadata": {
        "id": "L9uvTwVdbnxI"
      },
      "outputs": [],
      "source": [
        "# Fit the vectorizer to test lemmatized twitter df\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_df_test['lemmatized_text'])\n",
        "\n",
        "## Visualize the results\n",
        "# tfidf_matrix and tfidf_vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.toarray().sum(axis=0)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(zip(feature_names, tfidf_scores)))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xBm257Ts5Ub4",
      "metadata": {
        "id": "xBm257Ts5Ub4"
      },
      "outputs": [],
      "source": [
        "# Bag of Words for the test matrix\n",
        "bow_vectorizer = TfidfVectorizer(max_features=5000)  #  can be adjusted as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uyZF7DKM5C-W",
      "metadata": {
        "id": "uyZF7DKM5C-W"
      },
      "outputs": [],
      "source": [
        "# Fit the vectorizer to test lemmatized twitter df\n",
        "bow_matrix = bow_vectorizer.fit_transform(preprocessed_df_test['lemmatized_text'])\n",
        "\n",
        "# Plot heatmap\n",
        "subset_matrix = bow_matrix.toarray()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(subset_matrix, cmap=\"viridis\", annot=True, fmt=\".2g\",\n",
        "            xticklabels=bow_vectorizer.get_feature_names_out(),\n",
        "            yticklabels=range(bow_matrix.shape[0]))\n",
        "plt.title(\"Bag of Words Heatmap (Test Data)\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Tweets\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9XdGf2fEqI",
      "metadata": {
        "id": "3f9XdGf2fEqI"
      },
      "source": [
        "Extracting N-grams for the entire datset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UHccUBALfEQj",
      "metadata": {
        "id": "UHccUBALfEQj"
      },
      "outputs": [],
      "source": [
        "ngram_matrix_twitter, vectorizer_twitter = compute_ngrams(lemmatized_df, \\\n",
        "                                          text_column='lemmatized_text', ngram_range=(2, 3))\n",
        "\n",
        "# Feature names (n-grams)\n",
        "ngram_features_twitter = vectorizer_twitter.get_feature_names_out()\n",
        "\n",
        "print(f\"Top 10 n-grams: {ngram_features_twitter[:10]}\")\n",
        "print(f\"Shape of n-gram matrix: {ngram_features_twitter.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T1tFg4UZnwOb",
      "metadata": {
        "id": "T1tFg4UZnwOb"
      },
      "source": [
        "How N-grams are distributed by sentiments?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zkB0FY6knpP-",
      "metadata": {
        "id": "zkB0FY6knpP-"
      },
      "outputs": [],
      "source": [
        "# Convert the n-gram matrix to a DataFrame\n",
        "ngram_df = pd.DataFrame(\n",
        "    ngram_matrix_twitter.toarray(),\n",
        "    columns=ngram_features_twitter\n",
        ")\n",
        "ngram_df['sentiment'] = lemmatized_df['sentiment']  # Add the sentiment column\n",
        "\n",
        "# Aggregate n-grams by sentiment\n",
        "sentiment_ngrams = ngram_df.groupby('sentiment').sum()\n",
        "\n",
        "# Select top n-grams for each sentiment\n",
        "top_ngrams_per_sentiment = {}\n",
        "n_top = 10\n",
        "for sentiment in sentiment_ngrams.index:\n",
        "    top_ngrams = sentiment_ngrams.loc[sentiment].nlargest(n_top)\n",
        "    top_ngrams_per_sentiment[sentiment] = top_ngrams\n",
        "\n",
        "# Visualize the top n-grams for each sentiment\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "for i, (sentiment, top_ngrams) in enumerate(top_ngrams_per_sentiment.items(), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.barplot(\n",
        "    x=top_ngrams.values,\n",
        "    y=top_ngrams.index,\n",
        "    hue=top_ngrams.index,  # Assign hue to the y variable\n",
        "    dodge=False,           # Ensure no splitting\n",
        "    legend=False,          # Disable the legend\n",
        "    palette='viridis'\n",
        "    )\n",
        "    plt.title(f\"Top {n_top} N-grams ({sentiment})\", fontsize=14)\n",
        "    plt.xlabel('Frequency', fontsize=12)\n",
        "    plt.ylabel('N-Gram', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(images_dir,\"N-Grams_3.1.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "#save to /images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FeqTbIb4wtQs",
      "metadata": {
        "id": "FeqTbIb4wtQs"
      },
      "outputs": [],
      "source": [
        "# Fit the BoW vectorizer to full twitter data\n",
        "bow_matrix = bow_vectorizer.fit_transform(lemmatized_df['lemmatized_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cryPP72seY0l",
      "metadata": {
        "id": "cryPP72seY0l"
      },
      "outputs": [],
      "source": [
        "# Sum frequencies for each term\n",
        "term_frequencies = bow_matrix.sum(axis=0).A1  # Sum along columns\n",
        "top_indices = term_frequencies.argsort()[-20:][::-1]  # Indices of top 20 terms\n",
        "\n",
        "# Subset matrix for these terms\n",
        "subset_matrix = bow_matrix[:, top_indices][:20].toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(subset_matrix, cmap='viridis', annot=True, fmt=\".2f\",\n",
        "            xticklabels=bow_vectorizer.get_feature_names_out()[top_indices],  # Feature names for top terms\n",
        "            yticklabels=range(20))\n",
        "plt.title('Heatmap of Top 20 Terms (BoW)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7iZuFzTwwwm",
      "metadata": {
        "id": "c7iZuFzTwwwm"
      },
      "outputs": [],
      "source": [
        "# Fit the tf-idf vectorizer to full twitter data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_df['lemmatized_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dvQ36XgeZB3",
      "metadata": {
        "id": "9dvQ36XgeZB3"
      },
      "outputs": [],
      "source": [
        "## Visualize the results\n",
        "# Compute term frequencies\n",
        "tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
        "\n",
        "# Get feature names from the tfidf_vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Select top 200 terms (adjust if needed)\n",
        "top_indices = np.argsort(tfidf_scores)[-200:]\n",
        "top_features = feature_names[top_indices] # Now, feature_names is from tfidf_vectorizer\n",
        "top_scores = tfidf_scores[top_indices]\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(\n",
        "    dict(zip(top_features, top_scores))\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig(os.path.join(images_dir,\"TFIDF_3.1.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-jzHSx2ntiBt",
      "metadata": {
        "id": "-jzHSx2ntiBt"
      },
      "source": [
        "Embeddings (BERT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IKb5Y14j_-Cf",
      "metadata": {
        "id": "IKb5Y14j_-Cf"
      },
      "outputs": [],
      "source": [
        "# Load GloVe embeddings\n",
        "glove_file = \"glove.twitter.27B.200d.txt\"  # 200-dimensional embeddings\n",
        "glove_path = os.path.join(data_dir, glove_file)\n",
        "embeddings_dict = load_glove_embeddings(glove_path)\n",
        "\n",
        "#Compute embeddings for a dataset\n",
        "lemmatized_df['sentence_embedding'] = lemmatized_df['lemmatized_text'].apply(\n",
        "    lambda x: compute_sentence_embedding(x, embeddings_dict, embedding_dim=200)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DCV2GAjXuJU3",
      "metadata": {
        "id": "DCV2GAjXuJU3"
      },
      "outputs": [],
      "source": [
        "# Getting features ready for use:\n",
        "# Convert embeddings into a 2D NumPy array\n",
        "embeddings_matrix = np.vstack(lemmatized_df['sentence_embedding'].values)\n",
        "embeddings_sparse = csr_matrix(embeddings_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JdXlVuEf_JfU",
      "metadata": {
        "id": "JdXlVuEf_JfU"
      },
      "outputs": [],
      "source": [
        "#cleanup\n",
        "del ngram_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ZLUlvn-57Z_",
      "metadata": {
        "id": "7ZLUlvn-57Z_"
      },
      "source": [
        "# 4. Sentiment Classification\n",
        "\n",
        "This section aims to analyze the sentiment expressed in the preprocessed tweets using machine learning models. The goal is to classify tweets into different sentiment categories (e.g., positive, negative, neutral) and to understand the overall sentiment trends within the dataset. This involves:\n",
        "\n",
        "* Model Selection and Training: Choosing a suitable classification model and training it on the preprocessed tweet data. We'll start with a baseline simple model.\n",
        "\n",
        "* Model Evaluation: Assessing the performance of the trained model using appropriate metrics.\n",
        "* Insights and Visualization: Presenting the results of the sentiment analysis, including visualizations and insights derived from the model's predictions.\n",
        "\n",
        "\n",
        "**Input Overview**\n",
        "\n",
        "* Our input features `X` will be a combination (concatenation) of computed text features:\n",
        "`bow_matrix`, `tfidf_matrix`, `ngram_matrix_twitter`, `embeddings_sparse`.\n",
        "* Our target variable `y` is the sentiment : `{neg, neu, pos}`.\n",
        "* Our performance metric will be `f1-score`.   \n",
        "* X dimentions: `170675x10200`.\n",
        "* Sentiment distribution (class imbalance):  \n",
        "`neu:76894, pos:50239, neg:43542`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VuioIhl14jJS",
      "metadata": {
        "id": "VuioIhl14jJS"
      },
      "outputs": [],
      "source": [
        "# Features and labels\n",
        "X_bow = hstack([bow_matrix, ngram_matrix_twitter, embeddings_sparse])\n",
        "# X_tfidf = hstack([tfidf_matrix, ngram_matrix_twitter, embeddings_sparse])\n",
        "X_bow.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q7MoWXlRAOIj",
      "metadata": {
        "id": "Q7MoWXlRAOIj"
      },
      "outputs": [],
      "source": [
        "# Get feature names and align them with the combined feature matrix\n",
        "bow_features = bow_vectorizer.get_feature_names_out()\n",
        "ngram_features = vectorizer_twitter.get_feature_names_out()\n",
        "embedding_dim = 200\n",
        "embedding_feature_names = [f\"embedding_{i}\" for i in range(embedding_dim)]\n",
        "\n",
        "all_features = np.concatenate([bow_features, ngram_features, embedding_feature_names])  # Combine feature names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4gIrwKh2vT0A",
      "metadata": {
        "id": "4gIrwKh2vT0A",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# check sentiment distribution between all 3 versions:\n",
        "print(lemmatized_df['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m7djBMSQ4oHO",
      "metadata": {
        "id": "m7djBMSQ4oHO"
      },
      "source": [
        "##  4.1 Train-Test Data Split\n",
        " Split proportions: 70% training, 15% testing, 15% validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pT6GL1sdvTAO",
      "metadata": {
        "id": "pT6GL1sdvTAO"
      },
      "outputs": [],
      "source": [
        "y = lemmatized_df['sentiment']     # Sentiment labels\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the sentiment labels\n",
        "y_encoded = label_encoder.fit_transform(y)  # y contains 'pos', 'neg', 'neu'\n",
        "\n",
        "# Check the mapping\n",
        "print(\"Label Mapping:\", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150c9415-5bf7-4ff8-98df-d841a7bba890",
      "metadata": {
        "id": "150c9415-5bf7-4ff8-98df-d841a7bba890"
      },
      "outputs": [],
      "source": [
        "# Define custom colors for the sentiments\n",
        "sentiment_counts = y.value_counts()\n",
        "custom_colors = {\n",
        "    'pos': '#90EE90',  # Light Green\n",
        "    'neu': '#ADD8E6',   # Light Blue\n",
        "    'neg': '#FFB6C1'   # Light Red\n",
        "}\n",
        "\n",
        "colors = [custom_colors.get(sentiment, '#D3D3D3') for sentiment in sentiment_counts.index]\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sentiment_counts.values,\n",
        "    labels=sentiment_counts.index,\n",
        "    autopct=lambda p: f'{p:.1f}% ({int(p * sum(sentiment_counts.values) / 100)})',  # Percentage and count\n",
        "    colors=colors,  # Use custom colors\n",
        "    startangle=90\n",
        ")\n",
        "plt.title('Sentiment Distribution', fontsize=14)\n",
        "plt.savefig(os.path.join(images_dir,\"Sentiment_Dist_4.1.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "#save to /images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cc2jlpJO5vAk",
      "metadata": {
        "id": "Cc2jlpJO5vAk"
      },
      "outputs": [],
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_bow, y_encoded, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SkoWwP420l-9",
      "metadata": {
        "id": "SkoWwP420l-9"
      },
      "outputs": [],
      "source": [
        "# size of each set\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Number of features\n",
        "print(f\"Number of features: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DWZcqR4EYD4m",
      "metadata": {
        "id": "DWZcqR4EYD4m"
      },
      "outputs": [],
      "source": [
        "# save features and variables for future use, if needed\n",
        "# pd.to_pickle(X_train, os.path.join(model_dir, \"X_train.pkl\"))\n",
        "# pd.to_pickle(X_val, os.path.join(model_dir, \"X_val.pkl\"))\n",
        "# pd.to_pickle(X_test, os.path.join(model_dir, \"X_test.pkl\"))\n",
        "# pd.to_pickle(y_train, os.path.join(model_dir, \"y_train.pkl\"))\n",
        "# pd.to_pickle(y_val, os.path.join(model_dir, \"y_val.pkl\"))\n",
        "# pd.to_pickle(y_test, os.path.join(model_dir, \"y_test.pkl\"))\n",
        "# pd.to_pickle(label_encoder, os.path.join(model_dir, \"label_encoder.pkl\"))\n",
        "# pd.to_pickle(bow_vectorizer, os.path.join(model_dir, \"bow_vectorizer.pkl\"))\n",
        "# pd.to_pickle(ngram_features_twitter, os.path.join(model_dir, \"vectorizer_twitter.pkl\"))\n",
        "# pd.to_pickle(embeddings_dict, os.path.join(model_dir, \"embeddings_dict.pkl\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5C0RTXca1uq3",
      "metadata": {
        "id": "5C0RTXca1uq3"
      },
      "source": [
        "## 4.2 Sentiment Classification with Simple Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5iyfqcVVYZ83",
      "metadata": {
        "id": "5iyfqcVVYZ83"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Model\n",
        "log_reg = LogisticRegression(\n",
        "    penalty='l2',  # L2 regularization (Ridge regression penalty)\n",
        "    C=1.0,  # Regularization strength (smaller = stronger regularization)\n",
        "    max_iter=1000,  # Ensure convergence\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Save/load logreg\n",
        "path_to_base_model = os.path.join(models_dir, 'trained_base_model.pkl')\n",
        "\n",
        "if os.path.exists(path_to_base_model):\n",
        "  with open(path_to_base_model, 'rb') as file:\n",
        "    log_reg = pickle.load(file)\n",
        "  print(\"Loaded Base LogReg model\")\n",
        "\n",
        "else:\n",
        "  print(\"Base Model not found, fitting...\")\n",
        "  # Train the model\n",
        "  log_reg.fit(X_train, y_train)\n",
        "  with open(path_to_base_model, 'wb') as file:\n",
        "    pickle.dump(log_reg, file)\n",
        "    print(\"Base Model saved ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7S8e1VYnZVYm",
      "metadata": {
        "id": "7S8e1VYnZVYm"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "y_val_log = log_reg.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_val\n",
        "\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_log_decoded = label_encoder.inverse_transform(y_pred_log)\n",
        "y_val_log_decoded = label_encoder.inverse_transform(y_val_log)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Base Model LogReg Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_log_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Base Model LogReg Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_log_decoded, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vGymIuwFaIkn",
      "metadata": {
        "id": "vGymIuwFaIkn"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_decoded, y_pred_log_decoded, labels=label_encoder.classes_)\n",
        "\n",
        "# Normalize confusion matrix by rows to show percentages\n",
        "#cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages\n",
        "\n",
        "# Display the normalized confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(cmap='Blues', colorbar=True, ax=ax)\n",
        "\n",
        "plt.title(\"LogReg Confusion Matrix on Test Set\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26956b6-f687-42a4-a8ff-86393a7d1c95",
      "metadata": {
        "id": "e26956b6-f687-42a4-a8ff-86393a7d1c95"
      },
      "source": [
        "## 4.3 Selecting Advanced Model for Feature Reduction\n",
        "In this section we'll deploy several advanced models:\n",
        "SDG, Random Forest, Decision Trees, XGBoost. We'll evaluate the performance of every model and pick the best-performing to use for feature reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023267fd-7111-47c8-9ba3-994c6d386bb9",
      "metadata": {
        "id": "023267fd-7111-47c8-9ba3-994c6d386bb9"
      },
      "outputs": [],
      "source": [
        "# SGDClassifier with logistic regression loss\n",
        "sgd_model = SGDClassifier(\n",
        "    loss='log_loss',          # Logistic regression loss\n",
        "    penalty='l2',             # L2 regularization\n",
        "    alpha=1e-4,               # Regularization strength\n",
        "    max_iter=1000,            # Maximum number of iterations\n",
        "    tol=1e-3,                 # Convergence tolerance\n",
        "    random_state=42\n",
        ")\n",
        "# Random Forest model with specific parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Desicion Tree with specific parameters\n",
        "dt_model =DecisionTreeClassifier(criterion = 'gini',\n",
        "                                 max_depth = 20,\n",
        "                                 min_samples_leaf = 2,\n",
        "                                 min_samples_split = 10,\n",
        "                                 random_state=42)\n",
        "\n",
        "# Define the XGBoost model with specific parameters\n",
        "xgb_model = XGBClassifier(learning_rate=0.2, max_depth=10, n_estimators=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nN9PpQcXKzGs",
      "metadata": {
        "id": "nN9PpQcXKzGs"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the DT model\n",
        "path_to_dt_model = os.path.join(models_dir, 'trained_dt_model.pkl')\n",
        "\n",
        "if os.path.exists(path_to_dt_model):\n",
        "  with open(path_to_dt_model, 'rb') as file:\n",
        "    dt_model = pickle.load(file)\n",
        "  print(\"Loaded DT model\")\n",
        "\n",
        "else:\n",
        "  print(\"DT Model not found, fitting...\")\n",
        "  dt_model.fit(X_train, y_train)\n",
        "  with open(path_to_dt_model, 'wb') as file:\n",
        "    pickle.dump(dt_model, file)\n",
        "    print(\"DT Model saved ...\")\n",
        "\n",
        "# Evaluate the DT model\n",
        "y_pred_dt= dt_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zI1ttTwsLPSL",
      "metadata": {
        "id": "zI1ttTwsLPSL"
      },
      "outputs": [],
      "source": [
        "path_to_xgb_model = os.path.join(models_dir, 'trained_xgb_model.pkl')\n",
        "\n",
        "if os.path.exists(path_to_xgb_model):\n",
        "  with open(path_to_xgb_model, 'rb') as file:\n",
        "    xgb_model = pickle.load(file)\n",
        "  print(\"Loaded XGB model...\")\n",
        "\n",
        "else:\n",
        "  print(\"Model not found, fitting...\")\n",
        "  xgb_model.fit(X_train, y_train)\n",
        "  with open(path_to_xgb_model, 'wb') as file:\n",
        "    pickle.dump(xgb_model, file)\n",
        "    print(\"Model saved ...\")\n",
        "\n",
        "# Evaluate the XGBoost model\n",
        "y_pred_xgb= xgb_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rPGe6uI3L3Zs",
      "metadata": {
        "id": "rPGe6uI3L3Zs"
      },
      "outputs": [],
      "source": [
        "# RF model is too large to be stored in GitHuib (~70 Mb)\n",
        "# The only model yopu'd have to re-compute (~5 min)\n",
        "path_to_rf_model = os.path.join(models_dir, 'trained_rf_model.pkl')\n",
        "\n",
        "if os.path.exists(path_to_rf_model):\n",
        "  with open(path_to_rf_model, 'rb') as file:\n",
        "    rf_model = pickle.load(file)\n",
        "  print(\"Loaded RF model...\")\n",
        "\n",
        "else:\n",
        "  print(\"Model not found, fitting...\")\n",
        "  rf_model.fit(X_train, y_train)\n",
        "  with open(path_to_rf_model, 'wb') as file:\n",
        "    pickle.dump(rf_model, file)\n",
        "    print(\"Model saved ...\")\n",
        "\n",
        "# Evaluate the RF model\n",
        "y_pred_rf = rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mftoh9zqMKTt",
      "metadata": {
        "id": "Mftoh9zqMKTt"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the SGD model\n",
        "path_to_sgd_model = os.path.join(models_dir, 'trained_sgd_model.pkl')\n",
        "\n",
        "if os.path.exists(path_to_sgd_model):\n",
        "  with open(path_to_sgd_model, 'rb') as file:\n",
        "    sgd_model = pickle.load(file)\n",
        "  print(\"Loaded SGD model...\")\n",
        "else:\n",
        "  print(\"Model not found, fitting...\")\n",
        "  sgd_model.fit(X_train, y_train)\n",
        "  with open(path_to_sgd_model, 'wb') as file:\n",
        "    pickle.dump(sgd_model, file)\n",
        "    print(\"Model saved ...\")\n",
        "sgd_model.fit(X_train, y_train)\n",
        "y_pred_sgd = sgd_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454c3ae4-1959-4c59-ab36-9fe86c82803c",
      "metadata": {
        "id": "454c3ae4-1959-4c59-ab36-9fe86c82803c"
      },
      "outputs": [],
      "source": [
        "# Decode labels for y_test and y_pred\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_pred_xgb_decoded = label_encoder.inverse_transform(y_pred_xgb)\n",
        "\n",
        "# SGD\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_pred_sgd_decoded = label_encoder.inverse_transform(y_pred_sgd)\n",
        "#RF\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_pred_rf_decoded = label_encoder.inverse_transform(y_pred_rf)\n",
        "#DT\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_pred_dt_decoded = label_encoder.inverse_transform(y_pred_dt)\n",
        "\n",
        "# Print Accuracy\n",
        "print(f\"Test Accuracy SGD Model: {accuracy_score(y_test_decoded, y_pred_sgd_decoded):.2f}\")\n",
        "print(f\"Test Accuracy XGB Model: {accuracy_score(y_test_decoded, y_pred_xgb_decoded):.2f}\")\n",
        "print(f\"Test Accuracy RF Model: {accuracy_score(y_test_decoded, y_pred_rf_decoded):.2f}\")\n",
        "print(f\"Test Accuracy DT Model: {accuracy_score(y_test_decoded, y_pred_dt_decoded):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F8Uu-FPe2xIo",
      "metadata": {
        "id": "F8Uu-FPe2xIo"
      },
      "source": [
        "## 4.4 Feature Reduction using Best Advanced Model\n",
        "We'll be using XGB model for feature selection since it's the most accurate one.\n",
        "Other dimentioonality reduction tools we can use would be PCA or SVD. These tools would create a linear combinations of featurer but we won't be able to interpret the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f52921-f67f-4174-b560-490975fbe842",
      "metadata": {
        "id": "c3f52921-f67f-4174-b560-490975fbe842"
      },
      "source": [
        "**Example Top 10 Most Important Feratures From XGB Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b30f399-3cb8-4cd5-a9bf-55b72d9355d0",
      "metadata": {
        "id": "4b30f399-3cb8-4cd5-a9bf-55b72d9355d0"
      },
      "outputs": [],
      "source": [
        "# Get feature importances\n",
        "# Map feature indices to actual feature names\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "importance_df = pd.DataFrame({'Feature': all_features, 'Importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "top_features = importance_df['Feature'][:10]  # Top feature indices or names\n",
        "\n",
        "# Plot top 10 features with their names\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_features, importance_df['Importance'][:10], color='lightcoral')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 10 Feature Importances (XGB)')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for descending order\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c82e94-c5d2-4839-b30e-45f9e0967772",
      "metadata": {
        "id": "62c82e94-c5d2-4839-b30e-45f9e0967772"
      },
      "source": [
        "**Reducing Original Dataset to Top 1000 Most Important Features from XGB Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d796efb5-6c06-4cac-9427-5464d0219c5f",
      "metadata": {
        "id": "d796efb5-6c06-4cac-9427-5464d0219c5f"
      },
      "outputs": [],
      "source": [
        "# Select top k features\n",
        "k = 1000\n",
        "top_k_features = importance_df['Feature'][:k]\n",
        "\n",
        "X_train_reduced = X_train[:, top_k_features.index]\n",
        "X_test_reduced = X_test[:, top_k_features.index]\n",
        "X_val_reduced = X_val[:, top_k_features.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f382ac57-3c16-4264-8d87-4046e30ca681",
      "metadata": {
        "id": "f382ac57-3c16-4264-8d87-4046e30ca681"
      },
      "outputs": [],
      "source": [
        "X_train_reduced.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1l4AxpFoa8BU",
      "metadata": {
        "id": "1l4AxpFoa8BU"
      },
      "source": [
        "## 4.5 Tuning Hyperparameters for Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kQNYPilwa5M2",
      "metadata": {
        "id": "kQNYPilwa5M2"
      },
      "outputs": [],
      "source": [
        "path_to_model = os.path.join(models_dir, 'trained_model.pkl')\n",
        "\n",
        "if os.path.exists(path_to_model):\n",
        "  with open(path_to_model, 'rb') as file:\n",
        "    best_model = pickle.load(file)\n",
        "  print(\"Loaded model, not running GridSearchCV...\")\n",
        "\n",
        "else:\n",
        "  print(\"Model not found, running GridSearchCV...\")\n",
        "# Define parameter grid\n",
        "  param_grid = {\n",
        "      'penalty': ['l1', 'l2'],\n",
        "      'C': [0.01, 0.1, 1],\n",
        "      'solver': ['liblinear', 'saga'],\n",
        "      'class_weight': [None, 'balanced']\n",
        "  }\n",
        "\n",
        "  # Create Logistic Regression model\n",
        "  log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "\n",
        "  # Grid search with F1 scoring\n",
        "  grid_search = GridSearchCV(\n",
        "      estimator=log_reg,\n",
        "      param_grid=param_grid,\n",
        "      scoring='recall_weighted',  # Weighted Precicion score\n",
        "      cv=3,\n",
        "      verbose=2,\n",
        "      n_jobs=1\n",
        "  )\n",
        "\n",
        "  # Fit on the training data\n",
        "  grid_search.fit(X_train_reduced, y_train)\n",
        "  best_model = grid_search.best_estimator_\n",
        "  # Best parameters and F1 score\n",
        "  print(\"Best Parameters:\", grid_search.best_params_)\n",
        "  print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "  with open(path_to_model, 'wb') as file:\n",
        "    pickle.dump(best_model, file)\n",
        "    print(\"Model saved ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0pXuFA03glp2",
      "metadata": {
        "id": "0pXuFA03glp2"
      },
      "source": [
        "## 4.6 Improved Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SOgDEwY8E-sI",
      "metadata": {
        "id": "SOgDEwY8E-sI"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the best model\n",
        "y_pred_best = best_model.predict(X_test_reduced)\n",
        "y_val_best = best_model.predict(X_val_reduced)\n",
        "\n",
        "# Decode labels for y_test and y_pred_best\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_best_decoded = label_encoder.inverse_transform(y_pred_best)\n",
        "y_val_best_decoded = label_encoder.inverse_transform(y_val_best)\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_best_decoded, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W7rD9fV7E-dr",
      "metadata": {
        "id": "W7rD9fV7E-dr"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_val_decoded, y_val_best_decoded, labels=label_encoder.classes_)\n",
        "\n",
        "# Normalize confusion matrix by rows to show percentages\n",
        "#cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages\n",
        "\n",
        "# Display the normalized confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(cmap='Blues', colorbar=True, ax=ax)\n",
        "\n",
        "plt.title(\"LogReg Confusion Matrix on Validation Set\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.savefig(os.path.join(images_dir,\"CM_4.6.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "# save to mages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f49c888-e062-4146-8418-6233112876e4",
      "metadata": {
        "id": "6f49c888-e062-4146-8418-6233112876e4"
      },
      "source": [
        "## 4.7 Feature Interpretation\n",
        "\n",
        "**General Observations:**\n",
        "\n",
        "* Magnitude of Coefficients:\n",
        "\n",
        "-Larger magnitudes (e.g., `death` for \"Negative\" or `best` for \"Positive\") show that the word has a stronger impact on the prediction for that class.\n",
        "\n",
        "* Overlap of Features Across Classes:\n",
        "\n",
        "Some words have high coefficients in multiple classes, leading to competition during prediction.\n",
        "For example, `death` has high negative coefficient in both Positive and Neutral sentiment, leading to competition between the two categiries when the world is absent.\n",
        "\n",
        "**Model Overview:**\n",
        "Overall Model Characteristics\n",
        "Strengths:\n",
        "The model is moderately regularized (C=1), meaning it can capture important patterns while avoiding overfitting.\n",
        "L1 regularization promotes feature sparsity, making the model interpretable by effectively performing feature selection.\n",
        "\n",
        "The dataset is imbalanced and  the prescense of class weighting `class_weight=balanced` ensures the optimal performance.\n",
        "Weaknesses:\n",
        "The `liblinear` solver not efficient for large sparse datasets or multiclass classification.\n",
        "L1 regularization might oversimplify the model if too many features are deemed irrelevant.\n",
        "\n",
        "**Model Performance:**\n",
        "\n",
        "Strong Points:\n",
        "\n",
        "The model performs very well overall, with high precision, recall, and F1-scores across all classes.\n",
        "\n",
        "The \"Neutral\" class has the highest recall (0.99), suggesting the model is highly sensitive to identifying neutral tweets.\n",
        "\n",
        "Areas for Improvement:\n",
        "\n",
        "Recall for the \"Negative\" class is lower (0.87), indicating the model misses some negative samples. This\n",
        "could be due to class imbalance or overlap in feature space. Some negative tweets might share characteristics with neutral or positive tweets.\n",
        "\n",
        "Precision for the \"Neutral\" class is slightly lower (0.92), suggesting some false positives for this majority class.\n",
        "\n",
        "Imbalanced Classes:\n",
        "\n",
        "The class distribution is imbalanced, with \"Neutral\" being the majority class. This can affect performance metrics, as the model might be biased toward the majority class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10480e0f-1b09-431d-ae5d-ff4d72eeb8df",
      "metadata": {
        "id": "10480e0f-1b09-431d-ae5d-ff4d72eeb8df"
      },
      "source": [
        "**Displaying Top 10 Features For Each Class:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df2ca990-b51a-4785-918d-cbd9ed23e25f",
      "metadata": {
        "id": "df2ca990-b51a-4785-918d-cbd9ed23e25f"
      },
      "outputs": [],
      "source": [
        "# Displaying top 10 features for each class:\n",
        "reduced_feaures = all_features[top_k_features.index]\n",
        "classes = best_model.classes_  # Class labels\n",
        "num_features_to_plot = 10  # Number of top features to display\n",
        "coefficients = best_model.coef_  # Coefficients for each class\n",
        "# Define a fixed color for each class\n",
        "class_colors = {\n",
        "    0: 'lightcoral',  # Negative\n",
        "    1: 'lightblue',   # Neutral\n",
        "    2: 'lightgreen'   # Positive\n",
        "}\n",
        "\n",
        "num_features_to_plot = 10  # Number of top features to display\n",
        "\n",
        "for class_idx in range(len(classes)):\n",
        "    # Decode the class label\n",
        "    class_label = label_encoder.inverse_transform([class_idx])[0]\n",
        "\n",
        "    # Get coefficients for the current class\n",
        "    class_coefficients = coefficients[class_idx]\n",
        "\n",
        "    # Get indices of the top features by absolute value of the coefficients\n",
        "    top_features_idx = np.argsort(np.abs(class_coefficients))[-num_features_to_plot:]\n",
        "\n",
        "    # Get corresponding feature names and coefficients\n",
        "    top_feature_names = [reduced_feaures[i] for i in top_features_idx]\n",
        "    top_feature_values = [class_coefficients[i] for i in top_features_idx]\n",
        "\n",
        "    # Assign the same color for all features of the current class\n",
        "    color = class_colors[class_idx]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(top_feature_names, top_feature_values, color=color)\n",
        "    plt.xlabel('Feature Importance', fontsize=12)\n",
        "    plt.ylabel('Features', fontsize=12)\n",
        "    plt.title(f'Top Features for Class: {class_label}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)  # Add vertical gridlines for reference\n",
        "    plt.tight_layout()  # Adjust layout for better spacing\n",
        "    plt.savefig(os.path.join(images_dir, f\"{class_idx}_Features_4.7.png\"), dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a0efd3-ed5a-43c2-a8ba-aa947412c540",
      "metadata": {
        "id": "44a0efd3-ed5a-43c2-a8ba-aa947412c540"
      },
      "source": [
        "**Feature Interprertation:**\n",
        "\n",
        "Class: Negative (neg)\n",
        "Interpretation: Words like `death`, `dangerous`, `fatality` have high positive coefficients for the \"Negative\" class. These words are strong indicators of negative sentiment in the dataset.\n",
        "\n",
        "When this word appears in a text, it strongly increases the probability of predicting the \"Negative\" class, possibly in the context of criticism or challenges.\n",
        "Actionable Insight: If these words are common, the model will consistently predict \"Negative\" when they are present.\n",
        "\n",
        "Class: Neutral (neu)\n",
        "Interpretation: Words like `free`, `help`, and `number` have relatively large negative coefficients. The absence of these words increases the likelihood of \"Neutral\" predictions.\n",
        "\n",
        "Class: Positive (pos)\n",
        "Interpretation: Words like `great`, `best`, `love`, and have high positive coefficients for the \"Positive\" class and strongly increases the likelihood of predicting \"Positive.\"\n",
        "Actionable Insight: The \"Positive\" class is strongly associated with words expressing happiness, approval, or success. Words like `death` and `cancer` have high negative value, indicating that they are inversely correlated with the target varible.   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iFbPQcpw74-o",
      "metadata": {
        "id": "iFbPQcpw74-o"
      },
      "source": [
        "# 5. Conclusion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98dfbfc-c1aa-4339-82fa-af791710af5f",
      "metadata": {
        "id": "f98dfbfc-c1aa-4339-82fa-af791710af5f"
      },
      "source": [
        "**The final model evaluation**\n",
        "\n",
        "Recapping the real-world problem as defined in the notebook:\n",
        "\n",
        "* Problem: Media outlets and public health organizations need a better understanding of how their COVID-19-related news influences public sentiment online.\n",
        "* Goal: Develop a system to analyze real-time feedback, classify sentiment, track sentiment trends, and identify patterns between news events and changes in sentiment.\n",
        "\n",
        "\n",
        "**Implications of Final Model Evaluation**\n",
        "\n",
        "* Model Reliability: The model performs reliably across various conditions when faced with new or unseen data. The goal is for the model to remain accurate over time.\n",
        "\n",
        "* Actionable Insights: If the model performs well in the evaluation, its outputs (sentiment classifications, trends, key themes) can be considered reliable for decision-making. Public health officials can use this data to gauge public response to health policies and modify communication strategies accordingly. Media outlets can use it to understand how their coverage is perceived. Influencer Identification: If the model can identify key influencers accurately, this can help organizations target their messages better. Misinformation Spread: The model can detect and identify keywords associated with misinformation spread and address it."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:Python_3_12]",
      "language": "python",
      "name": "conda-env-Python_3_12-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
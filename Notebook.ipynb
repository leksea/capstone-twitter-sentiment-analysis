{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70934233-4e52-4f0f-883b-56cbc48f299c",
      "metadata": {
        "id": "70934233-4e52-4f0f-883b-56cbc48f299c"
      },
      "source": [
        "# Twitter Sentiment Analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc",
      "metadata": {
        "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc"
      },
      "source": [
        "## Business Problem.\n",
        "\n",
        "We want to analyze a COVID-19 Twitter dataset to understand how positive and negative trends spread after news announcements. Additionally, we want to use a bot detection algorithm to determine what percentage of each sentiment is made up of bots and how this affects the general sentiment of the public (non-bots).\n",
        "\n",
        "Key Questions:\n",
        "\n",
        "How do positive and negative sentiments spread among users after a news announcement related to COVID-19?\n",
        "What proportion of tweets in each sentiment category (positive/negative/neutral) come from bots?\n",
        "How do bots influence the general sentiment of non-bot users?\n",
        "Purpose:\n",
        "\n",
        "Help media outlets measure the impact of their announcements on public sentiment.\n",
        "Assist public health agencies in identifying misinformation or bot-driven content to improve communication strategies.\n",
        "Support social media platforms in detecting and limiting bot activity that could distort public opinion.\n",
        "Goals:\n",
        "\n",
        "Track sentiment trends over time.\n",
        "Quantify bot participation in each sentiment category.\n",
        "Measure the influence of bots on genuine public sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets.\n",
        "\n",
        "1. [A Twitter Dataset of 40+ million tweets related to COVID-19.](https://zenodo.org/records/3723940)\n",
        "\n",
        "2. [Twitter Bots Accounts.](https://www.kaggle.com/datasets/davidmartngutirrez/twitter-bots-accounts)\n"
      ],
      "metadata": {
        "id": "lSJyODVEaJZs"
      },
      "id": "lSJyODVEaJZs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the required modules.\n",
        "\n",
        "We'll start with installing the requirements [available here](https://github.com/leksea/capstone-twitter-sentiment-analysis/blob/main/requirements.txt)."
      ],
      "metadata": {
        "id": "fQwFcnPjZpd9"
      },
      "id": "fQwFcnPjZpd9"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
        "!pip install -r 'requirements.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st62fukEaHir",
        "outputId": "866c8b3c-1a51-44ff-d00a-8531b1b53eb3"
      },
      "id": "st62fukEaHir",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-24 21:07:01--  https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]      98  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-24 21:07:01 (3.30 MB/s) - ‘requirements.txt’ saved [98/98]\n",
            "\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.8.1)\n",
            "Collecting cartopy (from -r requirements.txt (line 7))\n",
            "  Downloading Cartopy-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.9.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (11.0.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
            "Requirement already satisfied: jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from branca->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (2.0.6)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2024.12.14)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/dist-packages (from folium->-r requirements.txt (line 9)) (2024.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3->branca->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
            "Downloading Cartopy-0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cartopy\n",
            "Successfully installed cartopy-0.24.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing modules."
      ],
      "metadata": {
        "id": "Zd0e2IvvfuDJ"
      },
      "id": "Zd0e2IvvfuDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import branca.colormap as cm\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
        "import requests\n",
        "import folium\n",
        "from folium import plugins\n",
        "from folium.plugins import HeatMap\n",
        "import branca.colormap\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm, notebook\n",
        "%matplotlib inline\n",
        "# stop words for tokenizer\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "py58BnTUdbsd",
        "outputId": "cb589f4c-6fd6-4b1a-9cbd-4c1be76e6e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "py58BnTUdbsd",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading The Dataset."
      ],
      "metadata": {
        "id": "hnoqoMM-gr0F"
      },
      "id": "hnoqoMM-gr0F"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLmUhcBtfxFS"
      },
      "id": "dLmUhcBtfxFS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
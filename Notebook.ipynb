{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70934233-4e52-4f0f-883b-56cbc48f299c",
      "metadata": {
        "id": "70934233-4e52-4f0f-883b-56cbc48f299c"
      },
      "source": [
        "# Twitter Sentiment Analysis of Public Reaction to COVID-19 News\n",
        "\n",
        "**Project Overview:**\n",
        "\n",
        "This project aims to analyze a large dataset of COVID-19-related tweets to understand how public sentiment evolves and spreads in response to news announcements and events. By leveraging natural language processing (NLP) techniques and sentiment analysis models, we seek to gain valuable insights into the dynamics of online conversations surrounding the pandemic.\n",
        "\n",
        "**Importance and Motivation:**\n",
        "\n",
        "Understanding public sentiment during a global crisis like the COVID-19 pandemic is crucial for various stakeholders, including:\n",
        "\n",
        "- **Public Health Officials:** To gauge public response to health policies and interventions.\n",
        "- **Media Outlets:** To assess the impact of their news coverage on public perception.\n",
        "- **Government Agencies:** To monitor public opinion and tailor communication strategies.\n",
        "- **Researchers:** To study the spread of information and misinformation on social media.\n",
        "\n",
        "This project contributes to this understanding by providing a comprehensive analysis of Twitter data, revealing trends and patterns in public sentiment related to COVID-19.\n",
        "\n",
        "Notebook Structure\n",
        "---\n",
        "<details>\n",
        "<summary><b>1. Business Problem and Objectives</b></summary>\n",
        "   Define the problem being addressed and its relevance to real-world scenarios.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>2. Data Acquisition and Preparation</b></summary>\n",
        "\n",
        "- ### **2.1 Data Source and Download**  \n",
        "  Explanation of the dataset source and how it was obtained.  \n",
        "\n",
        "- ### **2.2 Installing Required Modules**  \n",
        "  List and install the libraries needed for the project.  \n",
        "\n",
        "- ### **2.3 Importing Modules and Global Variables**  \n",
        "  Set up imports and define constants or global variables.  \n",
        "\n",
        "- ### **2.4 Defining Supplemental Functions**  \n",
        "  Helper functions to streamline data processing.  \n",
        "\n",
        "- ### **2.5 Data Loading**  \n",
        "  Load the dataset into a DataFrame or suitable data structure.  \n",
        "\n",
        "- ### **2.6 Basic Data Understanding**  \n",
        "  Perform initial data exploration, including shape, columns, and types.  \n",
        "\n",
        "- ### **2.7 Data Cleaning and EDA: Date**  \n",
        "  Extract and analyze temporal trends in the dataset.\n",
        "\n",
        "- ### **2.8 Data Cleaning and EDA: Language**  \n",
        "  Extract relevant language subset.\n",
        "\n",
        "- ### **2.9 Data Cleaning and EDA: Location**  \n",
        "  Process location data to standardize and extract insights.  \n",
        "\n",
        "- ### **2.10 Data Cleaning and EDA: Source**  \n",
        "  Analyze the platforms from which tweets were sent.  \n",
        "\n",
        "- ### **2.11 Data Cleaning and EDA: Sentiment**  \n",
        "  Explore sentiment labels and their distribution.  \n",
        "\n",
        "- ### **2.12 Exploratory Data Analysis (EDA): Social Connections**  \n",
        "  Investigate user mentions, retweets, and network connections.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>3. Text Preprocessing and Feature Engineering</b></summary>\n",
        "\n",
        "- ### **3.1 Cleaning**  \n",
        "  Remove noise, including special characters, links, mentions, and hashtags.  \n",
        "\n",
        "- ### **3.2 Preprocessing**  \n",
        "  Tokenize, lemmatize, and remove stop words from the text data.  \n",
        "\n",
        "- ### **3.3 Feature Extraction**  \n",
        "  Generate n-grams, TF-IDF features, or embeddings for model input.  \n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>4. Sentiment Analysis</b></summary>\n",
        "\n",
        "- ### **4.1 Train-Test Data Split**  \n",
        "- ### **4.2 Base Model Training, Evaluation**   \n",
        "  Choose a simple base classification model and train it on the preprocessed data. Assess model performance using metrics like accuracy, precision, and recall.\n",
        "- ### **4.3 Model Hyperparameter Search**\n",
        "  Vary base model parameters to see if it improves model's perfomance.      \n",
        "- ### **4.4 Insights and Visualization**  \n",
        "  Visualize results and discuss findings, including strengths and limitations.\n",
        "- ### **4.5 Other Models Tested**\n",
        "  List other models tested, their parameters and performance.   \n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "<summary><b>6. Conclusion</b></summary>\n",
        "\n",
        "\n",
        "</details>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc",
      "metadata": {
        "id": "3d78d833-547c-4b7c-8527-1c64eef0d3cc"
      },
      "source": [
        "## 1.1 Business Problem and Objectives\n",
        "\n",
        "**Problem Statement:**\n",
        "\n",
        "Media outlets and public health organizations need a better understanding of how their COVID-19-related news and announcements influence public sentiment on Twitter. This project addresses this need by analyzing a large dataset of tweets to identify and track sentiment trends in response to news events.\n",
        "\n",
        "**Key Questions:**\n",
        "\n",
        "- How do positive and negative sentiments spread among users following a COVID-19 news announcement?\n",
        "- What are the key topics and themes associated with different sentiment trends?\n",
        "- Can we identify any patterns or correlations between news events and changes in public sentiment?\n",
        "\n",
        "**Project Objectives:**\n",
        "\n",
        "- To develop a robust NLP pipeline for cleaning, preprocessing, and analyzing Twitter data.\n",
        "- To apply sentiment analysis models to classify tweets and track sentiment trends over time.\n",
        "- To visualize and interpret the sentiment analysis results to provide actionable insights.\n",
        "- To potentially identify key influencers and networks driving sentiment on Twitter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Acquisition and Preparation\n"
      ],
      "metadata": {
        "id": "VveiYpccgaRY"
      },
      "id": "VveiYpccgaRY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Data Understanding.\n",
        "\n",
        "\n",
        "This section outlines the source of the data used in this project,  and provides instructions for downloading it.\n",
        "\n",
        "**Data Sources:**\n",
        "\n",
        "1. **Covid-19 Twitter Dataset:** The primary dataset for this Twitter sentiment analysis project is the \"Covid-19 Twitter Dataset\" available on Kaggle. This dataset contains a large collection of tweets related to COVID-19, including tweet text, user details, location, and sentiment labels.\n",
        "\n",
        "2. **GloVe Embeddings:** To enhance the analysis, we will utilize pre-trained GloVe embeddings from Stanford NLP. These word embeddings capture semantic relationships between words and can improve the performance of NLP models.\n",
        "\n",
        "**Data Relevance**:\n",
        "\n",
        "* The Covid-19 Twitter Dataset contains a vast collection of tweets related to the pandemic, providing a valuable source of public opinion and sentiment during this period.\n",
        "* This dataset is suitable for our project because it includes sentiment labels, allowing us to train and evaluate sentiment analysis models.\n",
        "\n",
        "\n",
        "**Data Limitations**\n",
        "\n",
        "* Recent changes to the Twitter API have significantly impacted the accessibility of tweet data for research and analysis.  Specifically, Twitter has severely restricted free API access. This means that retrieving the original dataset used in this project [(see Panacea's lab github page)](https://github.com/thepanacealab/covid19_twitter) is no longer possible without incurring substantial costs.\n",
        "* This dataset contains pre-computed sentiment labels, however, we don't know what method was used and how the accuracy of the sentiment was evaluated.\n",
        "\n",
        "**Download Instructions:**\n",
        "\n",
        "1. **Kaggle Dataset:** The dataset can be accessed and downloaded from the following Kaggle page:\n",
        "    [Covid-19 Twitter Dataset](https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset/data)\n",
        "\n",
        "2. ** GloVe Embeddings:** The pre-trained GloVe embeddings can be obtained from the [Stanford NLP website](https://nlp.stanford.edu/projects/glove/). For this project, we will use the \"glove.twitter.27B.zip\" file, which contains 10-200-dimensional embeddings trained on 27B Twitter tokens.\n",
        "\n",
        "**Data Storage:**\n",
        "\n",
        "   - **Local Execution:** If you are running the notebook locally, please download the dataset files and place them into a folder named `Data` within your project directory.\n",
        "  \n",
        "   - **Colab Environment:** If you are using Google Colab, it's best recommended to mount a google drive.\n",
        "\n",
        "**Data Loading:**\n",
        "\n",
        "1. **Loading Datasets:** code would check runtime environment and for local environment set the `data_dir` to be `Data` folder on the same level as the notebook. If you're using Colab, you'll need to adjust the defauld directory. Highly recommend to not use runtime-dependent directory.   \n",
        "2. **Loading GloVe Embeddings:** The script will automatically check if the GloVe embeddings are already present in the 'Data' directory. If not, it will download and extract them for you:\n",
        "```\n",
        "download_and_extract_glove(data_dir)\n",
        "```"
      ],
      "metadata": {
        "id": "lSJyODVEaJZs"
      },
      "id": "lSJyODVEaJZs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Installing Required Modules\n",
        "\n",
        "This section focuses on installing the necessary Python libraries and packages required for our Twitter sentiment analysis project. We accomplish this through the following steps:\n",
        "\n",
        "1. **Requirements File:**\n",
        "    - We retrieve the list of required packages from a `requirements.txt` file hosted on GitHub using `wget`. This file contains the names and versions of all the dependencies.\n",
        "    - This ensures that we install the correct versions of the libraries for compatibility and reproducibility.\n",
        "    - Here's the link to the requirements file on GitHub:\n",
        "       `https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt`\n",
        "\n",
        "2. **Installation using pip:**\n",
        "    - We use Python's `pip` package manager to install the libraries listed in the `requirements.txt` file.\n",
        "    - The `-r` flag instructs `pip` to read the requirements file and install all the packages listed within."
      ],
      "metadata": {
        "id": "fQwFcnPjZpd9"
      },
      "id": "fQwFcnPjZpd9"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
        "!pip install -r 'requirements.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st62fukEaHir",
        "outputId": "8c49c3cd-c0f5-4748-cb39-a4f2c74583d5"
      },
      "id": "st62fukEaHir",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-02 05:46:42--  https://raw.githubusercontent.com/leksea/capstone-twitter-sentiment-analysis/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165 [text/plain]\n",
            "Saving to: ‘requirements.txt.1’\n",
            "\n",
            "\rrequirements.txt.1    0%[                    ]       0  --.-KB/s               \rrequirements.txt.1  100%[===================>]     165  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-02 05:46:43 (2.92 MB/s) - ‘requirements.txt.1’ saved [165/165]\n",
            "\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: branca in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.8.1)\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.32.3)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.19.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (3.9.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (11.0.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: emot in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (3.1)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.4.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (2.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.4.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (7.34.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (2.5.1+cu121)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 4)) (2024.2)\n",
            "Requirement already satisfied: jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from branca->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (2.0.6)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.10/dist-packages (from cartopy->-r requirements.txt (line 7)) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 8)) (2024.12.14)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.10/dist-packages (from folium->-r requirements.txt (line 9)) (2024.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 10)) (2024.11.6)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy->-r requirements.txt (line 15)) (2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r requirements.txt (line 20)) (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (0.27.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 21)) (0.4.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 22)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 22)) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 20)) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3->branca->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 20)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 20)) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Importing Modules and Global Variables\n",
        "\n",
        "This section focuses on setting up the necessary environment for our analysis by importing the required Python modules and declaring global variables. We perform the following:\n",
        "\n",
        "1. **Module Imports:** We import a variety of modules that will be essential for data manipulation, analysis, visualization, and natural language processing tasks. These modules include:\n",
        "\n",
        "    - **Built-in Modules:** `os`, `string`, `re`, `glob`, `time`, `datetime`, `concurrent.futures`, `json`, `collections`, `concurrent`.\n",
        "    - **Data Processing and Analysis:** `numpy`, `pandas`.\n",
        "    - **Visualization:** `matplotlib.pyplot`, `seaborn`, `networkx`, `folium`, `branca.colormap`, `cartopy`.\n",
        "    - **Natural Language Processing (NLP):** `nltk`, `emot`, `emoji`.\n",
        "    - **Machine Learning:** `sklearn`.\n",
        "\n",
        "2. **Global Variable Declarations:**\n",
        "    - We define and initialize global variables that will be used throughout the analysis. These include:\n",
        "        - `geolocator`: An instance of the `Nominatim` geolocator from the `geopy` library for location standardization.\n",
        "        - `tqdm`: Enabling progress bars for long computations using `tqdm.pandas()`.\n",
        "        - `stop_words`: A set of English stop words from `nltk.corpus` for text preprocessing.\n",
        "        - `lemmatizer`: An instance of the `WordNetLemmatizer` from `nltk.stem` for lemmatization.\n",
        "\n",
        "3. **Downloading NLP Resources:**\n",
        "    - We download necessary resources for NLP tasks, such as stopwords, wordnet, and punkt using `nltk.download()`."
      ],
      "metadata": {
        "id": "Zd0e2IvvfuDJ"
      },
      "id": "Zd0e2IvvfuDJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# built-in modules\n",
        "import os\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "from collections import Counter\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "# Optimization with Parallel Computing:\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "# url processing for extracting the coordinates\n",
        "import requests\n",
        "# progress bar monitoring\n",
        "from tqdm import tqdm\n",
        "# data manupulation, analysis, sparce matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import rand, randint\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "# general data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# networkx for user connection visualization\n",
        "import networkx as nx\n",
        "# world maps\n",
        "import folium\n",
        "from folium import plugins\n",
        "from folium.plugins import HeatMap\n",
        "import branca.colormap as cm\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
        "# displaying the folium heatmap\n",
        "from IPython.display import display, HTML\n",
        "#for location standartization\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "# world cloud\n",
        "from wordcloud import WordCloud\n",
        "# Natural Language Processing (NLP)\n",
        "import nltk\n",
        "from emot.emo_unicode import UNICODE_EMOJI\n",
        "import emoji\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Embeddings\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "# Classification\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "%matplotlib inline\n",
        "## GLOBAL VARIABLES\n",
        "# stop words for tokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "# Initialize geolocator globally for efficient geocoding cache\n",
        "geolocator = Nominatim(user_agent=\"batch-geocoding\")\n",
        "# Enable tqdm for pandas, progress bar for long computations\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "py58BnTUdbsd",
        "outputId": "4cbc4c1c-91d9-43d1-be5c-5116ba99fc64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "py58BnTUdbsd",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Defining Supplemental Functions\n",
        "\n",
        "\n",
        "* ```def determine_data_dir()```\n",
        "* ```def download_and_extract_glove(data_dir)```\n",
        "* ```def display_categorical_vals(df)```\n",
        "* ```def has_special_chars(location) ```\n",
        "* ```def save_cache_to_json(cache, file_path=\"location_cache.json\")```\n",
        "* ```def load_cache_to_json(cache, file_path=\"location_cache.json\")```\n",
        "* ```def geocode_location(location)```\n",
        "* ```def batch_geocode(locations)```\n",
        "* ```def extract_word(location, position=\"first\")```\n",
        "* ```def split_geocoded_location(location)```\n",
        "* ```def get_coordinates(input_type, name, output_as='center', retries=3, delay=5)```\n",
        "* ```def add_coordinates_with_progress(df, city_col='city', state_col='state', country_col='country')```\n",
        "* ```def color(magnitude)```\n",
        "* ```def generateBaseMap(input_type, df, default_location=[40.693943, -73.985880], default_zoom_start=2)```\n",
        "* ```def extract_html_source(source_text)```\n",
        "* ```def replace_emoticons_with_emojis(text)```\n",
        "* ```def process_tweet_data(tweet, emoji_list=None)```\n",
        "* ``` def clean_tweets_with_progress_parallel(df, text_col='original_text', num_processes=6)```\n",
        "* ```def preprocess_text(df, text_column)```\n",
        "* ```def compute_ngrams(df, text_column, ngram_range=(2, 3), max_features=5000)```\n",
        "* ```def bert_embedding(sentence, tokenizer, model)```\n",
        "* ```def plot_normalized_confusion_matrix(y_true, y_pred, class_labels, title=\"Confusion Matrix\", cmap=\"Blues\")```\n",
        "\n",
        " **Optional**\n",
        "\n",
        "* ``` def fitness_function(selected_features, X, y, model, cv=3)```\n",
        "* ```def update_gwo(population, alpha, beta, delta,a)```\n",
        "* ```def hybrid_gwo_abc(X, y, model, n_wolves=10, n_iter=20)```\n",
        "\n"
      ],
      "metadata": {
        "id": "auiqucUetmLC"
      },
      "id": "auiqucUetmLC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplemental function to determine data directory\n",
        "# Input: none\n",
        "# Output: Data directory, depending on runtime environment.\n",
        "\n",
        "def determine_data_dir():\n",
        "    \"\"\"\n",
        "    Determines the data directory based on the execution environment:\n",
        "    - Local: Uses 'Data' directory in the current working directory.\n",
        "    - Cloud (e.g., Google Colab): Uses '/content' as the data directory.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the appropriate data directory.\n",
        "    \"\"\"\n",
        "    if 'COLAB_GPU' in os.environ:  # Check if running in Google Colab\n",
        "        data_dir = \"/content/drive/MyDrive/Colab_Notebooks/Data\"\n",
        "        print(f\"Running in Google Colab. Using data directory: {data_dir}\")\n",
        "    else:\n",
        "        data_dir = os.path.join(os.getcwd(), \"Data\")\n",
        "        print(f\"Running locally. Using data directory: {data_dir}\")\n",
        "\n",
        "        # Ensure the 'Data' directory exists locally\n",
        "        if not os.path.isdir(data_dir):\n",
        "            print(f\"The directory '{data_dir}' does not exist. Please create it and place the data files there.\")\n",
        "            raise FileNotFoundError(f\"'{data_dir}' directory is required for local execution.\")\n",
        "\n",
        "    return data_dir"
      ],
      "metadata": {
        "id": "dLmUhcBtfxFS"
      },
      "id": "dLmUhcBtfxFS",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_glove(data_dir):\n",
        "    \"\"\"\n",
        "    Downloads GloVe embeddings from Stanford, extracts them, and copies the specified file to data_dir.\n",
        "    \"\"\"\n",
        "    # Check if the file is already present in the data_dir, only download if it's not there\n",
        "    glove_file = Path(data_dir) / \"glove.twitter.27B.200d.txt\"\n",
        "    if not glove_file.is_file():\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        # Define GloVe URLs\n",
        "        glove_zip_url = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
        "        zip_file_path = os.path.join(data_dir, \"glove.twitter.27B.zip\")\n",
        "\n",
        "        # Download GloVe\n",
        "        os.system(f\"wget {glove_zip_url} -O {zip_file_path}\")\n",
        "\n",
        "        # Extract GloVe\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "\n",
        "        # Remove the downloaded ZIP file\n",
        "        os.remove(zip_file_path)\n",
        "\n",
        "        print(f\"GloVe embeddings downloaded and extracted to {data_dir}\")\n",
        "    else:\n",
        "        print(\"GloVe embeddings are already present in the data directory.\")"
      ],
      "metadata": {
        "id": "lpstvZMPRzd9"
      },
      "id": "lpstvZMPRzd9",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supplemental function will display unique values for all categorical columns in a dataframe.\n",
        "def display_categorical_vals(df):\n",
        "    # select categorical columns\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # print categorical columns and their unique values\n",
        "    for col in categorical_columns:\n",
        "        unique_values = df[col].unique()\n",
        "        print(f\"Column '{col}' has unique values: {unique_values}\")"
      ],
      "metadata": {
        "id": "jamXGPHa6A9n"
      },
      "execution_count": 5,
      "outputs": [],
      "id": "jamXGPHa6A9n"
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to check for special characters\n",
        "def has_special_chars(location):\n",
        "    return bool(re.search(r'[^\\w\\s,.-]', location))  # check for non-alphanumeric and non-space chars"
      ],
      "metadata": {
        "id": "713FrV2huiiK"
      },
      "id": "713FrV2huiiK",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_cache_to_json(cache, file_path=\"location_cache.json\"):\n",
        "    \"\"\"\n",
        "    Saves the location cache to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        cache (dict): The cache dictionary to save.\n",
        "        file_path (str): The file path where the cache will be saved.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        json.dump(cache, f)\n",
        "    print(f\"Cache saved to {file_path}\")"
      ],
      "metadata": {
        "id": "jWjMn4eshLyv"
      },
      "id": "jWjMn4eshLyv",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cache_from_json(file_path=\"location_cache.json\"):\n",
        "    \"\"\"\n",
        "    Loads the location cache from a JSON file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The file path from where the cache will be loaded.\n",
        "\n",
        "    Returns:\n",
        "        dict: The loaded cache dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "            cache = json.load(f)\n",
        "        print(f\"Cache loaded from {file_path}\")\n",
        "        return cache\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No cache file found at {file_path}. Starting with an empty cache.\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "Za01dz-NhMjF"
      },
      "id": "Za01dz-NhMjF",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to cache geocoding results\n",
        "try:\n",
        "    # Initialize cache from file\n",
        "    data_dir = determine_data_dir()\n",
        "    location_cache_file = os.path.join(data_dir, \"location_cache.json\")  # Path to cache file\n",
        "    location_cache = load_cache_from_json(location_cache_file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, initialize an empty cache\n",
        "    location_cache = {}\n",
        "\n",
        "# Supplemental function to use a geocoding API for location resolution\n",
        "def geocode_location(location):\n",
        "    \"\"\"\n",
        "    Resolve location using a geocoding API with caching.\n",
        "    Returns results in City, State, Country format.\n",
        "    \"\"\"\n",
        "    # Check cache first\n",
        "    if location in location_cache:\n",
        "        return location_cache[location]\n",
        "\n",
        "    try:\n",
        "        # Add a delay to respect API rate limits\n",
        "        geo = geolocator.geocode(location, addressdetails=True, exactly_one=True, timeout=10)\n",
        "        if geo:\n",
        "            # Default: Extract the address components\n",
        "            address = geo.raw.get('address', {})\n",
        "            city = address.get('city') or address.get('town') or address.get('village') or address.get('hamlet')\n",
        "            state = address.get('state')\n",
        "            country = address.get('country')\n",
        "\n",
        "            # Fallback: Parse city and country from display_name if missing\n",
        "            if not city:\n",
        "                try:\n",
        "                    city = geo.raw['display_name'].split(',')[0].strip()\n",
        "                except (KeyError, IndexError):\n",
        "                    city = \"Unknown\"\n",
        "            if not country:\n",
        "                try:\n",
        "                    country = geo.raw['display_name'].split(',')[-1].strip()\n",
        "                except (KeyError, IndexError):\n",
        "                    country = \"Unknown\"\n",
        "            # Fallback: Parse state dynamically from display_name if missing or ambiguous\n",
        "            if not state:\n",
        "                try:\n",
        "                    components = geo.raw['display_name'].split(',')\n",
        "                    components = [comp.strip() for comp in components]\n",
        "                    for i in range(len(components) - 1, -1, -1):  # Iterate backwards\n",
        "                        if 'County' not in components[i] and \"United States\" not in components[i]:\n",
        "                            state = components[i]\n",
        "                            break\n",
        "                except (KeyError, IndexError):\n",
        "                    state = \"Unknown\"\n",
        "\n",
        "            # Avoid redundancy: \"Country, Unknown, Country\"\n",
        "            if city == country:\n",
        "                city = \"Unknown\"\n",
        "            if state == country:\n",
        "                state = \"Unknown\"\n",
        "\n",
        "            # Construct the result in the desired format\n",
        "            result = f\"{city}, {state}, {country}\"\n",
        "        else:\n",
        "            result = \"Unknown\"\n",
        "    except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
        "        result = f\"Error: {e}\"\n",
        "\n",
        "    # Cache the result\n",
        "    location_cache[location] = result\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvEwHw9ut7F",
        "outputId": "b97f26c4-b1ef-41f4-8c96-7380bc49e38f"
      },
      "id": "rbvEwHw9ut7F",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/drive/MyDrive/Colab_Notebooks/Data\n",
            "Cache loaded from /content/drive/MyDrive/Colab_Notebooks/Data/location_cache.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# geocode multiple locations in parallel using ThreadPoolExecutor\n",
        "def batch_geocode(locations):\n",
        "    \"\"\"\n",
        "    Geocode multiple locations in parallel using ThreadPoolExecutor.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:  # max_workers can be adjusted as needed\n",
        "        # Use tqdm to wrap the executor's map method for progress tracking\n",
        "        for result in tqdm(executor.map(geocode_location, locations), \\\n",
        "                           total=len(locations), desc=\"Geocoding Progress\"):\n",
        "            results.append(result)\n",
        "    return results"
      ],
      "metadata": {
        "id": "dX7unUcQvWrg"
      },
      "id": "dX7unUcQvWrg",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract the first or last word from a location string\n",
        "def extract_word(location, position=\"first\"):\n",
        "    \"\"\"\n",
        "    Extract the first or last word from a location string.\n",
        "\n",
        "    Args:\n",
        "        location (str): The location string to process.\n",
        "        position (str): 'first' to extract the first word, 'last' to extract the last word.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted word or 'Unknown' if the location is empty or invalid.\n",
        "    \"\"\"\n",
        "    words = location.split()\n",
        "    if words:\n",
        "        return words[0] if position == \"first\" else words[-1]\n",
        "    return \"Unknown\""
      ],
      "metadata": {
        "id": "OojXElKzvuWp"
      },
      "id": "OojXElKzvuWp",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to split geocoded_location into City, State, and Country\n",
        "def split_geocoded_location(location):\n",
        "    if pd.notna(location):\n",
        "        parts = location.split(\",\")\n",
        "        parts = [p.strip() for p in parts]  # remove extra whitespace\n",
        "        city = parts[0] if len(parts) > 0 else \"Unknown\"\n",
        "        state = parts[1] if len(parts) > 1 else \"Unknown\"\n",
        "        country = parts[2] if len(parts) > 2 else \"Unknown\"\n",
        "        return city, state, country\n",
        "    return \"Unknown\", \"Unknown\", \"Unknown\""
      ],
      "metadata": {
        "id": "qCARRM5Mv5R4"
      },
      "id": "qCARRM5Mv5R4",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to cache coordinates\n",
        "try:\n",
        "    # Initialize cache from file\n",
        "    data_dir = determine_data_dir()\n",
        "    coordinate_cache_file = os.path.join(data_dir, \"coordinate_cache.json\")  # Path to cache file\n",
        "    coordinate_cache = load_cache_from_json(coordinate_cache_file)\n",
        "except FileNotFoundError:\n",
        "    # If the file doesn't exist, initialize an empty cache\n",
        "    coordinate_cache = {}\n",
        "\n",
        "# Update get_coordinates function to include caching\n",
        "def get_coordinates(input_type, name, output_as='center', retries=3, delay=5):\n",
        "    \"\"\"\n",
        "    Fetch coordinates of a city/state/country using Nominatim API with caching and retry logic.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): 'country', 'state', or 'city' to specify the type of input.\n",
        "        name (str): Name of the location.\n",
        "        output_as (str): 'center' or 'boundingbox' for coordinate type.\n",
        "        retries (int): Number of retry attempts.\n",
        "        delay (int): Delay between retries in seconds.\n",
        "\n",
        "    Returns:\n",
        "        list: [latitude, longitude]. Returns [0, 0] on failure.\n",
        "    \"\"\"\n",
        "    # Check the cache first\n",
        "    if name in coordinate_cache:\n",
        "        return coordinate_cache[name]\n",
        "\n",
        "    url = f\"http://nominatim.openstreetmap.org/search\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"ColabGeocoder/1.0 (leksea@gmail.com)\"\n",
        "    }\n",
        "    params = {\n",
        "        input_type: name,\n",
        "        \"format\": \"json\",\n",
        "        \"polygon\": 0\n",
        "    }\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if data:\n",
        "                if output_as == 'center':\n",
        "                    result = [float(data[0]['lat']), float(data[0]['lon'])]\n",
        "                elif output_as == 'boundingbox':\n",
        "                    result = [float(coord) for coord in data[0]['boundingbox']]\n",
        "                else:\n",
        "                    result = [0, 0]\n",
        "                # Cache the result\n",
        "                coordinate_cache[name] = result\n",
        "                return result\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching coordinates for {name}: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(f\"Retrying in {delay} seconds... ({attempt + 1}/{retries})\")\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                print(f\"Failed to fetch coordinates for {name} after {retries} attempts.\")\n",
        "                return [0, 0]\n",
        "\n",
        "    # Cache failed attempt as [0, 0] to avoid repeated retries\n",
        "    coordinate_cache[name] = [0, 0]\n",
        "    return [0, 0]"
      ],
      "metadata": {
        "id": "DfgwG_Z_xB7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131e0fd7-e21c-4767-fa43-6e8e65e69f85"
      },
      "id": "DfgwG_Z_xB7w",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/drive/MyDrive/Colab_Notebooks/Data\n",
            "Cache loaded from /content/drive/MyDrive/Colab_Notebooks/Data/coordinate_cache.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add_coordinates_with_progress function\n",
        "def add_coordinates_with_progress(df, city_col='city', state_col='state', country_col='country'):\n",
        "    \"\"\"\n",
        "    Add latitude and longitude coordinates to a DataFrame based on unique combinations\n",
        "    of City, State, and Country, only for rows where these are not 'Unknown'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        city_col (str): Column name for city.\n",
        "        state_col (str): Column name for state.\n",
        "        country_col (str): Column name for country.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The updated DataFrame with 'Latitude' and 'Longitude' columns.\n",
        "    \"\"\"\n",
        "    # Create a unique DataFrame of City, State, Country combinations\n",
        "    unique_locations = df[(df[city_col] != 'Unknown') &\n",
        "                          (df[state_col] != 'Unknown') &\n",
        "                          (df[country_col] != 'Unknown')][[city_col, state_col, country_col]].drop_duplicates()\n",
        "\n",
        "    # Define a helper function to fetch coordinates\n",
        "    def fetch_coords(row):\n",
        "        location_name = f\"{row[city_col]}, {row[state_col]}, {row[country_col]}\"\n",
        "        return get_coordinates('city', location_name)\n",
        "\n",
        "    # Add Latitude and Longitude columns to the unique locations\n",
        "    unique_locations[['latitude', 'longitude']] = unique_locations.progress_apply(fetch_coords, axis=1, result_type='expand')\n",
        "\n",
        "    # Create a mapping dictionary for efficient lookup\n",
        "    location_to_coords = unique_locations.set_index([city_col, state_col, country_col])[['latitude', 'longitude']].to_dict('index')\n",
        "\n",
        "    # Initialize Latitude and Longitude in the main DataFrame\n",
        "    df['latitude'], df['longitude'] = 0.0, 0.0\n",
        "\n",
        "    # Map coordinates back to the original DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        key = (row[city_col], row[state_col], row[country_col])\n",
        "        if key in location_to_coords:\n",
        "            df.at[index, 'latitude'] = location_to_coords[key]['latitude']\n",
        "            df.at[index, 'longitude'] = location_to_coords[key]['longitude']\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "7I6beD0fqZyw"
      },
      "id": "7I6beD0fqZyw",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine marker color based on tweet count\n",
        "def color(magnitude):\n",
        "    \"\"\"\n",
        "    Returns a color based on the magnitude using a hot-to-cool color map.\n",
        "    \"\"\"\n",
        "    if magnitude >= 2000:\n",
        "        return 'red'  # Hot color for high magnitude\n",
        "    elif 500 <= magnitude < 2000:\n",
        "        return 'orange'  # Medium-hot color\n",
        "    elif 100 <= magnitude < 500:\n",
        "        return 'yellow'  # Neutral color\n",
        "    elif 50 <= magnitude < 100:\n",
        "        return 'lightblue'  # Medium-cool color\n",
        "    else:\n",
        "        return 'blue'  # Cool color for low magnitude\n",
        "\n",
        "# Function to generate the heatmap\n",
        "def generateBaseMap(input_type, df, default_location=[37.774929, -122.419416], default_zoom_start=2):\n",
        "    \"\"\"\n",
        "    Function to generate a heatmap with markers for tweet distribution.\n",
        "\n",
        "    Args:\n",
        "        input_type (str): 'country' or 'city' to specify the type of heatmap.\n",
        "        df (pd.DataFrame): DataFrame containing latitude, longitude, tweet count, and name.\n",
        "        default_location (list): Default map center location as [latitude, longitude].\n",
        "        default_zoom_start (int): Default zoom level for the map.\n",
        "\n",
        "    Returns:\n",
        "        folium.Map: A folium map object with heatmap and markers.\n",
        "    \"\"\"\n",
        "    # Initialize the base map\n",
        "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
        "    marker_cluster = plugins.MarkerCluster().add_to(base_map)\n",
        "\n",
        "    # Add the heatmap\n",
        "    HeatMap(data=df[['latitude', 'longitude']].values.tolist(), radius=20, max_zoom=13).add_to(base_map)\n",
        "\n",
        "    # Add markers with popups\n",
        "    for lat, lon, tweet_count, name in zip(df['latitude'], df['longitude'], df['tweet_count'], df.iloc[:, 0]):\n",
        "        popup_content = folium.Popup(f\"{name}<br>{tweet_count} tweets\", max_width=300)\n",
        "        folium.Marker(\n",
        "            location=[lat, lon],\n",
        "            popup=popup_content,\n",
        "            icon=folium.Icon(color=color(tweet_count), icon='twitter', prefix='fa')\n",
        "        ).add_to(marker_cluster)\n",
        "\n",
        "    # Add a colormap legend\n",
        "    min_val, max_val = df['tweet_count'].min(), df['tweet_count'].max()\n",
        "    colormap = cm.LinearColormap(colors=['blue', 'yellow', 'red'], vmin=min_val, vmax=max_val)\n",
        "    colormap.caption = f\"{input_type.title()} Distribution of COVID-19 Tweets\"\n",
        "    colormap.add_to(base_map)\n",
        "\n",
        "    return base_map"
      ],
      "metadata": {
        "id": "KS8rd-tnyU53"
      },
      "id": "KS8rd-tnyU53",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract readable source from HTML content\n",
        "def extract_html_source(source_text):\n",
        "    \"\"\"\n",
        "    Extracts the readable text (e.g., 'Twitter for Android') from the source HTML string.\n",
        "\n",
        "    Args:\n",
        "        source_text (str): The raw HTML string in the source column.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned, readable source text.\n",
        "    \"\"\"\n",
        "    return re.sub(r'<.*?>', '', str(source_text)).strip()  # remove HTML tags"
      ],
      "metadata": {
        "id": "wGVwTKwrwBN8"
      },
      "id": "wGVwTKwrwBN8",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a dictionary mapping emoticons to emojis\n",
        "emoticon_to_emoji = {\n",
        "    \":)\": \"😊\",\n",
        "    \":D\": \"😃\",\n",
        "    \":(\": \"☹️\",\n",
        "    \":/\": \"😕\",\n",
        "    \":P\": \"😛\",\n",
        "    \";)\": \"😉\",\n",
        "    \":'(\": \"😢\",\n",
        "    \":o\": \"😮\",\n",
        "    \":|\": \"😐\",\n",
        "    \":))\": \"😂\",\n",
        "    \":*\": \"😘\",\n",
        "    \"xD\": \"😆\"\n",
        "}\n",
        "\n",
        "def replace_emoticons_with_emojis(text):\n",
        "    \"\"\"\n",
        "    Replaces emoticons in the text with corresponding emojis.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with emoticons replaced by emojis.\n",
        "    \"\"\"\n",
        "    # Use regex to find and replace emoticons\n",
        "    for emoticon, emoji in emoticon_to_emoji.items():\n",
        "        text = re.sub(re.escape(emoticon), emoji, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "clWtFweFUJir"
      },
      "id": "clWtFweFUJir",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# big cleaning function\n",
        "def process_tweet_data(tweet, emoji_list=None):\n",
        "    \"\"\"\n",
        "    Processes a tweet to extract mentions, hashtags, retweets, emojis, hyperlinks, and cleaned text.\n",
        "\n",
        "    Args:\n",
        "        tweet (str): The raw tweet text.\n",
        "        emoji_list (list): List of emojis to extract. Defaults to keys of UNICODE_EMOJI.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with extracted components and cleaned text.\n",
        "    \"\"\"\n",
        "    # ensure input is a string\n",
        "    tweet = str(tweet)\n",
        "    # default emoji list if not provided\n",
        "    if emoji_list is None:\n",
        "        emoji_list = list(UNICODE_EMOJI.keys())\n",
        "\n",
        "    # extract mentions\n",
        "    mentions = re.findall(r'@\\w+', tweet)\n",
        "\n",
        "    # extract hashtags\n",
        "    hashtags = re.findall(r'#\\w+', tweet)\n",
        "\n",
        "    # check for retweets and extract username after RT\n",
        "    retweets = re.findall(r'^RT @(\\w+)', tweet)\n",
        "    retweet_user = retweets[0] if retweets else None\n",
        "\n",
        "    # extract hyperlinks before emoji\n",
        "    hyperlinks = re.findall(r'https?://[^\\s]+|www\\.[^\\s]+', tweet)\n",
        "    tweet = re.sub(r'https?://[^\\s]+|www\\.[^\\s]+', '', tweet) #Remove URL\n",
        "\n",
        "    # extract emojis\n",
        "    tweet = replace_emoticons_with_emojis(tweet)\n",
        "    emojis = ''.join([char for char in tweet if char in emoji_list])\n",
        "\n",
        "    # replace emojis with text\n",
        "    tweet = emoji.demojize(tweet).replace('_', ' ')\n",
        "\n",
        "    # remove mentions, hashtags, retweets, emojis, and hyperlinks from the tweet\n",
        "    cleaned_text = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
        "    cleaned_text = re.sub(r'#\\w+', '', cleaned_text)  # Remove hashtags\n",
        "    cleaned_text = re.sub(r'^RT', '', cleaned_text)   # Remove retweets\n",
        "    cleaned_text = ''.join([char for char in cleaned_text if char not in emoji_list])  # Remove emojis\n",
        "\n",
        "    # remove special characters, extra spaces, numbers\n",
        "    cleaned_text = re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    cleaned_text = re.sub(r'\\d+', '', cleaned_text)\n",
        "\n",
        "    #lowercase the text, remove numbers\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return {\n",
        "        'mentions': mentions,\n",
        "        'hashtags': hashtags,\n",
        "        'retweets': retweets,\n",
        "        'emojis': emojis,\n",
        "        'hyperlinks': hyperlinks,\n",
        "        'cleaned_text': cleaned_text\n",
        "    }"
      ],
      "metadata": {
        "id": "RvxDCHurw1Lg"
      },
      "id": "RvxDCHurw1Lg",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_cleaned_text(tweet):\n",
        "    \"\"\"Helper function to extract 'cleaned_text' from process_tweet_data.\"\"\"\n",
        "    return process_tweet_data(tweet)['cleaned_text']\n",
        "\n",
        "def clean_tweets_with_progress_parallel(df, text_col='original_text', num_processes=6):\n",
        "    \"\"\"\n",
        "    Cleans tweet data in parallel using ProcessPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing tweets.\n",
        "        text_col (str): Column name containing the tweet text.\n",
        "        num_processes (int): Number of processes to use for parallel execution.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with new columns for cleaned text and extracted components.\n",
        "    \"\"\"\n",
        "    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n",
        "         # Extract only the 'cleaned_text' from process_tweet_data using the helper function\n",
        "        cleaned_texts = list(tqdm(executor.map(extract_cleaned_text, df[text_col]),\n",
        "                                   total=len(df), desc=\"Cleaning Tweets\"))\n",
        "\n",
        "    # Return as a pandas Series\n",
        "    return pd.Series(cleaned_texts, index=df.index, name='cleaned_text')"
      ],
      "metadata": {
        "id": "novHso6vyxEy"
      },
      "id": "novHso6vyxEy",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words and lemmatizer initialization\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(df, text_column):\n",
        "    \"\"\"\n",
        "    Preprocesses text data by tokenizing, removing stop words, and lemmatizing.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the text column.\n",
        "        text_column (str): Name of the column with raw text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with processed text columns.\n",
        "    \"\"\"\n",
        "    # Convert 'text_column' to string type to ensure it contains only string values\n",
        "    df[text_column] = df[text_column].astype(str)\n",
        "    # Tokenization\n",
        "    df['tokenized_text'] = df[text_column].progress_apply(word_tokenize)\n",
        "\n",
        "    # Stop word removal\n",
        "    df['filtered_tokens'] = df['tokenized_text'].progress_apply(\n",
        "        lambda tokens: [word for word in tokens if word.lower() not in stop_words]\n",
        "    )\n",
        "\n",
        "    # Lemmatization\n",
        "    df['lemmatized_tokens'] = df['filtered_tokens'].progress_apply(\n",
        "        lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    )\n",
        "\n",
        "    # Combine tokens back into text for n-gram computation\n",
        "    df['lemmatized_text'] = df['lemmatized_tokens'].progress_apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "E0GoMgylvvTK"
      },
      "id": "E0GoMgylvvTK",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ngrams(df, text_column, ngram_range=(2, 3), max_features=5000):\n",
        "    \"\"\"\n",
        "    Computes n-grams from preprocessed text data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame containing the processed text column.\n",
        "        text_column (str): Name of the column with preprocessed text.\n",
        "        ngram_range (tuple): Range of n-grams to compute (e.g., (2, 3) for bi-grams and tri-grams).\n",
        "        max_features (int): Maximum number of n-gram features.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Sparse matrix of n-grams and fitted CountVectorizer.\n",
        "    \"\"\"\n",
        "    # Compute n-grams using CountVectorizer\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
        "    ngram_matrix = vectorizer.fit_transform(df[text_column])\n",
        "\n",
        "    return ngram_matrix, vectorizer"
      ],
      "metadata": {
        "id": "vVqQglx_uZJA"
      },
      "id": "vVqQglx_uZJA",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute BERT embeddings for a single text\n",
        "def bert_embedding(text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Compute BERT embeddings for a single text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text.\n",
        "        tokenizer (BertTokenizer): Tokenizer for BERT.\n",
        "        model (BertModel): Pre-trained BERT model.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Sentence embedding (average of token embeddings).\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "# Parallelized function to compute BERT embeddings\n",
        "def compute_bert_embeddings_parallel(df, text_column, max_workers=4):\n",
        "    \"\"\"\n",
        "    Compute BERT embeddings for all texts in a DataFrame using ThreadPoolExecutor.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the text column.\n",
        "        text_column (str): Name of the column with text to embed.\n",
        "        max_workers (int): Maximum number of threads.\n",
        "\n",
        "    Returns:\n",
        "        list: List of BERT embeddings.\n",
        "    \"\"\"\n",
        "    texts = df[text_column].tolist()\n",
        "    embeddings = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit tasks to the executor\n",
        "        futures = {executor.submit(bert_embedding, text, tokenizer, model): text for text in texts}\n",
        "\n",
        "        # Use tqdm for progress bar\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing BERT embeddings\"):\n",
        "            try:\n",
        "                embeddings.append(future.result())\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing text: {e}\")\n",
        "                embeddings.append(None)  # Append None for failed embeddings\n",
        "\n",
        "    return torch.tensor([emb for emb in embeddings if emb is not None])"
      ],
      "metadata": {
        "id": "q_yhHxcjtGg6"
      },
      "id": "q_yhHxcjtGg6",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(file_path):\n",
        "    \"\"\"\n",
        "    Load GloVe embeddings into a dictionary.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the GloVe file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping words to their vector embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "7gl069T-ASgi"
      },
      "id": "7gl069T-ASgi",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, embeddings_dict, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Retrieve the embedding for a given word or a zero vector if the word is not in the embeddings.\n",
        "\n",
        "    Args:\n",
        "        word (str): Input word.\n",
        "        embeddings_dict (dict): Dictionary of pre-trained embeddings.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Embedding vector for the word.\n",
        "    \"\"\"\n",
        "    return embeddings_dict.get(word, np.zeros(embedding_dim))"
      ],
      "metadata": {
        "id": "J3ddMTmSAlUE"
      },
      "id": "J3ddMTmSAlUE",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sentence_embedding(sentence, embeddings_dict, embedding_dim=300):\n",
        "    \"\"\"\n",
        "    Compute sentence embedding by averaging word embeddings.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): Input sentence.\n",
        "        embeddings_dict (dict): Dictionary of pre-trained embeddings.\n",
        "        embedding_dim (int): Dimension of the embeddings.\n",
        "\n",
        "    Returns:\n",
        "        np.array: Sentence embedding.\n",
        "    \"\"\"\n",
        "    tokens = sentence.split()  # Tokenize the sentence\n",
        "    token_embeddings = [get_embedding(token, embeddings_dict, embedding_dim) for token in tokens]\n",
        "    return np.mean(token_embeddings, axis=0) if token_embeddings else np.zeros(embedding_dim)"
      ],
      "metadata": {
        "id": "5D10sbgTA1AJ"
      },
      "id": "5D10sbgTA1AJ",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting confusion matrix\n",
        "def plot_normalized_confusion_matrix(y_true, y_pred, class_labels, title=\"Confusion Matrix\", cmap=\"Blues\"):\n",
        "    \"\"\"\n",
        "    Plots a normalized confusion matrix (percentages).\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): Ground truth labels.\n",
        "        y_pred (array-like): Predicted labels.\n",
        "        class_labels (list): List of class label names.\n",
        "        title (str): Title of the confusion matrix plot.\n",
        "        cmap (str): Colormap for the heatmap.\n",
        "    \"\"\"\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
        "\n",
        "    # Normalize the confusion matrix by row (true class)\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "    # Plot the normalized confusion matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_labels)\n",
        "    disp.plot(cmap=cmap, ax=ax, colorbar=True)\n",
        "\n",
        "    plt.title(f'{title} (in %)')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3dpH8uXq9e48"
      },
      "id": "3dpH8uXq9e48",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_pipeline(pipeline, X_train, y_train, X_val, y_val, X_test, y_test, label_encoder):\n",
        "    \"\"\"\n",
        "    Trains a pipeline, evaluates it on validation and test sets, and prints performance metrics.\n",
        "\n",
        "    Args:\n",
        "        pipeline (Pipeline): A scikit-learn pipeline containing the model and preprocessing steps.\n",
        "        X_train (array-like): Training feature set.\n",
        "        y_train (array-like): Training labels.\n",
        "        X_val (array-like): Validation feature set.\n",
        "        y_val (array-like): Validation labels.\n",
        "        X_test (array-like): Test feature set.\n",
        "        y_test (array-like): Test labels.\n",
        "        label_encoder (LabelEncoder): Fitted label encoder for decoding labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing validation accuracy, test accuracy, and the test classification report.\n",
        "    \"\"\"\n",
        "    # Train the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    y_val_pred_encoded = pipeline.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred_encoded)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_test_pred_encoded = pipeline.predict(X_test)\n",
        "    y_test_pred = label_encoder.inverse_transform(y_test_pred_encoded)  # Decode predictions\n",
        "    y_test_decoded = label_encoder.inverse_transform(y_test)  # Decode actual labels\n",
        "\n",
        "    # Compute test accuracy\n",
        "    test_accuracy = accuracy_score(y_test_decoded, y_test_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "    print(\"Test Classification Report:\")\n",
        "    test_report = classification_report(y_test_decoded, y_test_pred, target_names=label_encoder.classes_)\n",
        "    print(test_report)\n",
        "\n",
        "    plot_normalized_confusion_matrix( y_true=y_test_decoded, y_pred=y_test_pred, \\\n",
        "                                     class_labels=label_encoder.classes_, \\\n",
        "                                      title=\"Confusion Matrix on Test Set\")\n",
        "    return {\n",
        "        'validation_accuracy': val_accuracy,\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'test_report': test_report\n",
        "    }, pipeline"
      ],
      "metadata": {
        "id": "oVoZpLJR99Gb"
      },
      "id": "oVoZpLJR99Gb",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness_function(selected_features, X, y, model, cv=3):\n",
        "    \"\"\"\n",
        "    Fitness function to evaluate feature subset.\n",
        "\n",
        "    Args:\n",
        "        selected_features (list): Binary list representing selected features.\n",
        "        X (array-like): Feature matrix.\n",
        "        y (array-like): Target labels.\n",
        "        model (sklearn model): Classifier to evaluate.\n",
        "        cv (int): Number of cross-validation folds.\n",
        "\n",
        "    Returns:\n",
        "        float: Negative cross-validation accuracy (to minimize).\n",
        "    \"\"\"\n",
        "    # Mask selected features\n",
        "    X_selected = X[:, selected_features == 1]\n",
        "\n",
        "    if X_selected.shape[1] == 0:  # Avoid empty feature set\n",
        "        return 1.0  # High error for invalid subsets\n",
        "\n",
        "    # Perform cross-validation and return negative accuracy\n",
        "    score = cross_val_score(model, X_selected, y, cv=cv, scoring='accuracy').mean()\n",
        "    return -score  # Negate because optimization minimizes"
      ],
      "metadata": {
        "id": "Cwk8177mDDDw"
      },
      "id": "Cwk8177mDDDw",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GWO population\n",
        "def initialize_population(n_wolves, n_features):\n",
        "    return np.random.randint(2, size=(n_wolves, n_features))\n",
        "\n",
        "# Update wolves with GWO\n",
        "def update_gwo(population, alpha, beta, delta, a):\n",
        "    n_wolves, n_features = population.shape\n",
        "    new_population = np.copy(population)\n",
        "\n",
        "    for i in range(n_wolves):\n",
        "        for j in range(n_features):\n",
        "            r1, r2 = rand(), rand()\n",
        "            A = 2 * a * r1 - a\n",
        "            C = 2 * r2\n",
        "            D_alpha = abs(C * alpha[j] - population[i, j])\n",
        "            X1 = alpha[j] - A * D_alpha\n",
        "\n",
        "            D_beta = abs(C * beta[j] - population[i, j])\n",
        "            X2 = beta[j] - A * D_beta\n",
        "\n",
        "            D_delta = abs(C * delta[j] - population[i, j])\n",
        "            X3 = delta[j] - A * D_delta\n",
        "\n",
        "            # Update position\n",
        "            new_population[i, j] = (X1 + X2 + X3) / 3\n",
        "\n",
        "    return (new_population > 0.5).astype(int)\n",
        "\n",
        "# Hybrid optimization (GWO + ABC)\n",
        "def hybrid_gwo_abc(X, y, model, n_wolves=10, n_iter=20):\n",
        "    n_features = X.shape[1]\n",
        "    population = initialize_population(n_wolves, n_features)\n",
        "\n",
        "    alpha, beta, delta = None, None, None\n",
        "    a = 2  # Linear reduction coefficient\n",
        "\n",
        "    for t in range(n_iter):\n",
        "        fitness = np.array([fitness_function(wolf, X, y, model) for wolf in population])\n",
        "\n",
        "        # Sort wolves by fitness\n",
        "        sorted_indices = np.argsort(fitness)\n",
        "        population = population[sorted_indices]\n",
        "\n",
        "        # Update alpha, beta, delta\n",
        "        alpha, beta, delta = population[0], population[1], population[2]\n",
        "\n",
        "        # Update positions using GWO\n",
        "        population = update_gwo(population, alpha, beta, delta, a)\n",
        "\n",
        "        # Apply local ABC exploitation to the top wolves (e.g., top 3)\n",
        "        for i in range(3):\n",
        "            local_search_wolf = np.copy(population[i])\n",
        "            for _ in range(randint(1, 5)):  # Random local updates\n",
        "                feature_idx = randint(0, n_features)\n",
        "                local_search_wolf[feature_idx] = 1 - local_search_wolf[feature_idx]  # Flip feature\n",
        "            # Accept if fitness improves\n",
        "            if fitness_function(local_search_wolf, X, y, model) < fitness[i]:\n",
        "                population[i] = local_search_wolf\n",
        "\n",
        "        # Reduce exploration coefficient\n",
        "        a -= 2 / n_iter\n",
        "\n",
        "    # Return the best solution\n",
        "    best_wolf = population[0]\n",
        "    return best_wolf"
      ],
      "metadata": {
        "id": "KbMbUyOFDHWx"
      },
      "id": "KbMbUyOFDHWx",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Data Loading\n",
        "\n",
        "This section focuses on loading the COVID-19 Twitter dataset into a Pandas DataFrame for analysis. We will perform the following steps:\n",
        "\n",
        "1. **Data Directory Determination:** Identify the appropriate directory where the data files are stored, considering both local and cloud (Colab) environments.\n",
        "\n",
        "2. **File Identification:** Locate all CSV files within the determined data directory using the `glob` library.\n",
        "\n",
        "3. **Data Loading and Concatenation:**\n",
        "    - Read each CSV file into a separate Pandas DataFrame using `pd.read_csv`.\n",
        "    - Concatenate all the individual DataFrames into a single DataFrame named `data` using `pd.concat`.\n",
        "    - Print information about the loaded data, including its dimensions and a preview of the first few rows.\n",
        "\n",
        "4. **Error Handling:** Implement error handling mechanisms to address potential issues during file loading, such as missing files or incorrect file formats."
      ],
      "metadata": {
        "id": "hnoqoMM-gr0F"
      },
      "id": "hnoqoMM-gr0F"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8Ir_LkSJww",
        "outputId": "b7e5d069-e68f-43a2-d78a-cae8b06d0843"
      },
      "id": "xm8Ir_LkSJww",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the files\n",
        "# determine the data directory\n",
        "# data_dir would be global variable\n",
        "data_dir = determine_data_dir()\n",
        "\n",
        "# locate all CSV files in the determined directory\n",
        "files_pattern = os.path.join(data_dir, \"*.csv\")\n",
        "files = glob.glob(files_pattern)\n",
        "\n",
        "# check if files are found\n",
        "if not files:\n",
        "    print(f\"No CSV files found in directory: {data_dir}\")\n",
        "else:\n",
        "     # load and inspect each file\n",
        "    dfs = []  # to store valid DataFrames\n",
        "    for file in files:\n",
        "        try:\n",
        "            # load the DataFrame\n",
        "            df = pd.read_csv(file)\n",
        "            rows, cols = df.shape\n",
        "            print(f\"File: {file} | Rows: {rows}, Columns: {cols}\")\n",
        "\n",
        "            # skip empty files or files with no columns\n",
        "            if rows == 0 or cols == 0:\n",
        "                print(f\"Skipping empty or invalid file: {file}\")\n",
        "                continue\n",
        "\n",
        "            # append to list if valid\n",
        "            dfs.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file}: {e}\")\n",
        "\n",
        "    # concatenate all valid DataFrames\n",
        "    if dfs:\n",
        "        data = pd.concat(dfs, ignore_index=True)\n",
        "        print(f\"Data loaded successfully with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
        "        print(data.head())\n",
        "    else:\n",
        "        print(\"No valid DataFrames to concatenate.\")"
      ],
      "metadata": {
        "id": "Gn7kjSy5DgOe",
        "outputId": "cdb11c35-1441-49c0-8c17-ed781b71c2ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Gn7kjSy5DgOe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab. Using data directory: /content/drive/MyDrive/Colab_Notebooks/Data\n",
            "File: /content/drive/MyDrive/Colab_Notebooks/Data/Covid-19 Twitter Dataset (Aug-Sep 2020).csv | Rows: 120509, Columns: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading GloVe embeddings (large file, might take a few minutes)\n",
        "download_and_extract_glove(data_dir)"
      ],
      "metadata": {
        "id": "S4iThCR6amwh"
      },
      "id": "S4iThCR6amwh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Basic Data Understanding\n",
        "\n",
        "This section focuses on gaining an initial understanding of the dataset's structure and contents.\n",
        "We will perform the following steps:\n",
        "1. **Data Overview:** Examine the basic information about the dataset, including the number of rows, columns, and data types.\n",
        "2. **Column Selection:** Identify and select the relevant columns for the analysis.\n"
      ],
      "metadata": {
        "id": "75eceDA73wLf"
      },
      "id": "75eceDA73wLf"
    },
    {
      "cell_type": "code",
      "source": [
        "# get general info about the dataset\n",
        "data.info()"
      ],
      "metadata": {
        "id": "tsrDMCYiEfUL"
      },
      "id": "tsrDMCYiEfUL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# info about the numeric columns\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "l8cIT_Ib3_UI"
      },
      "id": "l8cIT_Ib3_UI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select subset of tweets we'll be working with:\n",
        "cols_to_keep = ['id', 'source', 'created_at', 'original_text', \\\n",
        "                'lang', 'favorite_count', 'retweet_count', 'original_author', \\\n",
        "                'hashtags', 'user_mentions', 'place', 'sentiment', 'compound', 'pos', 'neu', 'neg']\n",
        "tweets_df = data[cols_to_keep].copy()\n",
        "# drop NaN ids\n",
        "tweets_df.dropna(subset=['id'], inplace=True)"
      ],
      "metadata": {
        "id": "p7qKCkF41B4j"
      },
      "id": "p7qKCkF41B4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_categorical_vals(tweets_df)"
      ],
      "metadata": {
        "id": "Lgl3O8AV0loS"
      },
      "id": "Lgl3O8AV0loS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Data Cleaning: Date"
      ],
      "metadata": {
        "id": "psy8TJ7r6Lit"
      },
      "id": "psy8TJ7r6Lit"
    },
    {
      "cell_type": "code",
      "source": [
        "#rename date column for clarity and convert to date\n",
        "tweets_df.rename(columns={'created_at': 'date'}, inplace=True)\n",
        "tweets_df['date'] = pd.to_datetime(tweets_df['date'])"
      ],
      "metadata": {
        "id": "bSzfHyP60pCc"
      },
      "id": "bSzfHyP60pCc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow by exploratory data analysis:\n",
        "* What were daily tweet patters?\n",
        "* What were the top 20 days with most tweets?  "
      ],
      "metadata": {
        "id": "5wNyc5aZ8KYM"
      },
      "id": "5wNyc5aZ8KYM"
    },
    {
      "cell_type": "code",
      "source": [
        "# exploratory analysis: plot number of tweets per day\n",
        "# group by date and count tweets\n",
        "tweets_per_day = tweets_df.groupby(tweets_df['date'].dt.date)['id'].count()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(tweets_per_day.index, tweets_per_day.values)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Tweets\")\n",
        "plt.title(\"Number of Tweets Per Day\")\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KWByJUQb8HnZ"
      },
      "id": "KWByJUQb8HnZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exploratory analysis: list the top 20 days with the most tweets\n",
        "# rename columns for clarity\n",
        "tweets_per_day.columns = ['date', 'tweet_count']\n",
        "\n",
        "# sort by tweet count and get the top 20\n",
        "top_20_days = tweets_per_day.sort_values(ascending=False).head(20)\n",
        "\n",
        "# display the result\n",
        "print(top_20_days)"
      ],
      "metadata": {
        "id": "tKhB8-_k6oVd"
      },
      "id": "tKhB8-_k6oVd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Data Cleaning: Language\n",
        "Before coming up with the strategy for each column, we'll check the contents of categorical data and the distributiuon of NaNs.\n",
        "\n",
        "* It would make sence that fields like ```hashtags``` and ```user_mentions``` would have missing values and we'll leave it as it is.\n",
        "* We'll check the ```lang``` and ```place``` columns.\n"
      ],
      "metadata": {
        "id": "Br52XIjx-PYG"
      },
      "id": "Br52XIjx-PYG"
    },
    {
      "cell_type": "code",
      "source": [
        "# number of NaNs in lang\n",
        "sum(tweets_df.lang.isna())"
      ],
      "metadata": {
        "id": "ZPUde-jG8BO2"
      },
      "id": "ZPUde-jG8BO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the tweet text\n",
        "tweets_df[tweets_df.lang.isna()]"
      ],
      "metadata": {
        "id": "NjCQDN2y-0Ww"
      },
      "id": "NjCQDN2y-0Ww",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only 3 rows where language is missing are missing the original text, so we'll discard them."
      ],
      "metadata": {
        "id": "peqUklAw_XG3"
      },
      "id": "peqUklAw_XG3"
    },
    {
      "cell_type": "code",
      "source": [
        "# drop rows where 'lang' is NaN\n",
        "tweets_df = tweets_df.dropna(subset=['lang'])\n",
        "\n",
        "# verify the changes\n",
        "print(f\"Number of NaNs in 'lang' after dropping: {sum(tweets_df.lang.isna())}\")\n",
        "\n",
        "# drop the lang column from the df\n",
        "tweets_df = tweets_df.drop(columns=['lang'])"
      ],
      "metadata": {
        "id": "UVXP7Mdl-_JG"
      },
      "id": "UVXP7Mdl-_JG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df"
      ],
      "metadata": {
        "id": "BSX8Xlp4QYRL"
      },
      "id": "BSX8Xlp4QYRL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rename place column to location for clarity\n",
        "tweets_df.rename(columns={'place': 'location'}, inplace=True)"
      ],
      "metadata": {
        "id": "VDUco94l_msI"
      },
      "id": "VDUco94l_msI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count of NaNs\n",
        "sum(tweets_df.location.isna())"
      ],
      "metadata": {
        "id": "IRZGktsdBCU8"
      },
      "id": "IRZGktsdBCU8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill location NaNs with Unknown\n",
        "tweets_df.fillna(value={'location':'Unknown'}, inplace=True)"
      ],
      "metadata": {
        "id": "DElV9lreBbuR"
      },
      "id": "DElV9lreBbuR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.9 Data Cleaning: Location\n",
        "\n",
        "Here we'll see if location can be cleaned up for further visualization. We'll start with checking the locations with at least 10 tweets.  "
      ],
      "metadata": {
        "id": "QltD9yKyKM2i"
      },
      "id": "QltD9yKyKM2i"
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate value counts for the 'location' column\n",
        "location_counts = tweets_df.location.value_counts()\n",
        "\n",
        "# filter to keep locations with at least 20 occurrences\n",
        "filtered_locations = location_counts[location_counts >= 10]\n",
        "\n",
        "# fisplay the filtered results\n",
        "print(filtered_locations)"
      ],
      "metadata": {
        "id": "v7mKo2s6BNWZ"
      },
      "id": "v7mKo2s6BNWZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll check the locations with the special characters (non-alphanumeric that are not the ```-,.```)."
      ],
      "metadata": {
        "id": "yJbEJxdeTk6I"
      },
      "id": "yJbEJxdeTk6I"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to create a boolean mask\n",
        "special_chars_mask = tweets_df['location'].apply(has_special_chars)\n",
        "\n",
        "# filter the DataFrame and get value counts\n",
        "locations_with_special_chars = tweets_df.loc[special_chars_mask, \\\n",
        "                                             'location'].value_counts()\n",
        "\n",
        "# display the result\n",
        "print(locations_with_special_chars)"
      ],
      "metadata": {
        "id": "q7Kf4O_JSdrL"
      },
      "id": "q7Kf4O_JSdrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll try to salvage the locations by using the first or the last word in a multi-word location. Otherwise, set it to the 'Unknown'."
      ],
      "metadata": {
        "id": "nXn706c4VKH0"
      },
      "id": "nXn706c4VKH0"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a boolean mask for one-word locations, those we won't be able to automatically ID\n",
        "one_word_mask = locations_with_special_chars.index.str.split().str.len() == 1\n",
        "one_word_locations = locations_with_special_chars[one_word_mask].index\n",
        "\n",
        "# replace those locations in the original DataFrame with 'Unknown'\n",
        "tweets_df['location'] = tweets_df['location'].replace(one_word_locations, 'Unknown')"
      ],
      "metadata": {
        "id": "NzY9yF_YUIKv"
      },
      "id": "NzY9yF_YUIKv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The brief explanation for ```geocode_location()``` function:\n",
        "* It utilizes the geocoding API. We give it a location (string), and it returns a best(first) guess for the location. For example, if I give it ```Paris```, it will return ```Paris, France``` and not ```Paris, Texas```.\n",
        "* It executes slowly."
      ],
      "metadata": {
        "id": "Xd9iepNmWjnz"
      },
      "id": "Xd9iepNmWjnz"
    },
    {
      "cell_type": "code",
      "source": [
        "# test it on few entries\n",
        "print(geocode_location('New York, NY'))             # Expected: \"New York, New York, United States\"\n",
        "print(geocode_location('Toronto, Ontario'))         # Expected: \"Toronto, Ontario, Canada\"\n",
        "print(geocode_location('India'))                    # Expected: \"Unknown, Unknown, India\"\n",
        "print(geocode_location('USA'))                      # Expected: \"Unknown, Unknown, United States\""
      ],
      "metadata": {
        "id": "J7zg7xziM9yi"
      },
      "id": "J7zg7xziM9yi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we implemented parallel optimization in ```batch_geocode()```."
      ],
      "metadata": {
        "id": "ygHbOge8X0EU"
      },
      "id": "ygHbOge8X0EU"
    },
    {
      "cell_type": "code",
      "source": [
        "# before applying, do minor cleaning: replace 'unknown' with 'Unknown'\n",
        "tweets_df.location.replace('unknown', 'Unknown')"
      ],
      "metadata": {
        "id": "TIdeQFu-PxRd"
      },
      "id": "TIdeQFu-PxRd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate value counts for the 'location' column\n",
        "location_counts = tweets_df.location.value_counts()\n",
        "\n",
        "# filter to keep locations with at least 10 occurrences\n",
        "filtered_locations = location_counts[location_counts >= 10]\n",
        "\n",
        "# remove 'Unknown' from filtered_locations\n",
        "filtered_locations_known = filtered_locations[filtered_locations.index != 'Unknown']\n",
        "\n",
        "# create a boolean mask for locations in filtered_locations_known\n",
        "mask = tweets_df['location'].isin(filtered_locations_known.index)\n",
        "\n",
        "# get unique locations from the filtered DataFrame\n",
        "unique_locations = tweets_df.loc[mask, 'location'].unique()"
      ],
      "metadata": {
        "id": "bNgR_KIOWndU"
      },
      "id": "bNgR_KIOWndU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code will apply batch geocoding requests. To respect the Nominatim API, we put a sleep(0.5) before each request and longer timeout (8s).\n",
        "The code would run ~40 minutes."
      ],
      "metadata": {
        "id": "j6l1lw7GfENf"
      },
      "id": "j6l1lw7GfENf"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply geocode_location only to the selected locations in parallel\n",
        "geocoded_results = dict(zip(unique_locations, batch_geocode(unique_locations)))\n",
        "\n",
        "# map the results back to the DataFrame\n",
        "tweets_df.loc[mask, 'geocoded_location'] = tweets_df.loc[mask, 'location'].map(geocoded_results)"
      ],
      "metadata": {
        "id": "aKl7CyUDNBMf"
      },
      "id": "aKl7CyUDNBMf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save location cache after processing\n",
        "save_cache_to_json(location_cache)"
      ],
      "metadata": {
        "id": "6pTfHQSQiun3"
      },
      "id": "6pTfHQSQiun3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check value counts\n",
        "tweets_df['geocoded_location'].value_counts()"
      ],
      "metadata": {
        "id": "4HJk5wdYNxyJ"
      },
      "id": "4HJk5wdYNxyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue location data cleaning. For all undecoded ```geocoded_location```, try to apply first then last words of ```location```."
      ],
      "metadata": {
        "id": "rmtVoVogWUIG"
      },
      "id": "rmtVoVogWUIG"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mask for rows where 'geocoded_location' is 'Unknown' and where its known\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "known_mask = tweets_df['geocoded_location'] != 'Unknown'\n",
        "# extract FIRST word from 'location' for 'Unknown' geocoded rows and batch process\n",
        "if unknown_mask.any():\n",
        "    first_words = tweets_df.loc[unknown_mask, 'location'].apply(lambda loc: extract_word(loc, position=\"first\"))\n",
        "\n",
        "    unique_first_words = first_words.unique()\n",
        "    unique_first_words_set = set(unique_first_words)\n",
        "    # extract known locations where geocoded_location is not 'Unknown'\n",
        "    known_locations = tweets_df.loc[known_mask, 'location']\n",
        "    # tokenize each location into words and find intersection with unique_first_words_set\n",
        "    known_words_in_locations = known_locations.str.split().apply(lambda words: unique_first_words_set.intersection(words))\n",
        "\n",
        "    # extract matches\n",
        "    matched_words = {word for words in known_words_in_locations for word in words}\n",
        "\n",
        "    # perform geocoding for matched first words\n",
        "    first_word_results = batch_geocode(matched_words)\n",
        "    # create a mapping of unique first words to geocoded results\n",
        "    first_word_mapping = dict(zip(matched_words, first_word_results))\n",
        "\n",
        "    # map the results back to the DataFrame\n",
        "    tweets_df.loc[unknown_mask, 'geocoded_location'] = first_words.map(first_word_mapping)"
      ],
      "metadata": {
        "id": "m2GOQnD6R2uc"
      },
      "id": "m2GOQnD6R2uc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update the mask for remaining 'Unknown' rows\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "known_mask = tweets_df['geocoded_location'] != 'Unknown'\n",
        "\n",
        "# extract LAST word from 'location' for remaining 'Unknown' rows and batch process\n",
        "if unknown_mask.any():\n",
        "    last_words = tweets_df.loc[unknown_mask, 'location'].apply(lambda loc: extract_word(loc, position=\"last\"))\n",
        "    unique_last_words = last_words.unique()\n",
        "    unique_last_words_set = set(last_words)\n",
        "\n",
        "    known_locations = tweets_df.loc[known_mask, 'location']\n",
        "\n",
        "     # tokenize each location into words and find intersection with unique_first_words_set\n",
        "    known_words_in_locations = known_locations.str.split().apply(lambda words: unique_last_words_set.intersection(words))\n",
        "\n",
        "    # extract matches\n",
        "    matched_words = {word for words in known_words_in_locations for word in words}\n",
        "\n",
        "    # perform geocoding for unique last words\n",
        "    last_word_results = batch_geocode(matched_words)  # Ensure batch_geocode is defined\n",
        "    # create a mapping of unique last words to geocoded results\n",
        "    last_word_mapping = dict(zip(matched_words, last_word_results))\n",
        "\n",
        "    # map the results back to the DataFrame\n",
        "    tweets_df.loc[unknown_mask, 'geocoded_location'] = last_words.map(last_word_mapping)"
      ],
      "metadata": {
        "id": "l5uLD4NlRufX"
      },
      "id": "l5uLD4NlRufX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if still 'Unknown', give up\n",
        "unknown_mask = tweets_df['geocoded_location'] == 'Unknown'\n",
        "if unknown_mask.any():\n",
        "    print(f\"Giving up on {unknown_mask.sum()} locations. Could not geocode these entries.\")"
      ],
      "metadata": {
        "id": "gRDg2uJDRxtq"
      },
      "id": "gRDg2uJDRxtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update location cache after processing\n",
        "save_cache_to_json(location_cache)"
      ],
      "metadata": {
        "id": "k_SZ2FctjEb9"
      },
      "id": "k_SZ2FctjEb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill location NaNs with Unknown\n",
        "tweets_df.fillna(value={'geocoded_location':'Unknown'}, inplace=True)"
      ],
      "metadata": {
        "id": "2orl_xq7Yazz"
      },
      "id": "2orl_xq7Yazz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll use ```split_geocoded_location()``` to split the geocoded_location into three columns: ```country```, ```state```, and ```city```."
      ],
      "metadata": {
        "id": "6qrp8WChaMCI"
      },
      "id": "6qrp8WChaMCI"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to create separate columns\n",
        "tweets_df[['city', 'state', 'country']] = tweets_df['geocoded_location'].apply(\n",
        "    lambda loc: pd.Series(split_geocoded_location(loc))\n",
        ")\n",
        "# display the DataFrame with new columns\n",
        "tweets_df[['geocoded_location', 'city', 'state', 'country']]"
      ],
      "metadata": {
        "id": "9virTNaVavK_"
      },
      "id": "9virTNaVavK_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minor fix: capitalization\n",
        "# Capitalize all entries in 'City', 'State', and 'Country' columns\n",
        "tweets_df['city'] = tweets_df['city'].str.title()\n",
        "tweets_df['state'] = tweets_df['state'].str.title()\n",
        "tweets_df['country'] = tweets_df['country'].str.title()"
      ],
      "metadata": {
        "id": "f3iWZ3sxbXCA"
      },
      "id": "f3iWZ3sxbXCA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll perform location EDA: tweet histograms by Country (Worldwide), by City (Worldwide), By  State(US), followed by additional cleaning."
      ],
      "metadata": {
        "id": "G3d4li4aCFQo"
      },
      "id": "G3d4li4aCFQo"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Country EDA: Remove 'Unknown' countries and plot histogram for top 20 (ordered)\n",
        "known_countries = tweets_df[tweets_df['country'] != 'Unknown']\n",
        "top_20_countries = known_countries['country'].value_counts().nlargest(20)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=top_20_countries.index,\n",
        "    x=top_20_countries.values,\n",
        "    hue=top_20_countries.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 Countries (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Country')\n",
        "for i, v in enumerate(top_20_countries.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KLMPIu6CBVMT"
      },
      "id": "KLMPIu6CBVMT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Remove 'Unknown' cities and plot histogram for top 50 (ordered)\n",
        "known_cities = tweets_df[tweets_df['city'] != 'Unknown']\n",
        "top_50_cities = known_cities['city'].value_counts().nlargest(50)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    y=top_50_cities.index,\n",
        "    x=top_50_cities.values,\n",
        "    hue=top_50_cities.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 50 Cities (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('City')\n",
        "for i, v in enumerate(top_50_cities.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4RIdk7xSCbBV"
      },
      "id": "4RIdk7xSCbBV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out 'Unknown' states and keep only US entries\n",
        "us_states = tweets_df[(tweets_df['country'] == 'United States') & (tweets_df['state'] != 'Unknown')]\n",
        "\n",
        "# Count occurrences of each state and get the top 20\n",
        "state_counts = us_states['state'].value_counts().nlargest(20)\n",
        "\n",
        "# Plot the distribution of tweets by the top 20 US states\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=state_counts.index,\n",
        "    x=state_counts.values,\n",
        "    hue=state_counts.index,  # Use state as hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Prevent splitting bars\n",
        "    legend=False  # Suppress the legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 US States (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('US State')\n",
        "\n",
        "# Add labels to the bars\n",
        "for i, v in enumerate(state_counts.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UKlTA2rvD9FJ"
      },
      "id": "UKlTA2rvD9FJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minor fixes: Move Florida, California, Texas, Michigan  from City to State and rename None in state to Unknown\n",
        "states_as_cities = ['Florida', 'California', 'Texas', 'Michigan']\n",
        "mask_states_as_cities = (tweets_df['city'].isin(states_as_cities)) & (tweets_df['country'] == 'United States')\n",
        "\n",
        "# update State and City columns for these entries\n",
        "tweets_df.loc[mask_states_as_cities, 'state'] = tweets_df.loc[mask_states_as_cities, 'city']\n",
        "tweets_df.loc[mask_states_as_cities, 'city'] = 'Unknown'\n",
        "\n",
        "# rename all 'None' in State to 'Unknown'\n",
        "tweets_df['state'] = tweets_df['state'].replace('None', 'Unknown')\n",
        "\n",
        "# fix \"Alba/Scotland\" entries\n",
        "mask_alba_scotland = tweets_df['city'].str.contains('Alba/Scotland', case=False, na=False)\n",
        "\n",
        "# update City and Country for Alba/Scotland\n",
        "tweets_df.loc[mask_alba_scotland, 'city'] = 'Unknown'\n",
        "tweets_df.loc[mask_alba_scotland, 'country'] = 'Scotland'"
      ],
      "metadata": {
        "id": "CpKOlC0mEcFv"
      },
      "id": "CpKOlC0mEcFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# redo the city plot:\n",
        "# 2. Remove 'Unknown' cities and plot histogram for top 50 (ordered)\n",
        "known_cities = tweets_df[tweets_df['city'] != 'Unknown']\n",
        "top_50_cities = known_cities['city'].value_counts().nlargest(50)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(\n",
        "    y=top_50_cities.index,\n",
        "    x=top_50_cities.values,\n",
        "    hue=top_50_cities.index,  # Assign y to hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Ensure no splitting\n",
        "    legend=False  # Disable legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 50 Cities (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('City')\n",
        "for i, v in enumerate(top_50_cities.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zpEycPkWcHrz"
      },
      "id": "zpEycPkWcHrz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# redo the states plot:\n",
        "# Filter out 'Unknown' states and keep only US entries\n",
        "us_states = tweets_df[(tweets_df['country'] == 'United States') & (tweets_df['state'] != 'Unknown')]\n",
        "\n",
        "# Count occurrences of each state and get the top 20\n",
        "state_counts = us_states['state'].value_counts().nlargest(20)\n",
        "\n",
        "# Plot the distribution of tweets by the top 20 US states\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    y=state_counts.index,\n",
        "    x=state_counts.values,\n",
        "    hue=state_counts.index,  # Use state as hue\n",
        "    palette='viridis',\n",
        "    dodge=False,  # Prevent splitting bars\n",
        "    legend=False  # Suppress the legend\n",
        ")\n",
        "plt.title('Distribution of Tweets by Top 20 US States (Excluding Unknown)')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('US State')\n",
        "\n",
        "# Add labels to the bars\n",
        "for i, v in enumerate(state_counts.values):\n",
        "    plt.text(v, i, f\"{v}\", va='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iN_jW3o1b_M4"
      },
      "id": "iN_jW3o1b_M4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USA tweets per day\n",
        "usa_tweets = tweets_df[tweets_df['country'] == 'United States']\n",
        "\n",
        "# group by date and count tweets per day\n",
        "tweets_per_day = usa_tweets.groupby(usa_tweets['date'].dt.date).size().reset_index(name='Total Tweets')\n",
        "tweets_per_day.columns = ['Date', 'Total Tweets']\n",
        "\n",
        "# bar plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.barplot(\n",
        "    data=tweets_per_day,\n",
        "    x='Date',\n",
        "    y='Total Tweets',\n",
        "    hue='Date',  # Assign the x variable to hue\n",
        "    palette='viridis',\n",
        "    legend=False  # Disable the legend\n",
        ")\n",
        "\n",
        "# customize x-axis ticks to show every third date\n",
        "xticks = plt.gca().get_xticks()\n",
        "xtick_labels = tweets_per_day['Date'].astype(str).values\n",
        "plt.xticks(\n",
        "    ticks=xticks[::3],  # show every third tick\n",
        "    labels=xtick_labels[::3],  # use corresponding labels\n",
        "    rotation=90  # rotate 45 deg for better visibility\n",
        ")\n",
        "\n",
        "plt.xticks(rotation=90)  # rotate x-axis labels for better readability\n",
        "plt.title('Tweets Per Day in the USA')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.tight_layout()  # Adjust layout to prevent label overlap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZiT_qKnDdz5K"
      },
      "id": "ZiT_qKnDdz5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Optional] Adding lattitude, longitude (long runtime)**. This piece of code will run for about 1 hr, it's going to retrieve geo coordinates (latitude, longitude) for all cities in the dataset. Strongly recommend executing the following ```save_cache_to_json()``` to save the work and downloading the file if you're running the notebook from the cloud.\n",
        "\n",
        "For this particular dataset or scope of the business problem it might not worth it, but for advanced analysis or futire work it might be a useful EDA section to utilize.  "
      ],
      "metadata": {
        "id": "fo8-mFt8rMaz"
      },
      "id": "fo8-mFt8rMaz"
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = add_coordinates_with_progress(tweets_df)"
      ],
      "metadata": {
        "id": "F23LeybSxdp2"
      },
      "id": "F23LeybSxdp2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save coordinate cache to json\n",
        "save_cache_to_json(coordinate_cache, \"coordinate_cache.json\")"
      ],
      "metadata": {
        "id": "i4f-X-xWq1CE"
      },
      "id": "i4f-X-xWq1CE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the map if it's not found\n",
        "try:\n",
        "    # Try to open the file\n",
        "    heatmap_file = os.path.join(data_dir, \"heatmap_city.html\")\n",
        "    with open(heatmap_file, \"r\") as file:\n",
        "        print(\"heatmap_city.html already exists. No need to recompute.\")\n",
        "        heatmap_city = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"heatmap.html does not exist. Generating the heatmap...\")\n",
        "    # Call the function to compute the heatmap\n",
        "    # Filter rows where both latitude and longitude are not 0.0\n",
        "    filtered_tweets_df = tweets_df[(tweets_df['latitude'] != 0.0) & (tweets_df['longitude'] != 0.0)]\n",
        "    # Create tweet_count column\n",
        "    filtered_tweets_df.loc[:, 'tweet_count'] = filtered_tweets_df.groupby('city')['city'].transform('count')\n",
        "    heatmap_city = generateBaseMap(input_type=\"city\", df=filtered_tweets_df)\n",
        "    heatmap_city.save(heatmap_file)"
      ],
      "metadata": {
        "id": "vTzcb11aBfOn"
      },
      "id": "vTzcb11aBfOn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the map directly in the notebook (might not work)\n",
        "display(HTML(heatmap_city))"
      ],
      "metadata": {
        "id": "4odEWaXzHfPI"
      },
      "id": "4odEWaXzHfPI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.10 Data Cleaning: Source\n",
        "Next, we'll replace the ```source``` values with readable meaningful values using ```extract_html_source()``` function that will extract the value between the HTML tags."
      ],
      "metadata": {
        "id": "w4PgWcupwYB6"
      },
      "id": "w4PgWcupwYB6"
    },
    {
      "cell_type": "code",
      "source": [
        "# replace the source with the meaningful value\n",
        "tweets_df.source.value_counts()"
      ],
      "metadata": {
        "id": "XWS6nPsxn2UX"
      },
      "id": "XWS6nPsxn2UX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to the 'source' column\n",
        "tweets_df['source'] = tweets_df['source'].apply(extract_html_source)\n",
        "# identify sources with counts less than 100\n",
        "source_counts = tweets_df['source'].value_counts()\n",
        "low_count_sources = source_counts[source_counts < 100].index\n",
        "\n",
        "# replace low-count sources with 'Other'\n",
        "tweets_df['source'] = tweets_df['source'].replace(low_count_sources, 'Other')\n",
        "#replace NaNs with 'Other'\n",
        "tweets_df['source'].fillna('Other', inplace=True)\n",
        "tweets_df['source'].value_counts()"
      ],
      "metadata": {
        "id": "GvPFCp0voW5f"
      },
      "id": "GvPFCp0voW5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the top 20 sources\n",
        "top_20_sources = tweets_df['source'].value_counts().nlargest(20)\n",
        "\n",
        "# Convert to a DataFrame for plotting\n",
        "top_20_df = top_20_sources.reset_index()\n",
        "top_20_df.columns = ['Source', 'Count']\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    data=top_20_df,\n",
        "    y='Source',\n",
        "    x='Count',\n",
        "    hue='Source',  # Assign the x variable to hue\n",
        "    palette='viridis',\n",
        "    legend=False  # Disable the legend\n",
        ")\n",
        "plt.title('Top 20 Twitter Sources')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Source')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AJmCePQOoSb5"
      },
      "id": "AJmCePQOoSb5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.11 Data Cleaning: Sentiment"
      ],
      "metadata": {
        "id": "asvDQcaVaZQp"
      },
      "id": "asvDQcaVaZQp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Show distribution of tweet sentiments\n",
        "sentiment_counts = tweets_df.sentiment.value_counts()\n",
        "print(sentiment_counts)\n",
        "# is there any NaNs\n",
        "print(tweets_df.sentiment.isna().sum())"
      ],
      "metadata": {
        "id": "95I77q_xoCPX"
      },
      "id": "95I77q_xoCPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom colors for the sentiments\n",
        "custom_colors = {\n",
        "    'pos': '#90EE90',  # Light Green\n",
        "    'neu': '#ADD8E6',   # Light Blue\n",
        "    'neg': '#FFB6C1'   # Light Red\n",
        "}\n",
        "\n",
        "# Ensure the colors map correctly to the sentiment categories\n",
        "print(\"Sentiment Categories:\", sentiment_counts.index)  # Debugging step\n",
        "colors = [custom_colors.get(sentiment, '#D3D3D3') for sentiment in sentiment_counts.index]\n",
        "\n",
        "# Check the colors being applied\n",
        "print(\"Applied Colors:\", colors)  # Debugging step\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(\n",
        "    sentiment_counts.values,\n",
        "    labels=sentiment_counts.index,\n",
        "    autopct=lambda p: f'{p:.1f}% ({int(p * sum(sentiment_counts.values) / 100)})',  # Percentage and count\n",
        "    colors=colors,  # Use custom colors\n",
        "    startangle=90\n",
        ")\n",
        "plt.title('Sentiment Distribution', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OkmtHZGsoE0N"
      },
      "id": "OkmtHZGsoE0N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sentiments and their corresponding colors\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
        "\n",
        "# Scatter plots for the relationship between sentiment scores and compound value\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (sentiment, color) in enumerate(zip(sentiments, colors), start=1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.scatterplot(\n",
        "        data=tweets_df,\n",
        "        x=sentiment,\n",
        "        y='compound',\n",
        "        alpha=0.6,\n",
        "        color=color\n",
        "    )\n",
        "    plt.title(f'Compound Score vs {sentiment.capitalize()}')\n",
        "    plt.xlabel(sentiment.capitalize())\n",
        "    plt.ylabel('Compound Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EQnrTxdO4TWj"
      },
      "id": "EQnrTxdO4TWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From Kaggle's data card:\n",
        "Algorithm Sentiment Classification of Tweets (compound, sentiment):\n",
        "\n",
        "if tweet[compound] < 0:\n",
        "tweet[sentiment] = 0.0 # assigned 0.0 for Negative Tweets\n",
        "elif tweet[compound] > 0:\n",
        "tweet[sentiment] = 1.0 # assigned 1.0 for Positive Tweets\n",
        "else:\n",
        "tweet[sentiment] = 0.5 # assigned 0.5 for Neutral Tweets\n",
        "end```"
      ],
      "metadata": {
        "id": "FHOVEGR4FPx6"
      },
      "id": "FHOVEGR4FPx6"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.reset_option('display.max_colwidth')\n",
        "\n",
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "positive_tweets = tweets_df[tweets_df['sentiment'] == 'pos']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = positive_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ],
      "metadata": {
        "id": "VsDAMyKDFj6Z"
      },
      "id": "VsDAMyKDFj6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "neutral_tweets = tweets_df[tweets_df['sentiment'] == 'neu']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = neutral_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ],
      "metadata": {
        "id": "N-Ozs-UCHEl9"
      },
      "id": "N-Ozs-UCHEl9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset for rows where sentiment is 'pos'\n",
        "negative_tweets = tweets_df[tweets_df['sentiment'] == 'neg']\n",
        "\n",
        "# Randomly select 5 rows and specific columns\n",
        "subset = negative_tweets[['original_text', 'pos', 'neu', 'neg', 'compound']].sample(n=5, random_state=42)\n",
        "subset"
      ],
      "metadata": {
        "id": "KfLiQNJuHRSC"
      },
      "id": "KfLiQNJuHRSC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatterplot for value distribution in neutral sentiment column."
      ],
      "metadata": {
        "id": "Kflp-YXVIGFk"
      },
      "id": "Kflp-YXVIGFk"
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df['neu'].value_counts()"
      ],
      "metadata": {
        "id": "vdy3Su78IFcX"
      },
      "id": "vdy3Su78IFcX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding sentiment compound distribution between positive, neutral, negative score values.\n",
        "\n",
        "Here we have a good distribution for positive, negative tweets. We're unable to plot neutral tweets's distribution because the variance is almost zero (most variables are around 0)."
      ],
      "metadata": {
        "id": "leAjLtZpzE2W"
      },
      "id": "leAjLtZpzE2W"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.kdeplot(\n",
        "    data=tweets_df,\n",
        "    x='compound',\n",
        "    hue='sentiment',\n",
        "    fill=True,\n",
        "    alpha=0.6,\n",
        "    palette={'neg': 'lightcoral', 'neu': 'lightblue', 'pos': 'lightgreen'}\n",
        ")\n",
        "plt.title('KDE of Compoundby by Sentiment')\n",
        "plt.xlabel('Compound Weighted')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VVtJyTzSzDqd"
      },
      "id": "VVtJyTzSzDqd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-computing the compound score using weighted formula: Compound Score=(pos−neg)×(1−neu)**\n",
        "\n",
        "We'll also exclude rows where all 3 values for `pos`, `neu`, `neg` are either 0 or 1."
      ],
      "metadata": {
        "id": "78HibLzkLK0S"
      },
      "id": "78HibLzkLK0S"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where all three values are either 0 or 1\n",
        "valid_rows = ~(\n",
        "    ((tweets_df['pos'] == 0) & (tweets_df['neu'] == 0) & (tweets_df['neg'] == 0)) |\n",
        "    ((tweets_df['pos'] == 1) & (tweets_df['neu'] == 1) & (tweets_df['neg'] == 1))\n",
        ")\n",
        "\n",
        "# Apply the filter explicitly with .loc[]\n",
        "tweets_df = tweets_df.loc[valid_rows]\n",
        "\n",
        "# Compute the weighted compound score using .loc[]\n",
        "tweets_df.loc[:, 'compound_weighted'] = (tweets_df['pos'] - tweets_df['neg']) * (1 - tweets_df['neu'])\n",
        "\n",
        "# Scale it to [0, 1]\n",
        "tweets_df['compound_weighted'] = (tweets_df['compound_weighted'] - tweets_df['compound_weighted'].min()) / (\n",
        "    tweets_df['compound_weighted'].max() - tweets_df['compound_weighted'].min()\n",
        ")\n",
        "\n",
        "# Display a preview of the updated DataFrame\n",
        "print(tweets_df[['pos', 'neu', 'neg', 'compound', 'compound_weighted', 'sentiment']].head())"
      ],
      "metadata": {
        "id": "6akAIPcRMnTV"
      },
      "id": "6akAIPcRMnTV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replot scores sentiments\n",
        "sentiments = ['pos', 'neu', 'neg']\n",
        "colors = ['lightgreen', 'lightblue', 'lightcoral']\n",
        "\n",
        "# Scatter plots for the relationship between sentiment scores and compound value\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "for i, (sentiment, color) in enumerate(zip(sentiments, colors), start=1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.scatterplot(\n",
        "        data=tweets_df,\n",
        "        x=sentiment,\n",
        "        y='compound_weighted',\n",
        "        alpha=0.6,\n",
        "        color=color\n",
        "    )\n",
        "    plt.title(f'Compound Score vs {sentiment.capitalize()}')\n",
        "    plt.xlabel(sentiment.capitalize())\n",
        "    plt.ylabel('Compound Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-iy7Olu1NiZM"
      },
      "id": "-iy7Olu1NiZM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting new and old compound score distributions.\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Plot weighted compound distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(tweets_df['compound_weighted'], bins=30, kde=False, color='purple')\n",
        "plt.title('Distribution of Weighted Compound Scores')\n",
        "plt.xlabel('Weighted Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Plot compound distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(tweets_df['compound'], bins=30, kde=False, color='orange')\n",
        "plt.title('Distribution of Original Compound Scores')\n",
        "plt.xlabel('Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wYpFMoLrOZdc"
      },
      "id": "wYpFMoLrOZdc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing new sentiments:\n",
        "\n",
        "# Define conditions and labels for new_simple_sentiment\n",
        "simple_conditions = [\n",
        "    (tweets_df['compound_weighted'] <= 0.49),  # Negative sentiment\n",
        "    (tweets_df['compound_weighted'] >0.49) & (tweets_df['compound_weighted'] <= 0.51),  # Neutral sentiment\n",
        "    (tweets_df['compound_weighted'] > 0.51)  # Positive sentiment\n",
        "]\n",
        "simple_labels = ['neg', 'neu', 'pos']\n",
        "\n",
        "# Define conditions and labels for new_advanced_sentiment\n",
        "advanced_conditions = [\n",
        "    (tweets_df['compound_weighted'] < 0.35),  # Strongly Negative\n",
        "    (tweets_df['compound_weighted'] >= 0.35) & (tweets_df['compound_weighted'] <= 0.49),  # Weakly Negative (neu-neg)\n",
        "    (tweets_df['compound_weighted'] >0.49) & (tweets_df['compound_weighted'] <= 0.51),  # Neutral sentiment\n",
        "    (tweets_df['compound_weighted'] > 0.51) & (tweets_df['compound_weighted'] <= 0.75),  # Weakly Positive (neu-pos)\n",
        "    (tweets_df['compound_weighted'] > 0.75)  # Strongly Positive\n",
        "]\n",
        "advanced_labels = ['neg', 'neu-neg', 'neu', 'neu-pos', 'pos']\n",
        "\n",
        "# Assign new_simple_sentiment\n",
        "tweets_df['new_simple_sentiment'] = pd.cut(\n",
        "    tweets_df['compound_weighted'],\n",
        "    bins=[-float('inf'), 0.49, 0.51, float('inf')],  # Define boundaries\n",
        "    labels=['neg', 'neu', 'pos']\n",
        ")\n",
        "\n",
        "# Assign new_advanced_sentiment\n",
        "tweets_df['new_advanced_sentiment'] = pd.cut(\n",
        "    tweets_df['compound_weighted'],\n",
        "    bins=[-float('inf'), 0.35, 0.49, 0.51, 0.75, float('inf')],  # Define boundaries\n",
        "    labels=['neg', 'neu-neg', 'neu', 'neu-pos', 'pos']\n",
        ")\n",
        "\n",
        "# Check the distribution of the new sentiment columns\n",
        "print(tweets_df['new_simple_sentiment'].value_counts())\n",
        "print(tweets_df['new_advanced_sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "-3jWXTSASXj8"
      },
      "id": "-3jWXTSASXj8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intermediate conclusions:\n",
        "* We don't know whow assigned scores are computed (it's not regular or weighted computation).\n",
        "* We can guess the prescence of a weights placed on the negative or positive parts of the score, when one dominates the other. That would explain the difference in compound score distribution, skewing tweets with more positive or negative scores urther away from neutral.\n",
        "* We should also consider that back in 2020 twitter was a better moderated environment, and many tweets would originate from official accounts, contributing to their neutral sentiment.  "
      ],
      "metadata": {
        "id": "3nq9SmWDWk1Z"
      },
      "id": "3nq9SmWDWk1Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.12 Exploratory Data Analysis (EDA): Social Connections\n",
        "Vanity fair. Lookng at social engagement, connections, popularity."
      ],
      "metadata": {
        "id": "sn9Hm_WY84o3"
      },
      "id": "sn9Hm_WY84o3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hashtags word cloud."
      ],
      "metadata": {
        "id": "_rsWWdIhAniD"
      },
      "id": "_rsWWdIhAniD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the list of hashtags\n",
        "all_hashtags = [\n",
        "    ''.join(hashtags)  # Join characters into a string\n",
        "    for hashtags in tweets_df['hashtags']\n",
        "    if isinstance(hashtags, (list, tuple, str, np.ndarray))  # Check if iterable\n",
        "]\n",
        "# Convert the list of hashtags into a single string, separated by spaces\n",
        "hashtag_text = ' '.join(all_hashtags)\n",
        "\n",
        "# Generate the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, \\\n",
        "                      background_color='white', colormap='viridis').generate(hashtag_text)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Hashtag Word Cloud\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYYTMdrq9nNs"
      },
      "id": "zYYTMdrq9nNs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Histograms for favorite tweets, retweets.\n",
        "Convert both to log scale because of the value range (wide range, outliers).\n"
      ],
      "metadata": {
        "id": "C1Lt_TCCArYL"
      },
      "id": "C1Lt_TCCArYL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-transform favorite_count (add 1 to avoid log(0))\n",
        "tweets_df['log_favorite_count'] = np.log1p(tweets_df['favorite_count'])\n",
        "\n",
        "# Histogram for log-transformed favorite_count\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(tweets_df['log_favorite_count'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribution of Log-Transformed Favorite Counts')\n",
        "plt.xlabel('Log(Favorite Count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Log-transform retweet_count (add 1 to avoid log(0))\n",
        "tweets_df['log_retweet_count'] = np.log1p(tweets_df['retweet_count'])\n",
        "\n",
        "# Histogram for log-transformed retweet_count\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(tweets_df['log_retweet_count'], bins=30, kde=True, color='green')\n",
        "plt.title('Distribution of Log-Transformed Retweet Counts')\n",
        "plt.xlabel('Log(Retweet Count)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZfLetef9-DkN"
      },
      "id": "ZfLetef9-DkN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log-transform favorite_count and retweet_count (add 1 to avoid log(0))\n",
        "tweets_df['log_favorite_count'] = np.log1p(tweets_df['favorite_count'])\n",
        "tweets_df['log_retweet_count'] = np.log1p(tweets_df['retweet_count'])\n",
        "\n",
        "# Scatter plot with log-transformed values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='log_favorite_count', y='log_retweet_count', data=tweets_df, alpha=0.6)\n",
        "plt.title('Log(Favorites) vs Log(Retweets)')\n",
        "plt.xlabel('Log(Favorite Count)')\n",
        "plt.ylabel('Log(Retweet Count)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EnxKnDBe-c7q"
      },
      "id": "EnxKnDBe-c7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Show trends in favorites and retweets over time.\n",
        "aggregated = tweets_df.groupby('date')[['favorite_count', 'retweet_count']].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(x='date', y='favorite_count', data=aggregated, label='Favorites', color='blue')\n",
        "plt.yscale('log')\n",
        "sns.lineplot(x='date', y='retweet_count', data=aggregated, label='Retweets', color='green')\n",
        "plt.title('Log Trends in Favorites and Retweets Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NNUqkPi5-mXH"
      },
      "id": "NNUqkPi5-mXH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top contributors by the number of tweets\n",
        "top_authors = tweets_df['original_author'].value_counts().head(10)\n",
        "# Bar Plot for Top Authors\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    x=top_authors.values,\n",
        "    y=top_authors.index,\n",
        "    hue=top_authors.index,  # Assign hue to the y variable\n",
        "    palette='viridis',\n",
        "    dodge=False,            # Ensure no splitting\n",
        "    legend=False            # Disable the legend\n",
        ")\n",
        "plt.title('Top 10 Authors by Number of Tweets')\n",
        "plt.xlabel('Number of Tweets')\n",
        "plt.ylabel('Author')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YgsLPGyT-uc6"
      },
      "id": "YgsLPGyT-uc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot authors based on their average favorite_count and retweet_count\n",
        "author_stats = tweets_df.groupby('original_author')[['favorite_count', \\\n",
        "                                                     'retweet_count']].mean().reset_index()\n",
        "\n",
        "author_stats['log_favorite_count'] = np.log1p(author_stats['favorite_count'])\n",
        "author_stats['log_retweet_count'] = np.log1p(author_stats['retweet_count'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='log_favorite_count', y='log_retweet_count', data=author_stats, alpha=0.6\n",
        ")\n",
        "plt.title('Author Popularity: Log-Transformed Average Favorites vs Retweets')\n",
        "plt.xlabel('Log(Average Favorite Count)')\n",
        "plt.ylabel('Log(Average Retweet Count)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWQxhA-u_GOW"
      },
      "id": "YWQxhA-u_GOW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the most mentioned users."
      ],
      "metadata": {
        "id": "ndi-I9-9__Hx"
      },
      "id": "ndi-I9-9__Hx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Process user_mentions: split by comma and remove empty/malformed entries\n",
        "tweets_df['user_mentions'] = tweets_df['user_mentions'].fillna('')  # Replace None with empty strings\n",
        "tweets_df['user_mentions_clean'] = tweets_df['user_mentions'].apply(lambda x: [mention.strip() for mention in x.split(',') if mention.strip()])\n",
        "\n",
        "# Flatten mentions and count frequencies\n",
        "all_mentions = [mention for mentions in tweets_df['user_mentions_clean'] for mention in mentions]\n",
        "mention_counts = Counter(all_mentions).most_common(10)\n",
        "\n",
        "# Convert to DataFrame for visualization\n",
        "mention_df = pd.DataFrame(mention_counts, columns=['user', 'count'])\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(\n",
        "    x='count',\n",
        "    y='user',\n",
        "    data=mention_df,\n",
        "    hue='user',        # Assign hue to the y variable\n",
        "    palette='magma',\n",
        "    dodge=False,       # Prevent bar splitting\n",
        "    legend=False       # Disable the legend\n",
        ")\n",
        "plt.title('Top 10 Mentioned Users', fontsize=14)\n",
        "plt.xlabel('Number of Mentions', fontsize=12)\n",
        "plt.ylabel('User', fontsize=12)\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53_X2kEx_Rlf"
      },
      "id": "53_X2kEx_Rlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Show the relationships between authors and mentioned users. With a subset that big, we won't have computational resources to compute a full graph.  "
      ],
      "metadata": {
        "id": "fZvsOhFo_6JZ"
      },
      "id": "fZvsOhFo_6JZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter nodes with degree above a certain threshold (200 connections)\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "# Populate the graph with edges from tweets_df\n",
        "for _, row in tweets_df.iterrows():\n",
        "    original_author = row['original_author']\n",
        "    mentioned_users = row['user_mentions_clean']  # List of mentioned users\n",
        "\n",
        "    # Add edges: author -> mentioned user\n",
        "    for mention in mentioned_users:\n",
        "        G.add_edge(original_author, mention)\n",
        "\n",
        "degree_threshold = 200\n",
        "high_degree_nodes = [node for node, degree in G.degree() if degree > degree_threshold]\n",
        "subgraph = G.subgraph(high_degree_nodes)\n",
        "\n",
        "# Draw the subgraph\n",
        "plt.figure(figsize=(12, 12))\n",
        "nx.draw_networkx(\n",
        "    subgraph,\n",
        "    node_size=50,\n",
        "    alpha=0.7,\n",
        "    font_size=8,\n",
        "    edge_color='gray'\n",
        ")\n",
        "plt.title(f'User Mentions Network (Nodes with Degree > {degree_threshold})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ZXy4hax_cYf"
      },
      "id": "5ZXy4hax_cYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Display correlation between different metrics (e.g., favorites, retweets, mentions).*italicized text*"
      ],
      "metadata": {
        "id": "zNKuJ_Nh_jDi"
      },
      "id": "zNKuJ_Nh_jDi"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(tweets_df[['favorite_count', 'retweet_count']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Between Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aUI5MgVT_lwr"
      },
      "id": "aUI5MgVT_lwr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize authors with bubbles representing their popularity."
      ],
      "metadata": {
        "id": "EUTwTTrb_w8t"
      },
      "id": "EUTwTTrb_w8t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate favorite_count and retweet_count for each author\n",
        "author_stats = tweets_df.groupby('original_author').agg({\n",
        "    'favorite_count': 'mean',\n",
        "    'retweet_count': 'mean',\n",
        "    'original_author': 'count'  # Count tweets for each author\n",
        "}).rename(columns={'original_author': 'tweet_count'}).reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    x='favorite_count',\n",
        "    y='retweet_count',\n",
        "    size='tweet_count',\n",
        "    data=author_stats,\n",
        "    sizes=(50, 500),\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title('Author Popularity: Favorites vs Retweets with Tweet Count')\n",
        "plt.xlabel('Average Favorite Count')\n",
        "plt.ylabel('Average Retweet Count')\n",
        "plt.legend(title='Tweet Count', loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "of54h0nm_0Ku"
      },
      "id": "of54h0nm_0Ku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Text Preprocessing and Feature Engineering\n",
        "\n",
        "In this section we'll be transforming the contents of the `original_tweet` column to the text format suitable for classification. This will include the following steps:\n",
        "\n",
        "**Cleaning**\n",
        "\n",
        "This step removes noise and unnecessary elements from the raw tweet text, such as special characters, links, mentions (@usernames), and hashtags (#topic), ii also detects emoji and converts it to plain text, (😀 to smile face).\n",
        "\n",
        "**Preprocessing**\n",
        "This section prepares the cleaned text for sentiment analysis by breaking it down into individual words (tokenization), removing common words that don't carry much meaning (stop word removal), and reducing words to their base form (lemmatization).\n",
        "**Feature Extraction**\n",
        "\n",
        "This stage involves creating numerical representations of the text that can be used as input for machine learning models. We'll be extracting\n",
        "* Bag of Words (BoW): This technique represents text as a collection of individual words and their frequencies, ignoring grammar and word order. It creates a numerical vector for each document, where each element represents the count of a specific word in the document's vocabulary.\n",
        "* N-grams: Creating combinations of words (e.g., \"Covid cases\" as a bigram, or \"Die of Covid\") to capture more context.\n",
        "* TF-IDF: Calculating the importance of words in a document relative to a collection of documents.\n",
        "* Embeddings: Using pre-trained models to create vector representations of words that capture semantic meaning.\n"
      ],
      "metadata": {
        "id": "MWCtcPLd3q8z"
      },
      "id": "MWCtcPLd3q8z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Text Cleaning\n",
        "We wrote ```process_tweet_data()```to do the following:\n",
        "* Remove and store emoji as a separate column(using default UNICODE_EMOJI).\n",
        "* Remove and store mentions @ as a separate column.\n",
        "* Remove and store retweets RT @ as a separate column.\n",
        "* Remove and store hashtags # as a separate column.\n",
        "* Remove and store URLs www. or t. or bit. as a separate column.\n",
        "* Remove special characters, whitespaces, numbers.\n",
        "* Lowercase and store text in ```cleaned_text``` column  \n",
        "\n",
        "\n",
        "For debugging purposes, we'll create a small dataframe with URLs, mentions, retweents, and emojis."
      ],
      "metadata": {
        "id": "Vkjyw35VopKU"
      },
      "id": "Vkjyw35VopKU"
    },
    {
      "cell_type": "code",
      "source": [
        "# example df\n",
        "df_test = {\n",
        "    'tweet_text': [\n",
        "        \"WOW!!! Check out this link: https://example.com :) @user123 #hashtag #fun\",\n",
        "        \"RT @user456: Another day in paradise! 😃 #sunnyday\",\n",
        "        \"Why so serious? :(( Visit www.example.org for details! #serious\"\n",
        "    ]\n",
        "}\n",
        "df_test = pd.DataFrame(df_test)\n",
        "\n",
        "# Clean the text data\n",
        "df_test = pd.concat([\n",
        "    df_test,\n",
        "    df_test['tweet_text'].apply(lambda x: pd.Series(process_tweet_data(x)))\n",
        "], axis=1)\n",
        "df_test"
      ],
      "metadata": {
        "id": "Hu094gmiwvFz"
      },
      "id": "Hu094gmiwvFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll apply text cleaning with progress to the subset of entire twitter df (takes about 2 hours minutes)"
      ],
      "metadata": {
        "id": "zG2ge4CK27kf"
      },
      "id": "zG2ge4CK27kf"
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_slice_df = tweets_df[['id', 'date', 'source', 'original_text', 'sentiment', \\\n",
        "                             'pos', 'neu', 'neg', 'compound', 'compound_weighted', \\\n",
        "                             'new_simple_sentiment', 'new_advanced_sentiment']].copy()"
      ],
      "metadata": {
        "id": "XvYD8vhPju9c"
      },
      "id": "XvYD8vhPju9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for NaNs before text pre-processing.\n",
        "tweets_slice_df.isna().sum()\n",
        "# drop NaNs from ID\n",
        "tweets_slice_df = tweets_slice_df.dropna(subset=['id'])\n",
        "#re-check for NaNs:\n",
        "tweets_slice_df.isna().sum()"
      ],
      "metadata": {
        "id": "j_q9NBBTeSbM"
      },
      "id": "j_q9NBBTeSbM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all retweets\n",
        "# Detect retweets: Create a mask for rows that are retweets\n",
        "retweet_mask = tweets_slice_df['original_text'].str.startswith('RT @')\n",
        "\n",
        "# Filter out retweets\n",
        "tweets_df_no_retweets = tweets_slice_df[~retweet_mask].copy()\n",
        "\n",
        "# Print results\n",
        "print(f\"Original DataFrame size: {tweets_df.shape[0]} rows\")\n",
        "print(f\"DataFrame size after removing retweets: {tweets_df_no_retweets.shape[0]} rows\")"
      ],
      "metadata": {
        "id": "JQp5P0qtgWjr"
      },
      "id": "JQp5P0qtgWjr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the function to each tweet and expand the results into separate columns\n",
        "cleaned_text_column = clean_tweets_with_progress_parallel(tweets_df_no_retweets, text_col='original_text')"
      ],
      "metadata": {
        "id": "rtFLmGL6kb_r"
      },
      "id": "rtFLmGL6kb_r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the cleaned text column to dataframe:\n",
        "tweets_df_no_retweets['cleaned_text'] = cleaned_text_column"
      ],
      "metadata": {
        "id": "JCGX7QGtPly3"
      },
      "id": "JCGX7QGtPly3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df_no_retweets = tweets_df_no_retweets.dropna(subset=['cleaned_text'])\n",
        "tweets_df_no_retweets.isna().sum()"
      ],
      "metadata": {
        "id": "JcPLYI2XPqb9"
      },
      "id": "JcPLYI2XPqb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save as inrermideate processing step\n",
        "path_to_saved_file = os.path.join(data_dir, \"tweets_df_no_retweets.csv\")  # Path to saved file\n",
        "tweets_df_no_retweets.to_csv(path_to_saved_file, index=False)\n",
        "print(\"tweets_df_no_retweets has been saved as 'tweets_df_no_retweets.csv'.\")"
      ],
      "metadata": {
        "id": "XdN06fpqOAP-"
      },
      "id": "XdN06fpqOAP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Text Preprocessing\n",
        "\n",
        "This section focuses on preparing the tweet text data for sentiment analysis by applying essential preprocessing techniques. The following steps are performed:\n",
        "\n",
        "* Tokenization: The text of each tweet is broken down into individual words or tokens using the word_tokenize function from the nltk library.\n",
        "\n",
        "* Stop Word Removal: Common words (like \"the,\" \"a,\" \"is\") that don't carry much meaning are removed from the tokenized text to reduce noise and improve analysis accuracy.\n",
        "\n",
        "* Lemmatization: Words are reduced to their base or root form (e.g., \"running\" becomes \"run\") using the WordNetLemmatizer to standardize the vocabulary and group similar words together.\n",
        "\n",
        "* Text Recombination: The preprocessed tokens are combined back into a single text string for further analysis."
      ],
      "metadata": {
        "id": "o_Kzz-ygoslI"
      },
      "id": "o_Kzz-ygoslI"
    },
    {
      "cell_type": "code",
      "source": [
        "# load tweets_slice_df if memory crashes\n",
        "path_to_saved_file = os.path.join(data_dir, \"tweets_df_no_retweets.csv\")  # Path to saved file\n",
        "tweets_df_no_retweets = pd.read_csv(path_to_saved_file, low_memory=False)\n",
        "tweets_df_no_retweets.dropna(subset=['cleaned_text'], inplace=True)"
      ],
      "metadata": {
        "id": "vi3waIylg09B"
      },
      "id": "vi3waIylg09B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df_no_retweets.head()"
      ],
      "metadata": {
        "id": "zw8XhSX1fzIc"
      },
      "id": "zw8XhSX1fzIc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text in the 'cleaned_text' column\n",
        "preprocessed_df_test = preprocess_text(df_test, text_column='cleaned_text')\n",
        "# Display the preprocessed DataFrame\n",
        "preprocessed_df_test[['cleaned_text', 'lemmatized_text']]"
      ],
      "metadata": {
        "id": "uhWbdBrPrTvy"
      },
      "id": "uhWbdBrPrTvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply lemmatization to entire dataframe\n",
        "lemmatized_df = preprocess_text(tweets_df_no_retweets, text_column='cleaned_text')"
      ],
      "metadata": {
        "id": "T9567GjY2sBe"
      },
      "id": "T9567GjY2sBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_df.isna().sum()"
      ],
      "metadata": {
        "id": "OEpUVykLWiP5"
      },
      "id": "OEpUVykLWiP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_df= lemmatized_df.dropna(subset=['lemmatized_text'])\n",
        "# checking NaNs\n",
        "lemmatized_df.isna().sum()"
      ],
      "metadata": {
        "id": "NmExG5RhtM73"
      },
      "id": "NmExG5RhtM73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save lemmatized_df if memory crashes\n",
        "path_to_saved_lemmatized_file = os.path.join(data_dir, \"lemmatized_df.csv\")  # Path to saved file\n",
        "lemmatized_df.to_csv(path_to_saved_lemmatized_file, index=False)\n",
        "print(\"lemmatized_df has been saved as 'lemmatized_df.csv'.\")"
      ],
      "metadata": {
        "id": "8mmpr-vBy2pv"
      },
      "id": "8mmpr-vBy2pv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Feature Extraction.\n",
        "In this section, we'll transform the preprocessed text data into numerical representations suitable for machine learning models. We'll explore techniques like:\n",
        "\n",
        "N-grams: Extracting sequences of adjacent words (e.g., \"social distancing,\" \"stay home\") to capture contextual information.\n",
        "TF-IDF: Calculating word importance based on their frequency within a document and across the entire corpus.\n",
        "Embeddings (optional): Representing words as dense vectors to capture semantic relationships.\n",
        "These extracted features will serve as input for the sentiment analysis models in the next stage."
      ],
      "metadata": {
        "id": "1uQsQ8CI5rTG"
      },
      "id": "1uQsQ8CI5rTG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll run test datafram through the N-grams generation, Bag of Words, TF-IDF."
      ],
      "metadata": {
        "id": "BqK3b_82vt6b"
      },
      "id": "BqK3b_82vt6b"
    },
    {
      "cell_type": "code",
      "source": [
        "# load lemmatized_df if memory crashes\n",
        "# path_to_saved_lemmatized_file = os.path.join(data_dir, \"lemmatized_df.csv\")  # Path to saved file\n",
        "# lemmatized_df = pd.read_csv(path_to_saved_lemmatized_file, low_memory=False)"
      ],
      "metadata": {
        "id": "XsyUUQMXzlnD"
      },
      "id": "XsyUUQMXzlnD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply N-gram computation to test lemmatized twitter df\n",
        "ngram_matrix_0, vectorizer_0 = compute_ngrams(preprocessed_df_test, \\\n",
        "                                          text_column='lemmatized_text', ngram_range=(2, 3))\n",
        "\n",
        "# Feature names (n-grams)\n",
        "ngram_features_0 = vectorizer_0.get_feature_names_out()\n",
        "\n",
        "print(f\"Top 10 n-grams: {ngram_features_0[:10]}\")\n",
        "print(f\"Shape of n-gram matrix: {ngram_matrix_0.shape}\")"
      ],
      "metadata": {
        "id": "HpqpRxgzwG6X"
      },
      "id": "HpqpRxgzwG6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # can be adjusted as needed"
      ],
      "metadata": {
        "id": "yS_svUDn5O3R"
      },
      "id": "yS_svUDn5O3R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer to test lemmatized twitter df\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_df_test['lemmatized_text'])\n",
        "\n",
        "## Visualize the results\n",
        "# tfidf_matrix and tfidf_vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.toarray().sum(axis=0)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(zip(feature_names, tfidf_scores)))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L9uvTwVdbnxI"
      },
      "id": "L9uvTwVdbnxI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words for the test matrix\n",
        "bow_vectorizer = TfidfVectorizer(max_features=5000)  #  can be adjusted as needed"
      ],
      "metadata": {
        "id": "xBm257Ts5Ub4"
      },
      "id": "xBm257Ts5Ub4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer to test lemmatized twitter df\n",
        "bow_matrix = bow_vectorizer.fit_transform(preprocessed_df_test['lemmatized_text'])\n",
        "\n",
        "# Plot heatmap\n",
        "subset_matrix = bow_matrix.toarray()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(subset_matrix, cmap=\"viridis\", annot=True, fmt=\".2g\",\n",
        "            xticklabels=bow_vectorizer.get_feature_names_out(),\n",
        "            yticklabels=range(bow_matrix.shape[0]))\n",
        "plt.title(\"Bag of Words Heatmap (Test Data)\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Tweets\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uyZF7DKM5C-W"
      },
      "id": "uyZF7DKM5C-W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting N-grams for the entire datset"
      ],
      "metadata": {
        "id": "3f9XdGf2fEqI"
      },
      "id": "3f9XdGf2fEqI"
    },
    {
      "cell_type": "code",
      "source": [
        "ngram_matrix_twitter, vectorizer_twitter = compute_ngrams(lemmatized_df, \\\n",
        "                                          text_column='lemmatized_text', ngram_range=(2, 3))\n",
        "\n",
        "# Feature names (n-grams)\n",
        "ngram_features_twitter = vectorizer_twitter.get_feature_names_out()\n",
        "\n",
        "print(f\"Top 10 n-grams: {ngram_features_twitter[:10]}\")\n",
        "print(f\"Shape of n-gram matrix: {ngram_features_twitter.shape}\")"
      ],
      "metadata": {
        "id": "UHccUBALfEQj"
      },
      "id": "UHccUBALfEQj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How N-grams are distributed by sentiments?"
      ],
      "metadata": {
        "id": "T1tFg4UZnwOb"
      },
      "id": "T1tFg4UZnwOb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the n-gram matrix to a DataFrame\n",
        "ngram_df = pd.DataFrame(\n",
        "    ngram_matrix_twitter.toarray(),\n",
        "    columns=ngram_features_twitter\n",
        ")\n",
        "ngram_df['sentiment'] = lemmatized_df['sentiment']  # Add the sentiment column\n",
        "\n",
        "# Aggregate n-grams by sentiment\n",
        "sentiment_ngrams = ngram_df.groupby('sentiment').sum()\n",
        "\n",
        "# Step 3: Select top n-grams for each sentiment\n",
        "top_ngrams_per_sentiment = {}\n",
        "n_top = 10\n",
        "for sentiment in sentiment_ngrams.index:\n",
        "    top_ngrams = sentiment_ngrams.loc[sentiment].nlargest(n_top)\n",
        "    top_ngrams_per_sentiment[sentiment] = top_ngrams\n",
        "\n",
        "# Step 4: Visualize the top n-grams for each sentiment\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "for i, (sentiment, top_ngrams) in enumerate(top_ngrams_per_sentiment.items(), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.barplot(\n",
        "    x=top_ngrams.values,\n",
        "    y=top_ngrams.index,\n",
        "    hue=top_ngrams.index,  # Assign hue to the y variable\n",
        "    dodge=False,           # Ensure no splitting\n",
        "    legend=False,          # Disable the legend\n",
        "    palette='viridis'\n",
        "    )\n",
        "    plt.title(f\"Top {n_top} N-grams ({sentiment})\", fontsize=14)\n",
        "    plt.xlabel('Frequency', fontsize=12)\n",
        "    plt.ylabel('N-Gram', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zkB0FY6knpP-"
      },
      "id": "zkB0FY6knpP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the BoW vectorizer to full twitter data\n",
        "bow_matrix = bow_vectorizer.fit_transform(lemmatized_df['lemmatized_text'])"
      ],
      "metadata": {
        "id": "FeqTbIb4wtQs"
      },
      "id": "FeqTbIb4wtQs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum frequencies for each term\n",
        "term_frequencies = bow_matrix.sum(axis=0).A1  # Sum along columns\n",
        "top_indices = term_frequencies.argsort()[-20:][::-1]  # Indices of top 20 terms\n",
        "\n",
        "# Subset matrix for these terms\n",
        "subset_matrix = bow_matrix[:, top_indices][:20].toarray()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(subset_matrix, cmap='viridis', annot=True, fmt=\".2f\",\n",
        "            xticklabels=bow_vectorizer.get_feature_names_out()[top_indices],  # Feature names for top terms\n",
        "            yticklabels=range(20))\n",
        "plt.title('Heatmap of Top 20 Terms (BoW)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cryPP72seY0l"
      },
      "id": "cryPP72seY0l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the tf-idf vectorizer to full twitter data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(lemmatized_df['lemmatized_text'])"
      ],
      "metadata": {
        "id": "c7iZuFzTwwwm"
      },
      "id": "c7iZuFzTwwwm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualize the results\n",
        "# Compute term frequencies\n",
        "tfidf_scores = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
        "\n",
        "# Get feature names from the tfidf_vectorizer\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Select top 200 terms (adjust if needed)\n",
        "top_indices = np.argsort(tfidf_scores)[-200:]\n",
        "top_features = feature_names[top_indices] # Now, feature_names is from tfidf_vectorizer\n",
        "top_scores = tfidf_scores[top_indices]\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(\n",
        "    dict(zip(top_features, top_scores))\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9dvQ36XgeZB3"
      },
      "id": "9dvQ36XgeZB3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings (BERT)."
      ],
      "metadata": {
        "id": "-jzHSx2ntiBt"
      },
      "id": "-jzHSx2ntiBt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "# bert_model_name = \"bert-base-uncased\"\n",
        "# tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "# model = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "# Apply BERT embeddings to dataset with progress bar\n",
        "# lemmatized_df['bert_embedding'] = compute_bert_embeddings_parallel(lemmatized_df, text_column='lemmatized_text')"
      ],
      "metadata": {
        "id": "F_jx83r6tc1g"
      },
      "id": "F_jx83r6tc1g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save lemmatize_df\n",
        "#lemmatized_df.to_csv(\"lemmatized_df_with_bert.csv\", index=False)\n",
        "#print(\"lemmatized_df has been saved as 'lemmatized_df_with_bert.csv'.\")"
      ],
      "metadata": {
        "id": "bjVGl_MBvIiI"
      },
      "id": "bjVGl_MBvIiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "glove_file = \"glove.twitter.27B.200d.txt\"  # 200-dimensional embeddings\n",
        "glove_path = os.path.join(data_dir, glove_file)\n",
        "embeddings_dict = load_glove_embeddings(glove_path)\n",
        "\n",
        "#Compute embeddings for a dataset\n",
        "lemmatized_df['sentence_embedding'] = lemmatized_df['lemmatized_text'].apply(\n",
        "    lambda x: compute_sentence_embedding(x, embeddings_dict, embedding_dim=200)\n",
        ")"
      ],
      "metadata": {
        "id": "IKb5Y14j_-Cf"
      },
      "id": "IKb5Y14j_-Cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting features ready for use:\n",
        "# Convert embeddings into a 2D NumPy array\n",
        "embeddings_matrix = np.vstack(lemmatized_df['sentence_embedding'].values)\n",
        "embeddings_sparse = csr_matrix(embeddings_matrix)"
      ],
      "metadata": {
        "id": "DCV2GAjXuJU3"
      },
      "id": "DCV2GAjXuJU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Sentiment Classification\n",
        "\n",
        "This section aims to analyze the sentiment expressed in the preprocessed tweets using machine learning models. The goal is to classify tweets into different sentiment categories (e.g., positive, negative, neutral) and to understand the overall sentiment trends within the dataset. This involves:\n",
        "\n",
        "* Model Selection and Training: Choosing a suitable classification model and training it on the preprocessed tweet data. We'll start with a baseline simple model.\n",
        "\n",
        "* Model Evaluation: Assessing the performance of the trained model using appropriate metrics.\n",
        "* Insights and Visualization: Presenting the results of the sentiment analysis, including visualizations and insights derived from the model's predictions.\n",
        "\n",
        "\n",
        "**Input Overview**\n",
        "\n",
        "* Our input features `X` will be a combination (concatenation) of computed text features:\n",
        "`bow_matrix`, `tfidf_matrix`, `ngram_matrix_twitter`, `embeddings_sparse`.\n",
        "* Our target variable `y` is the sentiment : `{neg, neu, pos}`.\n",
        "* Our performance metric will be `f1-score`.   \n",
        "* X dimentions: `170675x10200`.\n",
        "* Sentiment distribution (class imbalance):  \n",
        "`neu:76894, pos:50239, neg:43542`.\n"
      ],
      "metadata": {
        "id": "7ZLUlvn-57Z_"
      },
      "id": "7ZLUlvn-57Z_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and labels\n",
        "X_bow = hstack([bow_matrix, ngram_matrix_twitter, embeddings_sparse])\n",
        "X_tfidf = hstack([tfidf_matrix, ngram_matrix_twitter, embeddings_sparse])\n",
        "X_bow.shape"
      ],
      "metadata": {
        "id": "VuioIhl14jJS"
      },
      "id": "VuioIhl14jJS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check sentiment distribution between all 3 versions:\n",
        "print(lemmatized_df['sentiment'].value_counts())\n",
        "print(lemmatized_df['new_simple_sentiment'].value_counts())\n",
        "print(lemmatized_df['new_advanced_sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "4gIrwKh2vT0A"
      },
      "id": "4gIrwKh2vT0A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  4.1 Train-Test Data Split.\n",
        " Split proportions: 70% training, 15% testing, 15% validation."
      ],
      "metadata": {
        "id": "m7djBMSQ4oHO"
      },
      "id": "m7djBMSQ4oHO"
    },
    {
      "cell_type": "code",
      "source": [
        "y = lemmatized_df['sentiment']     # Sentiment labels\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the sentiment labels\n",
        "y_encoded = label_encoder.fit_transform(y)  # y contains 'pos', 'neg', 'neu'\n",
        "\n",
        "# Check the mapping\n",
        "print(\"Label Mapping:\", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))"
      ],
      "metadata": {
        "id": "pT6GL1sdvTAO"
      },
      "id": "pT6GL1sdvTAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_bow, y_encoded, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "Cc2jlpJO5vAk"
      },
      "id": "Cc2jlpJO5vAk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# size of each set\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Number of features\n",
        "print(f\"Number of features: {X_train.shape[1]}\")"
      ],
      "metadata": {
        "id": "SkoWwP420l-9"
      },
      "id": "SkoWwP420l-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save features and variables for future use\n",
        "pd.to_pickle(X_train, os.path.join(data_dir, \"X_train.pkl\"))\n",
        "pd.to_pickle(X_val, os.path.join(data_dir, \"X_val.pkl\"))\n",
        "pd.to_pickle(X_test, os.path.join(data_dir, \"X_test.pkl\"))\n",
        "pd.to_pickle(y_train, os.path.join(data_dir, \"y_train.pkl\"))\n",
        "pd.to_pickle(y_val, os.path.join(data_dir, \"y_val.pkl\"))\n",
        "pd.to_pickle(y_test, os.path.join(data_dir, \"y_test.pkl\"))\n",
        "pd.to_pickle(label_encoder, os.path.join(data_dir, \"label_encoder.pkl\"))\n",
        "pd.to_pickle(bow_vectorizer, os.path.join(data_dir, \"bow_vectorizer.pkl\"))\n",
        "pd.to_pickle(ngram_features_twitter, os.path.join(data_dir, \"vectorizer_twitter.pkl\"))\n",
        "pd.to_pickle(embeddings_dict, os.path.join(data_dir, \"embeddings_dict.pkl\"))"
      ],
      "metadata": {
        "id": "DWZcqR4EYD4m"
      },
      "id": "DWZcqR4EYD4m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Sentiment Classification with Simple Logistic Regression Model."
      ],
      "metadata": {
        "id": "5C0RTXca1uq3"
      },
      "id": "5C0RTXca1uq3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Model\n",
        "log_reg = LogisticRegression(\n",
        "    penalty='l2',  # L2 regularization (Ridge regression penalty)\n",
        "    C=1.0,  # Regularization strength (smaller = stronger regularization)\n",
        "    max_iter=1000,  # Ensure convergence\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5iyfqcVVYZ83"
      },
      "id": "5iyfqcVVYZ83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "y_val_log = log_reg.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_val\n",
        "\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_log_decoded = label_encoder.inverse_transform(y_pred_log)\n",
        "y_val_log_decoded = label_encoder.inverse_transform(y_val_log)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Base Model LogReg Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_log_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Base Model LogReg Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_log_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "7S8e1VYnZVYm"
      },
      "id": "7S8e1VYnZVYm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test_decoded, y_pred_log_decoded, labels=label_encoder.classes_)\n",
        "\n",
        "# Normalize confusion matrix by rows to show percentages\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentages\n",
        "\n",
        "# Display the normalized confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_percent, display_labels=label_encoder.classes_)\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp.plot(cmap='Blues', colorbar=True, ax=ax)\n",
        "\n",
        "plt.title(\"LogReg Confusion Matrix (Percentages) on Test Set\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vGymIuwFaIkn"
      },
      "id": "vGymIuwFaIkn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Tuning Hyperparameters for Base Logisstic Regression Model"
      ],
      "metadata": {
        "id": "1l4AxpFoa8BU"
      },
      "id": "1l4AxpFoa8BU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Grid search with F1 scoring\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=log_reg,\n",
        "    param_grid=param_grid,\n",
        "    scoring=make_scorer(f1_score, average='weighted'),  # Weighted F1 score\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and F1 score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best F1 Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "kQNYPilwa5M2"
      },
      "id": "kQNYPilwa5M2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Improved Model Performance and Feature Interpretation."
      ],
      "metadata": {
        "id": "0pXuFA03glp2"
      },
      "id": "0pXuFA03glp2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Feature Reduction (Experimental)"
      ],
      "metadata": {
        "id": "QeqthMaZEDoh"
      },
      "id": "QeqthMaZEDoh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure X_bow is dense\n",
        "X_dense = csr_matrix(X_bow).toarray()\n",
        "\n",
        "# Use the hybrid GWO + ABC optimizer\n",
        "best_features = hybrid_gwo_abc(X_dense, y_encoded, model=SVC(kernel='linear'))"
      ],
      "metadata": {
        "id": "X7_z--b3BfSu"
      },
      "id": "X7_z--b3BfSu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the best feature mask\n",
        "selected_indices = np.where(best_features == 1)[0]\n",
        "X_bow_selected = X_bow[:, selected_indices]\n",
        "\n",
        "print(f\"Reduced feature set from {X_bow.shape[1]} to {X_bow_selected.shape[1]}\")"
      ],
      "metadata": {
        "id": "1T0nWRwGBfnT"
      },
      "id": "1T0nWRwGBfnT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test-val split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_bow_selected, y_encoded, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "# Train SVM\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "Ry6eJEEtEmMU"
      },
      "id": "Ry6eJEEtEmMU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Other Tested Classification Models."
      ],
      "metadata": {
        "id": "3VReoTUA71g3"
      },
      "id": "3VReoTUA71g3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.1 SDG Classifier Performance."
      ],
      "metadata": {
        "id": "Ywzy8538Qz2E"
      },
      "id": "Ywzy8538Qz2E"
    },
    {
      "cell_type": "code",
      "source": [
        "# SGDClassifier with logistic regression loss\n",
        "sgd_model = SGDClassifier(\n",
        "    loss='log_loss',          # Logistic regression loss\n",
        "    penalty='l2',             # L2 regularization\n",
        "    alpha=1e-4,               # Regularization strength\n",
        "    max_iter=1000,            # Maximum number of iterations\n",
        "    tol=1e-3,                 # Convergence tolerance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and evaluate the SGD model\n",
        "sgd_model.fit(X_train, y_train)\n",
        "y_pred_sgd = sgd_model.predict(X_test)\n",
        "y_val_sgd = sgd_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_sgd\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_sgd_decoded = label_encoder.inverse_transform(y_pred_sgd)\n",
        "y_val_sgd_decoded = label_encoder.inverse_transform(y_val_sgd)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_sgd_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_sgd_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "3w8cBoZHre_5"
      },
      "id": "3w8cBoZHre_5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients = pipeline_sgd.named_steps['sgd'].coef_\n",
        "\n",
        "# Get feature names and align them with the combined feature matrix\n",
        "bow_features = bow_vectorizer.get_feature_names_out()\n",
        "ngram_features = vectorizer_twitter.get_feature_names_out()\n",
        "embedding_dim = 200\n",
        "embedding_feature_names = [f\"embedding_{i}\" for i in range(embedding_dim)]\n",
        "\n",
        "all_features = np.concatenate([bow_features, ngram_features, embedding_feature_names])  # Combine feature names\n",
        "\n",
        "# Ensure alignment between coefficients and feature names\n",
        "if coefficients.shape[1] != len(all_features):\n",
        "    raise ValueError(\"Mismatch between coefficients and feature names.\")\n",
        "\n",
        "# Analyze top features for each sentiment\n",
        "for i, sentiment in enumerate(label_encoder.classes_):\n",
        "    top_indices = np.argsort(coefficients[i])[-10:]  # Top 10 features for each sentiment\n",
        "    top_features = all_features[top_indices]  # Use combined features\n",
        "\n",
        "    print(f\"\\nTop Features for {sentiment.capitalize()} Sentiment:\")\n",
        "    for feature, coef in zip(top_features, coefficients[i][top_indices]):\n",
        "        if feature.startswith(\"embedding_\"):\n",
        "            # Extract the embedding index from the feature name (e.g., embedding_11 -> 11)\n",
        "            embedding_index = int(feature.split(\"_\")[1])\n",
        "            print(f\"{feature} (Embedding Value): {coef:.4f}\")\n",
        "        else:\n",
        "            print(f\"{feature}: {coef:.4f}\")"
      ],
      "metadata": {
        "id": "FYbw9_9X1aC7"
      },
      "id": "FYbw9_9X1aC7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.2. Random Forest"
      ],
      "metadata": {
        "id": "rs_Rk3ZsbySX"
      },
      "id": "rs_Rk3ZsbySX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model with specific parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "# Train and evaluate the RF model\n",
        "\n",
        "y_pred_rf= rf_model.predict(X_test)\n",
        "y_val_rf = rf_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_rf\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_rf_decoded = label_encoder.inverse_transform(y_pred_rf)\n",
        "y_val_rf_decoded = label_encoder.inverse_transform(y_val_rf)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Random Forest Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_rf_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Random Forest Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_rf_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "DgZUJdCkA513"
      },
      "id": "DgZUJdCkA513",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desicion Tree with specific parameters\n",
        "dt_model =DecisionTreeClassifier(criterion = 'gini',\n",
        "                                 max_depth = 20,\n",
        "                                 min_samples_leaf = 2,\n",
        "                                 min_samples_split = 10,\n",
        "                                 random_state=42)\n",
        "\n",
        "\n",
        "# Train and evaluate the DT model\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt= dt_model.predict(X_test)\n",
        "y_val_dt = dt_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_dt\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_dt_decoded = label_encoder.inverse_transform(y_pred_dt)\n",
        "y_val_dt_decoded = label_encoder.inverse_transform(y_val_dt)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"Decision Trees Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_dt_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"Decision Trees Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_dt_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "7uk4_l3g7HWE"
      },
      "id": "7uk4_l3g7HWE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5.3 XGBoost"
      ],
      "metadata": {
        "id": "MSIJ0KwohOrB"
      },
      "id": "MSIJ0KwohOrB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the XGBoost model with specific parameters\n",
        "xgb_model = XGBClassifier(learning_rate=0.2, max_depth=10, n_estimators=200)\n",
        "\n",
        "# Train and evaluate the XGBoost model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_xgb= xgb_model.predict(X_test)\n",
        "y_val_xgb = xgb_model.predict(X_val)\n",
        "\n",
        "# Decode labels for y_test and y_pred_xgb\n",
        "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
        "y_val_decoded = label_encoder.inverse_transform(y_val)\n",
        "y_pred_xgb_decoded = label_encoder.inverse_transform(y_pred_xgb)\n",
        "y_val_xgb_decoded = label_encoder.inverse_transform(y_val_xgb)\n",
        "\n",
        "# Compute classification report for test set\n",
        "print(\"XGB Classification Report for Test Set:\")\n",
        "print(classification_report(y_test_decoded, y_pred_xgb_decoded, target_names=label_encoder.classes_))\n",
        "\n",
        "# Compute classification report for validation set\n",
        "print(\"XGB Classification Report for Validation Set:\")\n",
        "print(classification_report(y_val_decoded, y_val_xgb_decoded, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "id": "Y6zdbCy-afec"
      },
      "id": "Y6zdbCy-afec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Conclusion."
      ],
      "metadata": {
        "id": "iFbPQcpw74-o"
      },
      "id": "iFbPQcpw74-o"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}